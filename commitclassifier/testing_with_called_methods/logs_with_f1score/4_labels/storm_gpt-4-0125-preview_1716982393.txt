File: storm, pull request #448
Model: gpt-4-0125-preview

Diff number: 1
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/Config.java b/storm-core/src/jvm/backtype/storm/Config.java
index 73a1976ff..1237c28f2 100644
--- a/storm-core/src/jvm/backtype/storm/Config.java
+++ b/storm-core/src/jvm/backtype/storm/Config.java
@@ -441,6 +441,12 @@ public class Config extends HashMap<String, Object> {
     public static final String NIMBUS_AUTHORIZER = "nimbus.authorizer";
     public static final Object NIMBUS_AUTHORIZER_SCHEMA = String.class;
 
+    /**
+     * Impersonation user ACL config entries.
+     */
+    public static final String NIMBUS_IMPERSONATION_ACL = "nimbus.impersonation.acl";
+    public static final Object NIMBUS_IMPERSONATION_ACL_SCHEMA = ConfigValidation.MapOfStringToMapValidator;
+
     /**
      * How often nimbus should wake up to renew credentials if needed.
      */

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The addition of the 'NIMBUS_IMPERSONATION_ACL' configuration parameter directly impacts the security functionalities of the system, specifically in the area of authentication and access control. Impersonation, or the ability for one user to act on behalf of another, is a critical security feature that requires strict controls to prevent abuse. The introduction of an Access Control List (ACL) for impersonation suggests a mechanism is being put in place to manage and restrict who can impersonate other users within the system. This change is directly related to ensuring that only authorized users can perform impersonation, which is essential for maintaining the integrity and security of user identities and their associated actions within the system.",
  "confidence": 9
}
```

Diff number: 2
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/ConfigValidation.java b/storm-core/src/jvm/backtype/storm/ConfigValidation.java
index 14394a08b..24991d783 100644
--- a/storm-core/src/jvm/backtype/storm/ConfigValidation.java
+++ b/storm-core/src/jvm/backtype/storm/ConfigValidation.java
@@ -168,12 +168,18 @@ public class ConfigValidation {
      * Validates a list of Strings.
      */
     public static Object StringsValidator = listFv(String.class, true);
-    
+
     /**
      * Validates a map of Strings to Numbers.
      */
     public static Object MapOfStringToNumberValidator = mapFv(String.class, Number.class, true);
 
+    /**
+     * Validates a map of Strings to a map of Strings to a list.
+     * {str -> {str -> [str,str]}
+     */
+    public static Object MapOfStringToMapValidator = mapFv(fv(String.class, false), mapFv(fv(String.class, false), listFv(String.class, false), false), true);
+
     /**
      * Validates is a list of Maps.
      */

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_not_security_relevant",
  "reason": "The provided diff file shows modifications in the ConfigValidation.java file of the storm-core project. The changes include the addition of a new validator, `MapOfStringToMapValidator`, which validates a map of strings to a map of strings to a list. This type of validation is typically used to ensure the integrity and structure of configuration data rather than directly impacting security functionalities such as authentication, encryption, access control, or logging of security events. However, depending on how this new validator is used within the application, it could indirectly affect the system's security posture by ensuring that configuration data, which might include security-relevant settings, adheres to a specific structure. Without further context on how this validator is applied to security-sensitive configurations, it's challenging to directly link the change to a security functionality.",
  "confidence": 7
}
```

Diff number: 3
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/StormSubmitter.java b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
index 54cc70100..63cc5a146 100644
--- a/storm-core/src/jvm/backtype/storm/StormSubmitter.java
+++ b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
@@ -175,22 +175,19 @@ public class StormSubmitter {
     }
 
     /**
-     * Submits a topology to run on the cluster. A topology runs forever or until
-     * explicitly killed.
      *
-     *
-     * @param name the name of the storm.
-     * @param stormConf the topology-specific configuration. See {@link Config}.
-     * @param topology the processing to execute.
-     * @param opts to manipulate the starting of the topology
-     * @param progressListener to track the progress of the jar upload process
-     * @throws AlreadyAliveException if a topology with this name is already running
-     * @throws InvalidTopologyException if an invalid topology was submitted
-     * @throws AuthorizationException if authorization is failed
+     * @param name
+     * @param stormConf
+     * @param topology
+     * @param opts
+     * @param progressListener
+     * @param asUser The user as which this topology should be submitted.
+     * @throws AlreadyAliveException
+     * @throws InvalidTopologyException
+     * @throws AuthorizationException
      */
-    @SuppressWarnings("unchecked")
-    public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
-             ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
+    public static void submitTopologyAs(String name, Map stormConf, StormTopology topology, SubmitOptions opts, ProgressListener progressListener, String asUser)
+            throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
         if(!Utils.isValidConf(stormConf)) {
             throw new IllegalArgumentException("Storm conf is not valid. Must be json-serializable");
         }
@@ -218,25 +215,25 @@ public class StormSubmitter {
             if(localNimbus!=null) {
                 LOG.info("Submitting topology " + name + " in local mode");
                 if(opts!=null) {
-                    localNimbus.submitTopologyWithOpts(name, stormConf, topology, opts);                    
+                    localNimbus.submitTopologyWithOpts(name, stormConf, topology, opts);
                 } else {
                     // this is for backwards compatibility
-                    localNimbus.submitTopology(name, stormConf, topology);                                            
+                    localNimbus.submitTopology(name, stormConf, topology);
                 }
             } else {
                 String serConf = JSONValue.toJSONString(stormConf);
-                NimbusClient client = NimbusClient.getConfiguredClient(conf);
-                if(topologyNameExists(conf, name)) {
+                NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser);
+                if(topologyNameExists(conf, name, asUser)) {
                     throw new RuntimeException("Topology with name `" + name + "` already exists on cluster");
                 }
-                String jar = submitJar(conf, progressListener);
+                String jar = submitJarAs(conf, System.getProperty("storm.jar"), progressListener, asUser);
                 try {
                     LOG.info("Submitting topology " +  name + " in distributed mode with conf " + serConf);
                     if(opts!=null) {
-                        client.getClient().submitTopologyWithOpts(name, jar, serConf, topology, opts);                    
+                        client.getClient().submitTopologyWithOpts(name, jar, serConf, topology, opts);
                     } else {
                         // this is for backwards compatibility
-                        client.getClient().submitTopology(name, jar, serConf, topology);                                            
+                        client.getClient().submitTopology(name, jar, serConf, topology);
                     }
                 } catch(InvalidTopologyException e) {
                     LOG.warn("Topology submission exception: "+e.get_msg());
@@ -254,6 +251,26 @@ public class StormSubmitter {
         }
     }
 
+    /**
+     * Submits a topology to run on the cluster. A topology runs forever or until
+     * explicitly killed.
+     *
+     *
+     * @param name the name of the storm.
+     * @param stormConf the topology-specific configuration. See {@link Config}.
+     * @param topology the processing to execute.
+     * @param opts to manipulate the starting of the topology
+     * @param progressListener to track the progress of the jar upload process
+     * @throws AlreadyAliveException if a topology with this name is already running
+     * @throws InvalidTopologyException if an invalid topology was submitted
+     * @throws AuthorizationException if authorization is failed
+     */
+    @SuppressWarnings("unchecked")
+    public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
+             ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
+
+    }
+
     /**
      * Submits a topology to run on the cluster with a progress bar. A topology runs forever or until
      * explicitly killed.
@@ -310,8 +327,8 @@ public class StormSubmitter {
         });
     }
 
-    private static boolean topologyNameExists(Map conf, String name) {
-        NimbusClient client = NimbusClient.getConfiguredClient(conf);
+    private static boolean topologyNameExists(Map conf, String name, String asUser) {
+        NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser);
         try {
             ClusterSummary summary = client.getClient().getClusterInfo();
             for(TopologySummary s : summary.get_topologies()) {
@@ -342,19 +359,13 @@ public class StormSubmitter {
         return submitJar(conf, localJar, null);
     }
 
-    /**
-     * Submit jar file
-     * @param conf the topology-specific configuration. See {@link Config}.
-     * @param localJar file path of the jar file to submit
-     * @param listener progress listener to track the jar file upload
-     * @return the remote location of the submitted jar
-     */
-    public static String submitJar(Map conf, String localJar, ProgressListener listener) {
+
+    public static String submitJarAs(Map conf, String localJar, ProgressListener listener, String asUser) {
         if (localJar == null) {
             throw new RuntimeException("Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
         }
 
-        NimbusClient client = NimbusClient.getConfiguredClient(conf);
+        NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser);
         try {
             String uploadLocation = client.getClient().beginFileUpload();
             LOG.info("Uploading topology jar " + localJar + " to assigned location: " + uploadLocation);
@@ -385,12 +396,23 @@ public class StormSubmitter {
             LOG.info("Successfully uploaded topology jar to assigned location: " + uploadLocation);
             return uploadLocation;
         } catch(Exception e) {
-            throw new RuntimeException(e);            
+            throw new RuntimeException(e);
         } finally {
             client.close();
         }
     }
 
+    /**
+     * Submit jar file
+     * @param conf the topology-specific configuration. See {@link Config}.
+     * @param localJar file path of the jar file to submit
+     * @param listener progress listener to track the jar file upload
+     * @return the remote location of the submitted jar
+     */
+    public static String submitJar(Map conf, String localJar, ProgressListener listener) {
+        return submitJarAs(conf,localJar, listener, null);
+    }
+
     /**
      * Interface use to track progress of file upload
      */

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.command;

import java.util.Arrays;
import java.util.Map;

import org.apache.commons.lang.ArrayUtils;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.NimbusSummary;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.ServerUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class ShellSubmission {
    private static final Logger LOG = LoggerFactory.getLogger(ShellSubmission.class);

    public static void main(String[] args) throws Exception {
        if (args.length <= 1) {
            LOG.error("Arguments should be of the form: <path_to_jar> [argument...]");
            System.exit(-1);
        }
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        try (NimbusClient client = NimbusClient.getConfiguredClient(conf)) {
            NimbusSummary ns = client.getClient().getLeader();
            String host = ns.get_host();
            int port = ns.get_port();
            String jarPath = StormSubmitter.submitJar(conf, args[0]);
            String[] newArgs = (String[]) ArrayUtils.addAll(Arrays.copyOfRange(args, 1, args.length),
                    new String[]{host, String.valueOf(port), jarPath});
            ServerUtils.execCommand(newArgs);
        }
    }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.storm;

import java.util.Map;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.generated.SubmitOptions;
import org.apache.storm.generated.TopologyInitialStatus;
import org.apache.storm.testing.TestWordCounter;
import org.apache.storm.topology.TopologyBuilder;
import org.junit.jupiter.api.Test;

import static org.junit.jupiter.api.Assertions.fail;

public class TestStormSubmitter {

    @Test
    public void invalidTopologyWithoutSpout() {
        String expectedExceptionMsgFragment = "does not have any spout";
        TopologyBuilder tb = new TopologyBuilder();
        tb.setBolt("bolt1", new TestWordCounter(), 10).shuffleGrouping("spout1");
        tb.setBolt("bolt11", new TestWordCounter(), 10).shuffleGrouping("bolt1");
        tb.setBolt("bolt12", new TestWordCounter(), 10).shuffleGrouping("bolt1");
        StormTopology topology = tb.createTopology();
        Map<String, Object> topoConf = null;
        SubmitOptions opts = new SubmitOptions(TopologyInitialStatus.INACTIVE);

        try {
            StormSubmitter.submitTopologyAs("test-topo-without-spout", topoConf, topology, opts, null, "none");
            fail("Topology without spout should fail in submission");
        } catch (InvalidTopologyException ex) {
            if (!ex.getMessage().contains(expectedExceptionMsgFragment)) {
                String err = String.format("Topology submit failure should contain string \"%s\", but is \"%s\"",
                        expectedExceptionMsgFragment, ex.getMessage());
                fail(err);
            }
        } catch (Throwable ex) {
            ex.printStackTrace();
            fail("Unexpected exception submitting topology without spout: " + ex);
        }
    }
}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.sql;

import java.io.BufferedOutputStream;
import java.io.ByteArrayOutputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.List;
import java.util.Map;
import java.util.jar.Attributes;
import java.util.jar.JarOutputStream;
import java.util.jar.Manifest;
import java.util.zip.ZipEntry;
import org.apache.calcite.sql.SqlNode;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.generated.SubmitOptions;
import org.apache.storm.sql.javac.CompilingClassLoader;
import org.apache.storm.sql.parser.SqlCreateFunction;
import org.apache.storm.sql.parser.SqlCreateTable;
import org.apache.storm.sql.parser.StormParser;

class StormSqlImpl extends StormSql {
    private final StormSqlContext sqlContext;

    StormSqlImpl() {
        sqlContext = new StormSqlContext();
    }

    @Override
    public void submit(
        String name, Iterable<String> statements, Map<String, Object> topoConf, SubmitOptions opts,
        StormSubmitter.ProgressListener progressListener, String asUser)
        throws Exception {
        for (String sql : statements) {
            StormParser parser = new StormParser(sql);
            SqlNode node = parser.impl().parseSqlStmtEof();
            if (node instanceof SqlCreateTable) {
                sqlContext.interpretCreateTable((SqlCreateTable) node);
            } else if (node instanceof SqlCreateFunction) {
                sqlContext.interpretCreateFunction((SqlCreateFunction) node);
            } else {
                AbstractStreamsProcessor processor = sqlContext.compileSql(sql);
                StormTopology topo = processor.build();

                Path jarPath = null;
                try {
                    // QueryPlanner on Streams mode configures the topology with compiled classes,
                    // so we need to add new classes into topology jar
                    // Topology will be serialized and sent to Nimbus, and deserialized and executed in workers.

                    jarPath = Files.createTempFile("storm-sql", ".jar");
                    System.setProperty("storm.jar", jarPath.toString());
                    packageTopology(jarPath, processor);
                    StormSubmitter.submitTopologyAs(name, topoConf, topo, opts, progressListener, asUser);
                } finally {
                    if (jarPath != null) {
                        Files.delete(jarPath);
                    }
                }
            }
        }
    }

    @Override
    public void explain(Iterable<String> statements) throws Exception {
        for (String sql : statements) {
            System.out.println("===========================================================");
            System.out.println("query>");
            System.out.println(sql);
            System.out.println("-----------------------------------------------------------");

            StormParser parser = new StormParser(sql);
            SqlNode node = parser.impl().parseSqlStmtEof();
            if (node instanceof SqlCreateTable) {
                sqlContext.interpretCreateTable((SqlCreateTable) node);
                System.out.println("No plan presented on DDL");
            } else if (node instanceof SqlCreateFunction) {
                sqlContext.interpretCreateFunction((SqlCreateFunction) node);
                System.out.println("No plan presented on DDL");
            } else {
                String plan = sqlContext.explain(sql);
                System.out.println("plan>");
                System.out.println(plan);
            }

            System.out.println("===========================================================");
        }
    }

    private void packageTopology(Path jar, AbstractStreamsProcessor processor) throws IOException {
        Manifest manifest = new Manifest();
        Attributes attr = manifest.getMainAttributes();
        attr.put(Attributes.Name.MANIFEST_VERSION, "1.0");
        attr.put(Attributes.Name.MAIN_CLASS, processor.getClass().getCanonicalName());
        try (JarOutputStream out = new JarOutputStream(
            new BufferedOutputStream(new FileOutputStream(jar.toFile())), manifest)) {
            List<CompilingClassLoader> classLoaders = processor.getClassLoaders();
            if (classLoaders != null && !classLoaders.isEmpty()) {
                for (CompilingClassLoader classLoader : classLoaders) {
                    for (Map.Entry<String, ByteArrayOutputStream> e : classLoader.getClasses().entrySet()) {
                        out.putNextEntry(new ZipEntry(e.getKey().replace(".", "/") + ".class"));
                        out.write(e.getValue().toByteArray());
                        out.closeEntry();
                    }
                }
            }
        }
    }
}

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The modifications introduce a new method 'submitTopologyAs' which allows submitting a topology as a specific user ('asUser' parameter). This change could potentially impact the system's security by altering the authentication flow or enabling privilege escalation if not properly validated and authorized. The method 'submitJarAs' and modifications to 'topologyNameExists' and 'NimbusClient.getConfiguredClientAs' to accept an 'asUser' parameter also suggest changes in how authentication and authorization might be handled, potentially affecting the security of topology submission and jar file upload processes. If the 'asUser' parameter is not securely validated or if the authorization checks are insufficient, it could lead to unauthorized access or actions within the system. Additionally, the removal of detailed documentation in the method signature reduces the clarity around the security expectations and error handling related to authorization failures.",
  "confidence": 8
}
```

Diff number: 4
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/ITransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/ITransportPlugin.java
index 7575d71cb..5ba25576a 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/ITransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/ITransportPlugin.java
@@ -54,6 +54,8 @@ public interface ITransportPlugin {
      * Connect to the specified server via framed transport 
      * @param transport The underlying Thrift transport.
      * @param serverHost server host
+     * @param asUser the user as which the connection should be established, and all the subsequent actions should be executed.
+     *               Only applicable when using secure storm cluster. A null/blank value here will just indicate to use the logged in user.
      */
-    public TTransport connect(TTransport transport, String serverHost) throws IOException, TTransportException;
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws IOException, TTransportException;
 }

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The modification in the ITransportPlugin interface to include an 'asUser' parameter in the 'connect' method directly impacts the authentication mechanism of the system. By allowing connections to be established as a specific user, this change introduces a significant alteration in how user identities are handled during the connection process. This could potentially enhance the system's security by providing a more granular control over connection permissions and ensuring actions are executed under the correct user context, especially in a secure storm cluster environment. However, it also raises concerns about the proper implementation of this feature, such as ensuring that the 'asUser' parameter cannot be exploited to impersonate other users without proper authorization. The security of the system now partially depends on how this new parameter is validated and used within the system, making it a critical change to the authentication process.",
  "confidence": 9
}
```

Diff number: 5
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java b/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java
index 4033f1870..a252f85b8 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java
@@ -22,6 +22,9 @@ import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.net.InetAddress;
 import com.google.common.annotations.VisibleForTesting;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import java.security.AccessControlContext;
 import java.security.AccessController;
 import java.security.Principal;
@@ -39,6 +42,10 @@ public class ReqContext {
     private InetAddress _remoteAddr;
     private Integer _reqID;
     private Map _storm_conf;
+    private Principal realPrincipal;
+
+    private static final Logger LOG = LoggerFactory.getLogger(ReqContext.class);
+
 
     /**
      * Get a request context associated with current thread
@@ -87,7 +94,7 @@ public class ReqContext {
      * Set remote subject explicitly
      */
     public void setSubject(Subject subject) {
-        _subject = subject;	
+        _subject = subject;
     }
 
     /**
@@ -106,6 +113,24 @@ public class ReqContext {
         if (princs.size()==0) return null;
         return (Principal) (princs.toArray()[0]);
     }
+
+    public void setRealPrincipal(Principal realPrincipal) {
+        this.realPrincipal = realPrincipal;
+    }
+    /**
+     * The real principal associated with the subject.
+     */
+    public Principal realPrincipal() {
+        return this.realPrincipal;
+    }
+
+    /**
+     * Returns true if this request is an impersonation request.
+     * @return
+     */
+    public boolean isImpersonating() {
+        return this.realPrincipal != null;
+    }
     
     /**
      * request ID of this request
@@ -113,4 +138,5 @@ public class ReqContext {
     public Integer requestID() {
         return _reqID;
     }
+
 }

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}
```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes introduced in the diff file are directly related to security functionalities, specifically in the context of authentication and logging. The addition of a 'realPrincipal' field and methods related to it ('setRealPrincipal', 'realPrincipal', and 'isImpersonating') directly impact the authentication mechanism by introducing the concept of impersonation. This allows for distinguishing between the actual principal (the entity that has authenticated) and a potentially impersonated principal (the identity being assumed). This is a critical change as it affects how authentication and authorization decisions are made, potentially allowing for actions to be performed on behalf of another user. Furthermore, the introduction of logging capabilities ('LOG' variable and its initialization) could impact the system's security by providing a way to audit actions, which is essential for detecting and investigating security incidents. However, the security impact of logging depends on how it is implemented throughout the application, including what actions are logged and the level of detail provided. Given the context and the nature of the changes, they are directly relevant to the system's security posture.",
  "confidence": 9
}
```

Diff number: 6
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
index 64bec193b..532ebd726 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
@@ -116,7 +116,6 @@ public abstract class SaslTransportPlugin implements ITransportPlugin {
             TTransport trans = inProt.getTransport();
             //Sasl transport
             TSaslServerTransport saslTrans = (TSaslServerTransport)trans;
-
             //remote address
             TSocket tsocket = (TSocket)saslTrans.getUnderlyingTransport();
             Socket socket = tsocket.getSocket();
@@ -128,7 +127,7 @@ public abstract class SaslTransportPlugin implements ITransportPlugin {
             Subject remoteUser = new Subject();
             remoteUser.getPrincipals().add(new User(authId));
             req_context.setSubject(remoteUser);
-            
+
             //invoke service handler
             return wrapped.process(inProt, outProt);
         }

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The changes in the diff file consist of the removal of a blank line and the alteration of whitespace (changing a tab or spaces). These modifications do not impact any security functionalities directly. They do not alter the logic of authentication mechanisms, encryption algorithms, access control procedures, or the logging of security events. The changes are purely stylistic and do not affect the execution flow or the security posture of the system. Given that the context provided does not indicate any specific methods or security components being affected by these changes, it is clear that these modifications are unrelated to the system's security functionalities.",
  "confidence": 10
}
```

Diff number: 7
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java
index 0e1a3652a..ab06635fb 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java
@@ -90,8 +90,11 @@ public class SimpleTransportPlugin implements ITransportPlugin {
     /**
      * Connect to the specified server via framed transport 
      * @param transport The underlying Thrift transport.
+     * @param serverHost unused.
+     * @param asUser unused.
      */
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException {
+    @Override
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException {
         //create a framed transport
         TTransport conn = new TFramedTransport(transport);
 

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The modification in the SimpleTransportPlugin.java file involves an update to the 'connect' method signature, adding two new parameters: 'serverHost' and 'asUser'. While the comment indicates that these parameters are unused, their introduction suggests a future intention to utilize them, potentially for specifying the server to connect to and impersonating or specifying a user context for the connection. This could impact security by introducing new authentication or access control mechanisms, or by affecting how connections are established and managed. However, without the parameters being utilized in the current change, the immediate impact on security cannot be determined. Further analysis would be required once these parameters are actively used in the code to understand their implications fully. The addition of '@Override' annotation indicates that this method is implementing or overriding a method from an interface or superclass, which is a standard practice and does not directly impact security by itself.",
  "confidence": 7
}
```

Diff number: 8
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java b/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
index eaed61ff4..f547868f8 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
@@ -40,12 +40,12 @@ public class TBackoffConnect {
                                                               retryTimes);
     }
 
-    public TTransport doConnectWithRetry(ITransportPlugin transportPlugin, TTransport underlyingTransport, String host) throws IOException {
+    public TTransport doConnectWithRetry(ITransportPlugin transportPlugin, TTransport underlyingTransport, String host, String asUser) throws IOException {
         boolean connected = false;
         TTransport transportResult = null;
         while(!connected) {
             try {
-                transportResult = transportPlugin.connect(underlyingTransport, host);
+                transportResult = transportPlugin.connect(underlyingTransport, host, asUser);
                 connected = true;
             } catch (TTransportException ex) {
                 retryNext(ex);

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.util.Map;
import javax.security.auth.login.Configuration;
import org.apache.storm.Config;
import org.apache.storm.thrift.protocol.TBinaryProtocol;
import org.apache.storm.thrift.protocol.TProtocol;
import org.apache.storm.thrift.transport.TSocket;
import org.apache.storm.thrift.transport.TTransport;
import org.apache.storm.utils.ObjectReader;

public class ThriftClient implements AutoCloseable {
    protected TProtocol protocol;
    protected boolean retryForever = false;
    private TTransport transport;
    private String host;
    private Integer port;
    private Integer timeout;
    private Map conf;
    private ThriftConnectionType type;
    private String asUser;

    public ThriftClient(Map<String, Object> topoConf, ThriftConnectionType type, String host) {
        this(topoConf, type, host, null, null, null);
    }

    public ThriftClient(Map<String, Object> topoConf, ThriftConnectionType type, String host, Integer port, Integer timeout) {
        this(topoConf, type, host, port, timeout, null);
    }

    public ThriftClient(Map<String, Object> topoConf, ThriftConnectionType type, String host, Integer port, Integer timeout,
                        String asUser) {
        //create a socket with server
        if (host == null) {
            throw new IllegalArgumentException("host is not set");
        }

        if (port == null) {
            port = type.getPort(topoConf);
        }

        if (timeout == null) {
            timeout = type.getSocketTimeOut(topoConf);
        }

        if (port <= 0 && !type.isFake()) {
            throw new IllegalArgumentException("invalid port: " + port);
        }

        this.host = host;
        this.port = port;
        this.timeout = timeout;
        conf = topoConf;
        this.type = type;
        this.asUser = asUser;
        if (!type.isFake()) {
            reconnect();
        }
    }

    public synchronized TTransport transport() {
        return transport;
    }

    public synchronized void reconnect() {
        close();
        TSocket socket = null;
        try {
            socket = new TSocket(host, port);
            if (timeout != null) {
                socket.setTimeout(timeout);
            }

            //construct a transport plugin
            ITransportPlugin transportPlugin = ClientAuthUtils.getTransportPlugin(type, conf);

            //TODO get this from type instead of hardcoding to Nimbus.
            //establish client-server transport via plugin
            //do retries if the connect fails
            TBackoffConnect connectionRetry
                = new TBackoffConnect(
                ObjectReader.getInt(conf.get(Config.STORM_NIMBUS_RETRY_TIMES)),
                ObjectReader.getInt(conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL)),
                ObjectReader.getInt(conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)),
                    retryForever);
            transport = connectionRetry.doConnectWithRetry(transportPlugin, socket, host, asUser);
        } catch (Exception ex) {
            // close the socket, which releases connection if it has created any.
            if (socket != null) {
                try {
                    socket.close();
                } catch (Exception e) {
                    //ignore
                }
            }
            throw new RuntimeException(ex);
        }
        protocol = null;
        if (transport != null) {
            protocol = new TBinaryProtocol(transport);
        }
    }

    @Override
    public synchronized void close() {
        if (transport != null) {
            transport.close();
            transport = null;
            protocol = null;
        }
    }
}

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The code changes in the diff file directly impact the authentication mechanism of the system. Specifically, the modification adds an 'asUser' parameter to the 'doConnectWithRetry' method and its call within the 'TBackoffConnect' class. This parameter is then passed to the 'transportPlugin.connect' method, which suggests that the system's authentication flow now includes the capability to specify the user context under which the connection should be established. This change could potentially allow for more granular control over access permissions and enhance the system's ability to enforce security policies based on user identity. However, without knowing the exact implementation of the 'transportPlugin.connect' method and how the 'asUser' parameter is used within it, it's difficult to fully assess the impact on security. If improperly implemented, this feature could introduce vulnerabilities, such as privilege escalation or bypassing of access controls, depending on how user identities are validated and used within the system.",
  "confidence": 8
}
```

Diff number: 9
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java b/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java
index ec337d990..9f77ab92d 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java
@@ -39,12 +39,17 @@ public class ThriftClient {
     private Integer _timeout;
     private Map _conf;
     private ThriftConnectionType _type;
+    private String _asUser;
 
     public ThriftClient(Map storm_conf, ThriftConnectionType type, String host) {
-        this(storm_conf, type, host, null, null);
+        this(storm_conf, type, host, null, null, null);
     }
 
-    public ThriftClient(Map storm_conf, ThriftConnectionType type, String host, Integer port, Integer timeout) {
+    public ThriftClient(Map storm_conf, ThriftConnectionType type, String host, Integer port, Integer timeout){
+        this(storm_conf, type, host, port, timeout, null);
+    }
+
+    public ThriftClient(Map storm_conf, ThriftConnectionType type, String host, Integer port, Integer timeout, String asUser) {
         //create a socket with server
         if (host==null) {
             throw new IllegalArgumentException("host is not set");
@@ -63,6 +68,7 @@ public class ThriftClient {
         _timeout = timeout;
         _conf = storm_conf;
         _type = type;
+        _asUser = asUser;
         reconnect();
     }
 
@@ -94,7 +100,7 @@ public class ThriftClient {
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_TIMES)),
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL)),
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)));
-            _transport = connectionRetry.doConnectWithRetry(transportPlugin, underlyingTransport, _host);
+            _transport = connectionRetry.doConnectWithRetry(transportPlugin, underlyingTransport, _host, _asUser);
         } catch (IOException ex) {
             throw new RuntimeException(ex);
         }

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The introduction of the '_asUser' variable and its integration into the ThriftClient constructor and the connection retry mechanism could potentially impact the system's security posture. This change suggests that the system now supports operations on behalf of another user, which is a significant modification to the authentication and authorization mechanisms. If not properly implemented and validated, this could lead to privilege escalation, where a user might execute actions with the permissions of another user without proper authorization. The security impact of this change depends on how the '_asUser' variable is used throughout the system, how the user's identity and permissions are verified, and whether there are sufficient checks and balances to prevent unauthorized use of this feature. Without further context on how authentication and authorization are handled, and how '_asUser' is validated, it's challenging to fully assess the security implications. However, given the nature of the change, it warrants a closer examination of the authentication flow and permission checks associated with the '_asUser' functionality.",
  "confidence": 7
}
```

Diff number: 10
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
new file mode 100644
index 000000000..1e947ae67
--- /dev/null
+++ b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
@@ -0,0 +1,148 @@
+package backtype.storm.security.auth.authorizer;
+
+import backtype.storm.Config;
+import backtype.storm.security.auth.*;
+import com.google.common.collect.ImmutableSet;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.*;
+
+
+public class ImpersonationAuthorizer implements IAuthorizer {
+    private static final Logger LOG = LoggerFactory.getLogger(ImpersonationAuthorizer.class);
+
+    protected Map<String, ImpersonationACL> userImpersonationACL;
+    protected IPrincipalToLocal _ptol;
+    protected IGroupMappingServiceProvider _groupMappingProvider;
+
+    @Override
+    public void prepare(Map conf) {
+        userImpersonationACL = new HashMap<String, ImpersonationACL>();
+
+        Map<String, Map<String, List<String>>> userToHostAndGroup = (Map<String, Map<String, List<String>>>) conf.get(Config.NIMBUS_IMPERSONATION_ACL);
+
+        if (userToHostAndGroup != null) {
+            for (String user : userToHostAndGroup.keySet()) {
+                Set<String> groups = ImmutableSet.copyOf(userToHostAndGroup.get(user).get("groups"));
+                Set<String> hosts = ImmutableSet.copyOf(userToHostAndGroup.get(user).get("hosts"));
+                userImpersonationACL.put(user, new ImpersonationACL(user, groups, hosts));
+            }
+        }
+
+        _ptol = AuthUtils.GetPrincipalToLocalPlugin(conf);
+        _groupMappingProvider = AuthUtils.GetGroupMappingServiceProviderPlugin(conf);
+    }
+
+    @Override
+    public boolean permit(ReqContext context, String operation, Map topology_conf) {
+        if (!context.isImpersonating()) {
+            LOG.debug("Not an impersonation attempt.");
+            return true;
+        }
+
+        String impersonatingPrincipal = context.realPrincipal().getName();
+        String impersonatingUser = _ptol.toLocal(context.realPrincipal());
+        String userBeingImpersonated = _ptol.toLocal(context.principal());
+        InetAddress remoteAddress = context.remoteAddress();
+
+        LOG.info("user = {}, principal = {} is attmepting to impersonate user = {} for operation = {} from host = {}",
+                impersonatingUser, impersonatingPrincipal, userBeingImpersonated, operation, remoteAddress);
+
+        /**
+         * no config is present for impersonating principal or user, do not permit impersonation.
+         */
+        if (!userImpersonationACL.containsKey(impersonatingPrincipal) && !userImpersonationACL.containsKey(impersonatingUser)) {
+            LOG.info("user = {}, principal = {} is trying to impersonate user {}, but config {} does not have entry for impersonating user or principal." +
+                    "Please see SECURITY.MD to learn how to configure users for impersonation."
+                    , impersonatingUser, impersonatingPrincipal, userBeingImpersonated, Config.NIMBUS_IMPERSONATION_ACL);
+            return false;
+        }
+
+        ImpersonationACL principalACL = userImpersonationACL.get(impersonatingPrincipal);
+        ImpersonationACL userACL = userImpersonationACL.get(impersonatingUser);
+
+        Set<String> authorizedHosts = new HashSet<String>();
+        Set<String> authorizedGroups = new HashSet<String>();
+
+        if (principalACL != null) {
+            authorizedHosts.addAll(principalACL.authorizedHosts);
+            authorizedGroups.addAll(principalACL.authorizedGroups);
+        }
+
+        if (userACL != null) {
+            authorizedHosts.addAll(userACL.authorizedHosts);
+            authorizedGroups.addAll(userACL.authorizedGroups);
+        }
+
+        LOG.debug("user = {}, principal = {} is allowed to impersonate groups = {} from hosts = {} ",
+                impersonatingUser, impersonatingPrincipal, authorizedGroups, authorizedHosts);
+
+        if (!isAllowedToImpersonateFromHost(authorizedHosts, remoteAddress)) {
+            LOG.info("user = {}, principal = {} is not allowed to impersonate from host {} ",
+                    impersonatingUser, impersonatingPrincipal, remoteAddress);
+            return false;
+        }
+
+        if (!isAllowedToImpersonateUser(authorizedGroups, userBeingImpersonated)) {
+            LOG.info("user = {}, principal = {} is not allowed to impersonate any group that user {} is part of.",
+                    impersonatingUser, impersonatingPrincipal, userBeingImpersonated);
+            return false;
+        }
+
+        LOG.info("Allowing impersonation of user {} by user {}", userBeingImpersonated, impersonatingUser);
+        return true;
+    }
+
+    private boolean isAllowedToImpersonateFromHost(Set<String> authorizedHosts, InetAddress remoteAddress) {
+        return authorizedHosts.contains(remoteAddress.getCanonicalHostName()) ||
+                authorizedHosts.contains(remoteAddress.getHostName()) ||
+                authorizedHosts.contains(remoteAddress.getHostAddress());
+    }
+
+    private boolean isAllowedToImpersonateUser(Set<String> authorizedGroups, String userBeingImpersonated) {
+        Set<String> groups = null;
+        try {
+            groups = _groupMappingProvider.getGroups(userBeingImpersonated);
+        } catch (IOException e) {
+            throw new RuntimeException("failed to get groups for user " + userBeingImpersonated);
+        }
+
+        if (groups == null || groups.isEmpty()) {
+            return false;
+        }
+
+        for (String group : groups) {
+            if (authorizedGroups.contains(group)) {
+                return true;
+            }
+        }
+
+        return false;
+    }
+
+    protected class ImpersonationACL {
+        public String impersonatingUser;
+        //Groups this user is authorized to impersonate.
+        public Set<String> authorizedGroups;
+        //Hosts this user is authorized to impersonate from.
+        public Set<String> authorizedHosts;
+
+        private ImpersonationACL(String impersonatingUser, Set<String> authorizedGroups, Set<String> authorizedHosts) {
+            this.impersonatingUser = impersonatingUser;
+            this.authorizedGroups = authorizedGroups;
+            this.authorizedHosts = authorizedHosts;
+        }
+
+        @Override
+        public String toString() {
+            return "ImpersonationACL{" +
+                    "impersonatingUser='" + impersonatingUser + '\'' +
+                    ", authorizedGroups=" + authorizedGroups +
+                    ", authorizedHosts=" + authorizedHosts +
+                    '}';
+        }
+    }
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}
```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The provided diff introduces a new ImpersonationAuthorizer class within the storm-core security authentication and authorization framework. This class is directly involved in the security functionality of the system, specifically in handling impersonation authorization. Impersonation in a distributed system like Apache Storm is a critical security feature that allows one entity (a user or a service) to perform actions on behalf of another entity. The ImpersonationAuthorizer class checks if an impersonating user or principal has the necessary permissions to impersonate another user based on predefined ACLs (Access Control Lists). This involves validating the impersonating user's or principal's rights against the configured impersonation ACLs, determining if the impersonation attempt is allowed from the originating host, and if the impersonating entity is part of authorized groups. The security implications of this feature are significant as improper implementation or configuration could lead to unauthorized access or privilege escalation. For instance, if the ACLs are not correctly configured or if the checks are not properly enforced, it could allow an attacker to impersonate a more privileged user, thereby gaining unauthorized access to resources or operations they should not have access to. The introduction of this feature increases the system's complexity and the potential attack surface. Therefore, it is crucial that this feature is thoroughly reviewed, tested, and securely configured to prevent abuse. The confidence in this assessment is high due to the direct involvement of the new class in handling security-sensitive impersonation functionality.",
  "confidence": 10
}
```

Diff number: 12
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
index cb6857906..ad642d8b3 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
@@ -51,10 +51,10 @@ public class DigestSaslTransportPlugin extends SaslTransportPlugin {
     }
 
     @Override
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException, IOException {
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {
         ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);
-        TSaslClientTransport wrapper_transport = new TSaslClientTransport(DIGEST, 
-                null, 
+        TSaslClientTransport wrapper_transport = new TSaslClientTransport(DIGEST,
+                null,
                 AuthUtils.SERVICE, 
                 serverHost,
                 null,

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The code changes in the DigestSaslTransportPlugin.java file modify the signature of the 'connect' method by adding an 'asUser' parameter, which is not used within the provided diff. This modification suggests an intention to alter the authentication mechanism, potentially to support impersonation or specify the user context under which the connection should be established. Although the direct impact of this change on security cannot be fully assessed without understanding how the 'asUser' parameter is intended to be used in the authentication process, it indicates a modification in the way authentication might be handled. Given that authentication is a critical security component, any changes to its mechanisms should be carefully reviewed to ensure they do not introduce vulnerabilities, such as improper authentication or elevation of privilege risks. However, without additional context on how the 'asUser' parameter affects the authentication flow or is validated, it's challenging to definitively classify the security implications.",
  "confidence": 7
}
```

Diff number: 13
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
index a0e4839df..0ff3d35bd 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
@@ -20,8 +20,13 @@ package backtype.storm.security.auth.digest;
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
+
+import backtype.storm.security.auth.ReqContext;
+import backtype.storm.security.auth.SaslTransportPlugin;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
+import javax.security.auth.Subject;
 import javax.security.auth.callback.Callback;
 import javax.security.auth.callback.CallbackHandler;
 import javax.security.auth.callback.NameCallback;
@@ -107,8 +112,19 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private void handleAuthorizeCallback(AuthorizeCallback ac) {
         String authenticationID = ac.getAuthenticationID();
-        LOG.debug("Successfully authenticated client: authenticationID=" + authenticationID);
-        ac.setAuthorizedID(authenticationID);
+        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID + " authorizationID= " + ac.getAuthorizationID());
+
+        //if authorizationId is not set, set it to authenticationId.
+        if(ac.getAuthorizationID() == null) {
+            ac.setAuthorizedID(authenticationID);
+        }
+
+        //When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We
+        //add the authNid as the real user in reqContext's subject which will be used during authorization.
+        if(!authenticationID.equals(ac.getAuthorizationID())) {
+            ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        }
+
         ac.setAuthorized(true);
     }
 }

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file directly impact the authentication mechanism of the system. Specifically, the modifications to the ServerCallbackHandler.java file involve changes to how authentication and authorization IDs are handled during the authentication process. The logging level change from debug to info for logging the successful authentication of a client increases the visibility of authentication events, which is relevant for security monitoring and auditing purposes. Additionally, the introduction of logic to handle cases where the authentication ID (authNid) and authorization ID (authZid) are not equal introduces a new pathway for setting the authorized ID and potentially for impersonation checks. This could have implications for how identity and access management is enforced within the system, particularly in scenarios involving impersonation or delegation of rights. The addition of a mechanism to add the authenticating ID as the real user in the request context's subject for authorization purposes further emphasizes the security relevance, as it directly pertains to how user identities are managed and verified within the system.",
  "confidence": 9
}
```

Diff number: 14
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
index 451f87bb4..b6cccadd2 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
@@ -32,6 +32,8 @@ import javax.security.auth.login.AppConfigurationEntry;
 import javax.security.auth.login.Configuration;
 import javax.security.auth.login.LoginException;
 import javax.security.sasl.Sasl;
+
+import org.apache.commons.lang.StringUtils;
 import org.apache.thrift.transport.TSaslClientTransport;
 import org.apache.thrift.transport.TSaslServerTransport;
 import org.apache.thrift.transport.TTransport;
@@ -51,7 +53,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
 
     public TTransportFactory getServerTransportFactory() throws IOException {
         //create an authentication callback handler
-        CallbackHandler server_callback_handler = new ServerCallbackHandler(login_conf);
+        CallbackHandler server_callback_handler = new ServerCallbackHandler(login_conf, storm_conf);
         
         //login our principal
         Subject subject = null;
@@ -92,7 +94,8 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
         return wrapFactory;
     }
 
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException, IOException {
+    @Override
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {
         //create an authentication callback handler
         ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);
         
@@ -114,7 +117,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
                         +AuthUtils.LOGIN_CONTEXT_CLIENT+"\" in login configuration file "+ login_conf);
         }
 
-        final String principal = getPrincipal(subject); 
+        final String principal = StringUtils.isBlank(asUser) ? getPrincipal(subject) : asUser;
         String serviceName = AuthUtils.get(login_conf, AuthUtils.LOGIN_CONTEXT_CLIENT, "serviceName");
         if (serviceName == null) {
             serviceName = AuthUtils.SERVICE; 
@@ -138,7 +141,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
                     new PrivilegedExceptionAction<Void>() {
                 public Void run() {
                     try {
-                        LOG.debug("do as:"+ principal);
+                        LOG.info("do as:"+ principal);
                         sasalTransport.open();
                     }
                     catch (Exception e) {

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file directly impact the security functionalities of the system, specifically the authentication mechanism related to Kerberos. The introduction of a new parameter 'asUser' in the 'connect' method allows specifying a different user for the Kerberos authentication, which could potentially alter the authentication flow and impact the principle of least privilege if not properly managed. Additionally, changing the logging level from debug to info for logging the principal being authenticated could lead to sensitive information being logged at a less secure level, potentially exposing user information in logs that are more accessible. The inclusion of 'org.apache.commons.lang.StringUtils' and its use to check if 'asUser' is blank before deciding which principal to use for authentication further indicates modifications in how authentication details are handled, emphasizing the security relevance of these changes.",
  "confidence": 9
}
```

Diff number: 15
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
index 9dc75c4db..7b143f0d6 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
@@ -18,22 +18,19 @@
 
 package backtype.storm.security.auth.kerberos;
 
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
+import backtype.storm.security.auth.AuthUtils;
+import backtype.storm.security.auth.ReqContext;
+import backtype.storm.security.auth.SaslTransportPlugin;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import javax.security.auth.callback.Callback;
-import javax.security.auth.callback.CallbackHandler;
-import javax.security.auth.callback.NameCallback;
-import javax.security.auth.callback.PasswordCallback;
-import javax.security.auth.callback.UnsupportedCallbackException;
+
+import javax.security.auth.Subject;
+import javax.security.auth.callback.*;
 import javax.security.auth.login.AppConfigurationEntry;
 import javax.security.auth.login.Configuration;
 import javax.security.sasl.AuthorizeCallback;
-import javax.security.sasl.RealmCallback;
-
-import backtype.storm.security.auth.AuthUtils;
+import java.io.IOException;
+import java.util.Map;
 
 /**
  * SASL server side callback handler
@@ -43,7 +40,7 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private String userName;
 
-    public ServerCallbackHandler(Configuration configuration) throws IOException {
+    public ServerCallbackHandler(Configuration configuration, Map stormConf) throws IOException {
         if (configuration==null) return;
 
         AppConfigurationEntry configurationEntries[] = configuration.getAppConfigurationEntry(AuthUtils.LOGIN_CONTEXT_SERVER);
@@ -52,6 +49,7 @@ public class ServerCallbackHandler implements CallbackHandler {
             LOG.error(errorMessage);
             throw new IOException(errorMessage);
         }
+
     }
 
     public void handle(Callback[] callbacks) throws UnsupportedCallbackException {
@@ -78,9 +76,19 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private void handleAuthorizeCallback(AuthorizeCallback ac) {
         String authenticationID = ac.getAuthenticationID();
-        LOG.debug("Successfully authenticated client: authenticationID=" + authenticationID);
-        ac.setAuthorized(true);
+        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID + " authorizationID= " + ac.getAuthorizationID());
+
+        //if authorizationId is not set, set it to authenticationId.
+        if(ac.getAuthorizationID() == null) {
+            ac.setAuthorizedID(authenticationID);
+        }
 
-        ac.setAuthorizedID(authenticationID);
+        //When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We
+        //add the authNid as the real user in reqContext's subject which will be used during authorization.
+        if(!ac.getAuthenticationID().equals(ac.getAuthorizationID())) {
+            ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        }
+
+        ac.setAuthorized(true);
     }
 }

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file are directly related to the authentication mechanism of the system, specifically within the Kerberos ServerCallbackHandler. The modifications include the addition of a new parameter to the ServerCallbackHandler constructor, which now accepts a Map named stormConf, potentially to configure or utilize additional settings during the authentication process. Furthermore, the handling of AuthorizeCallback has been altered to log more detailed information, including both the authenticationID and authorizationID, which enhances the visibility of authentication events, a critical aspect of security monitoring. Most notably, the code introduces logic to handle scenarios where the authentication ID (authNid) and authorization ID (authZid) are not the same, which implies an attempt at impersonation. In such cases, the authenticating user (authNid) is set as the 'real user' in the ReqContext's subject. This change is significant as it directly impacts how user identities are managed and verified during the authentication process, potentially affecting the system's ability to securely manage user sessions and access controls. The addition of this impersonation detection and handling logic could either strengthen the system's security by providing a clearer audit trail and enforcing stricter checks or introduce new vulnerabilities if not implemented correctly, especially in how it interacts with the broader authorization framework.",
  "confidence": 9
}
```

Diff number: 16
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java b/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
index 3218e4994..b2a2a7d85 100644
--- a/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
+++ b/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
@@ -41,7 +41,7 @@ public class DRPCClient extends ThriftClient implements DistributedRPC.Iface {
     }
 
     public DRPCClient(Map conf, String host, int port, Integer timeout) throws TTransportException {
-        super(conf, ThriftConnectionType.DRPC, host, port, timeout);
+        super(conf, ThriftConnectionType.DRPC, host, port, timeout, null);
         this.host = host;
         this.port = port;
         this.client = new DistributedRPC.Client(_protocol);

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The modification in the DRPCClient constructor involves the addition of a 'null' parameter to the super constructor call. This change is potentially security relevant because it alters the initialization parameters of a ThriftClient, which is used for remote procedure calls. The specific impact of this 'null' parameter is not clear without further context, such as the implementation of the ThriftClient constructor and how it handles this new parameter. If this 'null' parameter is related to security features such as authentication credentials, encryption settings, or specific security protocols, it could potentially weaken or bypass security mechanisms. However, without additional information about the handling of this parameter and the overall security architecture of the system, it is difficult to precisely determine the security implications. Therefore, further analysis is required to assess whether this change could lead to security vulnerabilities, such as improper authentication, lack of encryption, or other security risks.",
  "confidence": 5
}
```

Diff number: 18
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
new file mode 100644
index 000000000..4d9f5da7b
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
@@ -0,0 +1,289 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.task.OutputCollector;
+import backtype.storm.task.TopologyContext;
+import backtype.storm.tuple.Tuple;
+import backtype.storm.topology.base.BaseRichBolt;
+import backtype.storm.topology.OutputFieldsDeclarer;
+import org.apache.storm.hive.common.HiveWriter;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import org.apache.hive.hcatalog.streaming.*;
+import org.apache.storm.hive.common.HiveOptions;
+import org.apache.storm.hive.common.HiveUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.Map.Entry;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.io.IOException;
+
+public class HiveBolt extends  BaseRichBolt {
+    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
+    private OutputCollector collector;
+    private HiveOptions options;
+    private Integer currentBatchSize;
+    private ExecutorService callTimeoutPool;
+    private transient Timer heartBeatTimer;
+    private Boolean kerberosEnabled = false;
+    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
+    private UserGroupInformation ugi = null;
+    HashMap<HiveEndPoint, HiveWriter> allWriters;
+
+    public HiveBolt(HiveOptions options) {
+        this.options = options;
+        this.currentBatchSize = 0;
+    }
+
+    @Override
+    public void prepare(Map conf, TopologyContext topologyContext, OutputCollector collector)  {
+        try {
+            if(options.getKerberosPrincipal() == null && options.getKerberosKeytab() == null) {
+                kerberosEnabled = false;
+            } else if(options.getKerberosPrincipal() != null && options.getKerberosKeytab() != null) {
+                kerberosEnabled = true;
+            } else {
+                throw new IllegalArgumentException("To enable Kerberos, need to set both KerberosPrincipal " +
+                                                   " & KerberosKeytab");
+            }
+
+            if (kerberosEnabled) {
+                try {
+                    ugi = HiveUtils.authenticate(options.getKerberosKeytab(), options.getKerberosPrincipal());
+                } catch(HiveUtils.AuthenticationFailed ex) {
+                    LOG.error("Hive Kerberos authentication failed " + ex.getMessage(), ex);
+                    throw new IllegalArgumentException(ex);
+                }
+            }
+            this.collector = collector;
+            allWriters = new HashMap<HiveEndPoint,HiveWriter>();
+            String timeoutName = "hive-bolt-%d";
+            this.callTimeoutPool = Executors.newFixedThreadPool(1,
+                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
+            heartBeatTimer = new Timer();
+            setupHeartBeatTimer();
+        } catch(Exception e) {
+            LOG.warn("unable to make connection to hive ",e);
+        }
+    }
+
+    @Override
+    public void execute(Tuple tuple) {
+        try {
+            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
+            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
+            HiveWriter writer = getOrCreateWriter(endPoint);
+            if(timeToSendHeartBeat.compareAndSet(true, false)) {
+                enableHeartBeatOnAllWriters();
+            }
+            writer.write(options.getMapper().mapRecord(tuple));
+            currentBatchSize++;
+            if(currentBatchSize >= options.getBatchSize()) {
+                flushAllWriters();
+                currentBatchSize = 0;
+            }
+            collector.ack(tuple);
+        } catch(Exception e) {
+            this.collector.reportError(e);
+            collector.fail(tuple);
+            flushAndCloseWriters();
+        }
+    }
+
+    @Override
+    public void declareOutputFields(OutputFieldsDeclarer declarer) {
+
+    }
+
+    @Override
+    public void cleanup() {
+        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
+            try {
+                HiveWriter w = entry.getValue();
+                LOG.info("Flushing writer to {}", w);
+                w.flush(false);
+                LOG.info("Closing writer to {}", w);
+                w.close();
+            } catch (Exception ex) {
+                LOG.warn("Error while closing writer to " + entry.getKey() +
+                         ". Exception follows.", ex);
+                if (ex instanceof InterruptedException) {
+                    Thread.currentThread().interrupt();
+                }
+            }
+        }
+
+        ExecutorService toShutdown[] = {callTimeoutPool};
+        for (ExecutorService execService : toShutdown) {
+            execService.shutdown();
+            try {
+                while (!execService.isTerminated()) {
+                    execService.awaitTermination(
+                                 options.getCallTimeOut(), TimeUnit.MILLISECONDS);
+                }
+            } catch (InterruptedException ex) {
+                LOG.warn("shutdown interrupted on " + execService, ex);
+            }
+        }
+        callTimeoutPool = null;
+        super.cleanup();
+        LOG.info("Hive Bolt stopped");
+    }
+
+
+    private void setupHeartBeatTimer() {
+        if(options.getHeartBeatInterval()>0) {
+            heartBeatTimer.schedule(new TimerTask() {
+                    @Override
+                    public void run() {
+                        timeToSendHeartBeat.set(true);
+                        setupHeartBeatTimer();
+                    }
+                }, options.getHeartBeatInterval() * 1000);
+        }
+    }
+
+    private void flushAllWriters()
+        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
+        for(HiveWriter writer: allWriters.values()) {
+            writer.flush(true);
+        }
+    }
+
+    /**
+     * Closes all writers and remove them from cache
+     * @return number of writers retired
+     */
+    private void closeAllWriters() {
+        try {
+            //1) Retire writers
+            for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+                entry.getValue().close();
+            }
+            //2) Clear cache
+            allWriters.clear();
+        } catch(Exception e) {
+            LOG.warn("unable to close writers. ", e);
+        }
+    }
+
+    private void flushAndCloseWriters() {
+        try {
+            flushAllWriters();
+        } catch(Exception e) {
+            LOG.warn("unable to flush hive writers. ", e);
+        } finally {
+            closeAllWriters();
+        }
+    }
+
+    private void enableHeartBeatOnAllWriters() {
+        for (HiveWriter writer : allWriters.values()) {
+            writer.setHeartBeatNeeded();
+        }
+    }
+
+    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
+        throws HiveWriter.ConnectFailure, InterruptedException {
+        try {
+            HiveWriter writer = allWriters.get( endPoint );
+            if( writer == null ) {
+                LOG.debug("Creating Writer to Hive end point : " + endPoint);
+                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options);
+                if(allWriters.size() > options.getMaxOpenConnections()){
+                    int retired = retireIdleWriters();
+                    if(retired==0) {
+                        retireEldestWriter();
+                    }
+                }
+                allWriters.put(endPoint, writer);
+            }
+            return writer;
+        } catch (HiveWriter.ConnectFailure e) {
+            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
+            throw e;
+        }
+    }
+
+    /**
+     * Locate writer that has not been used for longest time and retire it
+     */
+    private void retireEldestWriter() {
+        long oldestTimeStamp = System.currentTimeMillis();
+        HiveEndPoint eldest = null;
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(entry.getValue().getLastUsed() < oldestTimeStamp) {
+                eldest = entry.getKey();
+                oldestTimeStamp = entry.getValue().getLastUsed();
+            }
+        }
+        try {
+            LOG.info("Closing least used Writer to Hive end point : " + eldest);
+            allWriters.remove(eldest).close();
+        } catch (IOException e) {
+            LOG.warn("Failed to close writer for end point: " + eldest, e);
+        } catch (InterruptedException e) {
+            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
+            Thread.currentThread().interrupt();
+        }
+    }
+
+    /**
+     * Locate all writers past idle timeout and retire them
+     * @return number of writers retired
+     */
+    private int retireIdleWriters() {
+        int count = 0;
+        long now = System.currentTimeMillis();
+        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();
+
+        //1) Find retirement candidates
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
+                ++count;
+                retirees.add(entry.getKey());
+            }
+        }
+        //2) Retire them
+        for(HiveEndPoint ep : retirees) {
+            try {
+                LOG.info("Closing idle Writer to Hive end point : {}", ep);
+                allWriters.remove(ep).close();
+            } catch (IOException e) {
+                LOG.warn("Failed to close writer for end point: {}. Error: "+ ep, e);
+            } catch (InterruptedException e) {
+                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
+                Thread.currentThread().interrupt();
+            }
+        }
+        return count;
+    }
+
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The code changes introduce a new HiveBolt class that includes functionality for handling Kerberos authentication (kerberosEnabled flag, options.getKerberosPrincipal(), options.getKerberosKeytab(), and the call to HiveUtils.authenticate()). Kerberos is a network authentication protocol designed to provide strong authentication for client/server applications by using secret-key cryptography. The correct implementation and handling of Kerberos authentication are critical for ensuring the security of the system, especially in environments where sensitive data is processed or accessed. The changes also involve managing connections to Hive, including the creation and flushing of writers, which could impact data integrity and confidentiality if not properly handled. Additionally, the use of a heartbeat mechanism (setupHeartBeatTimer()) could have implications for the availability aspect of security, ensuring that connections remain alive. However, without a detailed review of how these authentication credentials are stored, managed, and transmitted, and how the heartbeat mechanism is implemented, it's challenging to fully assess the impact on the system's security posture.",
  "confidence": 7
}
```

Diff number: 19
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/DelimitedRecordHiveMapper.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/DelimitedRecordHiveMapper.java
new file mode 100644
index 000000000..d5167952a
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/DelimitedRecordHiveMapper.java
@@ -0,0 +1,143 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.bolt.mapper;
+
+
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Tuple;
+import storm.trident.tuple.TridentTuple;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.apache.hive.hcatalog.streaming.DelimitedInputWriter;
+import org.apache.hive.hcatalog.streaming.HiveEndPoint;
+import org.apache.hive.hcatalog.streaming.RecordWriter;
+import org.apache.hive.hcatalog.streaming.StreamingException;
+import org.apache.hive.hcatalog.streaming.TransactionBatch;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Date;
+import java.text.SimpleDateFormat;
+import java.io.IOException;
+
+public class DelimitedRecordHiveMapper implements HiveMapper {
+    private static final Logger LOG = LoggerFactory.getLogger(DelimitedRecordHiveMapper.class);
+    private static final String DEFAULT_FIELD_DELIMITER = ",";
+    private Fields columnFields;
+    private Fields partitionFields;
+    private String[] columnNames;
+    private String timeFormat;
+    private String fieldDelimiter = DEFAULT_FIELD_DELIMITER;
+    private SimpleDateFormat parseDate;
+
+    public DelimitedRecordHiveMapper() {
+    }
+
+    public DelimitedRecordHiveMapper withColumnFields(Fields columnFields) {
+        this.columnFields = columnFields;
+        List<String> tempColumnNamesList = this.columnFields.toList();
+        columnNames = new String[tempColumnNamesList.size()];
+        tempColumnNamesList.toArray(columnNames);
+        return this;
+    }
+
+    public DelimitedRecordHiveMapper withPartitionFields(Fields partitionFields) {
+        this.partitionFields = partitionFields;
+        return this;
+    }
+
+    public DelimitedRecordHiveMapper withFieldDelimiter(String delimiter){
+        this.fieldDelimiter = delimiter;
+        return this;
+    }
+
+    public DelimitedRecordHiveMapper withTimeAsPartitionField(String timeFormat) {
+        this.timeFormat = timeFormat;
+        parseDate = new SimpleDateFormat(timeFormat);
+        return this;
+    }
+
+    @Override
+    public RecordWriter createRecordWriter(HiveEndPoint endPoint)
+        throws StreamingException, IOException, ClassNotFoundException {
+        return new DelimitedInputWriter(columnNames, fieldDelimiter,endPoint);
+    }
+
+    @Override
+    public void write(TransactionBatch txnBatch, Tuple tuple)
+        throws StreamingException, IOException, InterruptedException {
+        txnBatch.write(mapRecord(tuple));
+    }
+
+    @Override
+    public List<String> mapPartitions(Tuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(Tuple tuple) {
+        StringBuilder builder = new StringBuilder();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                builder.append(tuple.getValueByField(field));
+                builder.append(fieldDelimiter);
+            }
+        }
+        return builder.toString().getBytes();
+    }
+
+    @Override
+    public List<String> mapPartitions(TridentTuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(TridentTuple tuple) {
+        StringBuilder builder = new StringBuilder();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                builder.append(tuple.getValueByField(field));
+                builder.append(fieldDelimiter);
+            }
+        }
+        return builder.toString().getBytes();
+    }
+
+    private String getPartitionsByTimeFormat() {
+        Date d = new Date();
+        return parseDate.format(d.getTime());
+    }
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_not_security_relevant",
  "reason": "The provided diff file introduces a new Java class, DelimitedRecordHiveMapper, which is primarily concerned with mapping and formatting data for insertion into a Hive database. The class includes functionalities for setting column and partition fields, customizing field delimiters, and handling date formats for partitioning. The security relevance of these changes is minimal as they do not directly impact critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. However, the manipulation of data formats and the potential inclusion of time-based data partitioning could indirectly affect the system's security posture if not properly validated or if used in conjunction with sensitive data without appropriate safeguards. The absence of explicit security-related functionalities or modifications within the diff suggests that these changes are unlikely to have a direct impact on the system's security, but a thorough analysis of how this class interacts with the rest of the system would be necessary to fully assess any potential indirect security implications.",
  "confidence": 7
}
```

Diff number: 20
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/HiveMapper.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/HiveMapper.java
new file mode 100644
index 000000000..a3b5531ed
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/HiveMapper.java
@@ -0,0 +1,81 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.bolt.mapper;
+
+
+import backtype.storm.tuple.Tuple;
+import storm.trident.tuple.TridentTuple;
+import java.util.List;
+import org.apache.hive.hcatalog.streaming.HiveEndPoint;
+import org.apache.hive.hcatalog.streaming.RecordWriter;
+import org.apache.hive.hcatalog.streaming.TransactionBatch;
+import org.apache.hive.hcatalog.streaming.StreamingException;
+import java.io.Serializable;
+
+import java.io.IOException;
+
+/**
+ * Maps a <code>backtype.storm.tuple.Tupe</code> object
+ * to a row in an Hive table.
+ */
+public interface HiveMapper extends Serializable {
+
+    /**
+     * Given a endPoint, returns a RecordWriter with columnNames.
+     *
+     * @param tuple
+     * @return
+     */
+
+    RecordWriter createRecordWriter(HiveEndPoint endPoint)
+        throws StreamingException, IOException, ClassNotFoundException;
+
+    void write(TransactionBatch txnBatch, Tuple tuple)
+        throws StreamingException, IOException, InterruptedException;
+
+    /**
+     * Given a tuple, return a hive partition values list.
+     *
+     * @param tuple
+     * @return List<String>
+     */
+    List<String> mapPartitions(Tuple tuple);
+
+    /**
+     * Given a tuple, maps to a HiveRecord based on columnFields
+     * @Param Tuple
+     * @return byte[]
+     */
+    byte[] mapRecord(Tuple tuple);
+
+    /**
+     * Given a TridetnTuple, return a hive partition values list.
+     *
+     * @param TridentTuple
+     * @return List<String>
+     */
+    List<String> mapPartitions(TridentTuple tuple);
+
+    /**
+     * Given a TridentTuple, maps to a HiveRecord based on columnFields
+     * @Param TridentTuple
+     * @return byte[]
+     */
+    byte[] mapRecord(TridentTuple tuple);
+
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The introduction of the HiveMapper interface in the Apache Storm Hive integration layer does not directly modify any explicit security functionalities such as authentication, encryption, or access control mechanisms. However, the methods defined within this interface, particularly those responsible for creating RecordWriters, writing to TransactionBatches, and mapping records and partitions, could potentially impact the system's security posture. The methods involve data processing and interaction with Hive, which could be leveraged to inject malicious data or exploit serialization vulnerabilities if not properly implemented or if the input data is not adequately validated. Additionally, the handling of IOExceptions, StreamingExceptions, and InterruptedExceptions could influence the robustness of error handling and logging practices, indirectly affecting the system's resilience to attacks. Without further context on how these methods are implemented and used, a definitive assessment of their security impact cannot be made, but the potential for security-relevant implications exists.",
  "confidence": 7
}
```

Diff number: 21
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java
new file mode 100644
index 000000000..ce3e4756b
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java
@@ -0,0 +1,132 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.bolt.mapper;
+
+
+import backtype.storm.tuple.Fields;
+import storm.trident.tuple.TridentTuple;
+import backtype.storm.tuple.Tuple;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hive.hcatalog.streaming.HiveEndPoint;
+import org.apache.hive.hcatalog.streaming.RecordWriter;
+import org.apache.hive.hcatalog.streaming.StreamingException;
+import org.apache.hive.hcatalog.streaming.StrictJsonWriter;
+import org.apache.hive.hcatalog.streaming.TransactionBatch;
+import org.json.simple.JSONObject;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Date;
+import java.text.SimpleDateFormat;
+import java.io.IOException;
+
+public class JsonRecordHiveMapper implements HiveMapper {
+    private static final Logger LOG = LoggerFactory.getLogger(DelimitedRecordHiveMapper.class);
+    private Fields columnFields;
+    private Fields partitionFields;
+    private String timeFormat;
+    private SimpleDateFormat parseDate;
+
+    public JsonRecordHiveMapper() {
+    }
+
+    public JsonRecordHiveMapper withColumnFields(Fields columnFields) {
+        this.columnFields = columnFields;
+        return this;
+    }
+
+    public JsonRecordHiveMapper withPartitionFields(Fields partitionFields) {
+        this.partitionFields = partitionFields;
+        return this;
+    }
+
+    public JsonRecordHiveMapper withTimeAsPartitionField(String timeFormat) {
+        this.timeFormat = timeFormat;
+        parseDate = new SimpleDateFormat(timeFormat);
+        return this;
+    }
+
+    @Override
+    public RecordWriter createRecordWriter(HiveEndPoint endPoint)
+        throws StreamingException, IOException, ClassNotFoundException {
+        return new StrictJsonWriter(endPoint);
+    }
+
+    @Override
+    public void write(TransactionBatch txnBatch, Tuple tuple)
+        throws StreamingException, IOException, InterruptedException {
+        txnBatch.write(mapRecord(tuple));
+    }
+
+    @Override
+    public List<String> mapPartitions(Tuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(Tuple tuple) {
+        JSONObject obj = new JSONObject();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                obj.put(field,tuple.getValueByField(field));
+            }
+        }
+        return obj.toJSONString().getBytes();
+    }
+
+    @Override
+    public List<String> mapPartitions(TridentTuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(TridentTuple tuple) {
+        JSONObject obj = new JSONObject();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                obj.put(field,tuple.getValueByField(field));
+            }
+        }
+        return obj.toJSONString().getBytes();
+    }
+
+    private String getPartitionsByTimeFormat() {
+        Date d = new Date();
+        return parseDate.format(d.getTime());
+    }
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The addition of the JsonRecordHiveMapper class introduces new functionality for mapping JSON records to Hive tables. While the changes do not directly modify authentication mechanisms, encryption algorithms, access control procedures, or logging levels, they could potentially impact the system's security in several ways. First, the handling of JSON objects and the conversion of data into byte arrays for storage could introduce vulnerabilities if the input data is not properly validated or sanitized, leading to injection attacks. Second, the use of SimpleDateFormat for partitioning data by time could be susceptible to locale-related issues or discrepancies in time representation, potentially affecting data integrity. Lastly, the introduction of new data processing paths increases the system's attack surface, necessitating a thorough security review to ensure that the implementation does not inadvertently expose sensitive information or provide new avenues for data leakage. However, without explicit evidence of security controls being bypassed or weakened, the changes require further analysis to fully understand their impact on the system's security posture.",
  "confidence": 7
}
```

Diff number: 22
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java
new file mode 100644
index 000000000..d3162949a
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java
@@ -0,0 +1,146 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.common;
+
+import java.io.Serializable;
+
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.storm.hive.bolt.mapper.HiveMapper;
+import org.apache.hive.hcatalog.streaming.*;
+
+
+public class HiveOptions implements Serializable {
+    protected HiveMapper mapper;
+    protected String databaseName;
+    protected String tableName;
+    protected String metaStoreURI;
+    protected Integer txnsPerBatch = 100;
+    protected Integer maxOpenConnections = 500;
+    protected Integer batchSize = 15000;
+    protected Integer idleTimeout = 0;
+    protected Integer callTimeout = 10000;
+    protected Integer heartBeatInterval = 240;
+    protected Boolean autoCreatePartitions = true;
+    protected String kerberosPrincipal;
+    protected String kerberosKeytab;
+
+    public HiveOptions(String metaStoreURI,String databaseName,String tableName,HiveMapper mapper) {
+        this.metaStoreURI = metaStoreURI;
+        this.databaseName = databaseName;
+        this.tableName = tableName;
+        this.mapper = mapper;
+    }
+
+    public HiveOptions withTxnsPerBatch(Integer txnsPerBatch) {
+        this.txnsPerBatch = txnsPerBatch;
+        return this;
+    }
+
+    public HiveOptions withMaxOpenConnections(Integer maxOpenConnections) {
+        this.maxOpenConnections = maxOpenConnections;
+        return this;
+    }
+
+    public HiveOptions withBatchSize(Integer batchSize) {
+        this.batchSize = batchSize;
+        return this;
+    }
+
+    public HiveOptions withIdleTimeout(Integer idleTimeout) {
+        this.idleTimeout = idleTimeout;
+        return this;
+    }
+
+    public HiveOptions withCallTimeout(Integer callTimeout) {
+        this.callTimeout = callTimeout;
+        return this;
+    }
+
+    public HiveOptions withHeartBeatInterval(Integer heartBeatInterval) {
+        this.heartBeatInterval = heartBeatInterval;
+        return this;
+    }
+
+    public HiveOptions withAutoCreatePartitions(Boolean autoCreatePartitions) {
+        this.autoCreatePartitions = autoCreatePartitions;
+        return this;
+    }
+
+    public HiveOptions withKerberosKeytab(String kerberosKeytab) {
+        this.kerberosKeytab = kerberosKeytab;
+        return this;
+    }
+
+    public HiveOptions withKerberosPrincipal(String kerberosPrincipal) {
+        this.kerberosPrincipal = kerberosPrincipal;
+        return this;
+    }
+
+    public String getMetaStoreURI() {
+        return metaStoreURI;
+    }
+
+    public String getDatabaseName() {
+        return databaseName;
+    }
+
+    public String getTableName() {
+        return tableName;
+    }
+
+    public HiveMapper getMapper() {
+        return mapper;
+    }
+
+    public Integer getBatchSize() {
+        return batchSize;
+    }
+
+    public Integer getCallTimeOut() {
+        return callTimeout;
+    }
+
+    public Integer getHeartBeatInterval() {
+        return heartBeatInterval;
+    }
+
+    public Integer getMaxOpenConnections() {
+        return maxOpenConnections;
+    }
+
+    public Integer getIdleTimeout() {
+        return idleTimeout;
+    }
+
+    public Integer getTxnsPerBatch() {
+        return txnsPerBatch;
+    }
+
+    public Boolean getAutoCreatePartitions() {
+        return autoCreatePartitions;
+    }
+
+    public String getKerberosPrincipal() {
+        return kerberosPrincipal;
+    }
+
+    public String getKerberosKeytab() {
+        return kerberosKeytab;
+    }
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The introduction of the HiveOptions class in the diff file includes configurations for Kerberos authentication (kerberosPrincipal and kerberosKeytab). Kerberos is a network authentication protocol designed to provide strong authentication for client/server applications by using secret-key cryptography. The inclusion of Kerberos authentication parameters suggests that the changes are intended to enhance the security of connections to a Hive server by supporting secure authentication mechanisms. However, without seeing the implementation details of how these parameters are used (e.g., whether they are properly validated, how the keytab file is secured, etc.), it's challenging to fully assess the impact on the system's security posture. Additionally, other configurations such as 'txnsPerBatch', 'maxOpenConnections', 'batchSize', 'idleTimeout', 'callTimeout', and 'heartBeatInterval' could indirectly impact security by affecting the system's resilience to DoS attacks or its ability to handle sensitive data securely. However, these configurations are more related to performance and resource management rather than direct security features.",
  "confidence": 7
}
```

Diff number: 23
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveUtils.java b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveUtils.java
new file mode 100644
index 000000000..5483b077c
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveUtils.java
@@ -0,0 +1,76 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.common;
+
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.storm.hive.bolt.mapper.HiveMapper;
+import org.apache.hive.hcatalog.streaming.*;
+
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.io.File;
+import java.io.IOException;
+
+public class HiveUtils {
+
+    public static HiveEndPoint makeEndPoint(List<String> partitionVals, HiveOptions options) throws ConnectionError {
+        if(partitionVals==null) {
+            return new HiveEndPoint(options.getMetaStoreURI(), options.getDatabaseName(), options.getTableName(), null);
+        }
+        return new HiveEndPoint(options.getMetaStoreURI(), options.getDatabaseName(), options.getTableName(), partitionVals);
+    }
+
+    public static HiveWriter makeHiveWriter(HiveEndPoint endPoint, ExecutorService callTimeoutPool, UserGroupInformation ugi, HiveOptions options)
+        throws HiveWriter.ConnectFailure, InterruptedException {
+        return new HiveWriter(endPoint, options.getTxnsPerBatch(), options.getAutoCreatePartitions(),
+                              options.getCallTimeOut(), callTimeoutPool, options.getMapper(), ugi);
+    }
+
+    public static synchronized UserGroupInformation authenticate(String keytab, String principal)
+    throws AuthenticationFailed {
+        File kfile = new File(keytab);
+        if (!(kfile.isFile() && kfile.canRead())) {
+            throw new IllegalArgumentException("The keyTab file: "
+                                               + keytab + " is nonexistent or can't read. "
+                                               + "Please specify a readable keytab file for Kerberos auth.");
+        }
+        try {
+            principal = SecurityUtil.getServerPrincipal(principal, "");
+        } catch (Exception e) {
+            throw new AuthenticationFailed("Host lookup error when resolving principal " + principal, e);
+        }
+        try {
+            UserGroupInformation.loginUserFromKeytab(principal, keytab);
+            return UserGroupInformation.getLoginUser();
+        } catch (IOException e) {
+            throw new AuthenticationFailed("Login failed for principal " + principal, e);
+        }
+    }
+
+     public static class AuthenticationFailed extends Exception {
+         public AuthenticationFailed(String reason, Exception cause) {
+             super("Kerberos Authentication Failed. " + reason, cause);
+         }
+     }
+
+
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.bolt;

import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.SerializationError;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.Config;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.BatchHelper;
import org.apache.storm.utils.TupleUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveBolt extends BaseRichBolt {
    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
    @VisibleForTesting
    Map<HiveEndPoint, HiveWriter> allWriters;
    private OutputCollector collector;
    private HiveOptions options;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean sendHeartBeat = new AtomicBoolean(false);
    private UserGroupInformation ugi = null;
    private BatchHelper batchHelper;
    private boolean tokenAuthEnabled;

    public HiveBolt(HiveOptions options) {
        this.options = options;
    }

    @Override
    public void prepare(Map<String, Object> conf, TopologyContext topologyContext, OutputCollector collector) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            this.collector = collector;
            this.batchHelper = new BatchHelper(options.getBatchSize(), collector);
            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());

            sendHeartBeat.set(true);
            heartBeatTimer = new Timer(topologyContext.getThisTaskId() + "-hb-timer", true);
            setupHeartBeatTimer();

        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    @Override
    public void execute(Tuple tuple) {
        try {
            if (batchHelper.shouldHandle(tuple)) {
                List<String> partitionVals = options.getMapper().mapPartitions(tuple);
                HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
                HiveWriter writer = getOrCreateWriter(endPoint);
                writer.write(options.getMapper().mapRecord(tuple));
                batchHelper.addBatch(tuple);
            }

            if (batchHelper.shouldFlush()) {
                flushAllWriters(true);
                LOG.info("acknowledging tuples after writers flushed ");
                batchHelper.ack();
            }
            if (TupleUtils.isTick(tuple)) {
                retireIdleWriters();
            }
        } catch (SerializationError se) {
            LOG.info("Serialization exception occurred, tuple is acknowledged but not written to Hive.", tuple);
            this.collector.reportError(se);
            collector.ack(tuple);
        } catch (Exception e) {
            batchHelper.fail(e);
            abortAndCloseWriters();
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {

    }

    @Override
    public void cleanup() {
        sendHeartBeat.set(false);
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                HiveWriter w = entry.getValue();
                w.flushAndClose();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }

        callTimeoutPool = null;
        if (heartBeatTimer != null) {
            heartBeatTimer.cancel();
        }
        super.cleanup();
        LOG.info("Hive Bolt stopped");
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        Map<String, Object> conf = super.getComponentConfiguration();
        if (conf == null) {
            conf = new Config();
        }

        if (options.getTickTupleInterval() > 0) {
            conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, options.getTickTupleInterval());
        }

        return conf;
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat.get()) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    void flushAllWriters(boolean rollToNext)
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(rollToNext);
        }
    }

    void abortAndCloseWriters() {
        try {
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().abort();
            } catch (Exception e) {
                LOG.error("Failed to abort hive transaction batch, HiveEndPoint " + entry.getValue() + " due to exception ", e);
            }
        }
    }

    /**
     * Closes all writers and remove them from cache.
     */
    private void closeAllWriters() {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().close();
            } catch (Exception e) {
                LOG.warn("unable to close writers. ", e);
            }
        }
        //2) Clear cache
        allWriters.clear();
    }

    @VisibleForTesting
    HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.debug("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    LOG.info("cached HiveEndPoint size {} exceeded maxOpenConnections {} ", allWriters.size(),
                             options.getMaxOpenConnections());
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
                HiveUtils.logAllHiveEndPoints(allWriters);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }
    }

    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        LOG.info("Attempting close eldest writers");
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        LOG.info("Attempting close idle writers");
        int count = 0;
        long now = System.currentTimeMillis();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retire(entry.getKey());
            }
        }
        return count;
    }

    private void retire(HiveEndPoint ep) {
        try {
            HiveWriter writer = allWriters.remove(ep);
            if (writer != null) {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                writer.flushAndClose();
            }
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
        }
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.bolt;

import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.SerializationError;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.Config;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.BatchHelper;
import org.apache.storm.utils.TupleUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveBolt extends BaseRichBolt {
    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
    @VisibleForTesting
    Map<HiveEndPoint, HiveWriter> allWriters;
    private OutputCollector collector;
    private HiveOptions options;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean sendHeartBeat = new AtomicBoolean(false);
    private UserGroupInformation ugi = null;
    private BatchHelper batchHelper;
    private boolean tokenAuthEnabled;

    public HiveBolt(HiveOptions options) {
        this.options = options;
    }

    @Override
    public void prepare(Map<String, Object> conf, TopologyContext topologyContext, OutputCollector collector) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            this.collector = collector;
            this.batchHelper = new BatchHelper(options.getBatchSize(), collector);
            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());

            sendHeartBeat.set(true);
            heartBeatTimer = new Timer(topologyContext.getThisTaskId() + "-hb-timer", true);
            setupHeartBeatTimer();

        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    @Override
    public void execute(Tuple tuple) {
        try {
            if (batchHelper.shouldHandle(tuple)) {
                List<String> partitionVals = options.getMapper().mapPartitions(tuple);
                HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
                HiveWriter writer = getOrCreateWriter(endPoint);
                writer.write(options.getMapper().mapRecord(tuple));
                batchHelper.addBatch(tuple);
            }

            if (batchHelper.shouldFlush()) {
                flushAllWriters(true);
                LOG.info("acknowledging tuples after writers flushed ");
                batchHelper.ack();
            }
            if (TupleUtils.isTick(tuple)) {
                retireIdleWriters();
            }
        } catch (SerializationError se) {
            LOG.info("Serialization exception occurred, tuple is acknowledged but not written to Hive.", tuple);
            this.collector.reportError(se);
            collector.ack(tuple);
        } catch (Exception e) {
            batchHelper.fail(e);
            abortAndCloseWriters();
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {

    }

    @Override
    public void cleanup() {
        sendHeartBeat.set(false);
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                HiveWriter w = entry.getValue();
                w.flushAndClose();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }

        callTimeoutPool = null;
        if (heartBeatTimer != null) {
            heartBeatTimer.cancel();
        }
        super.cleanup();
        LOG.info("Hive Bolt stopped");
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        Map<String, Object> conf = super.getComponentConfiguration();
        if (conf == null) {
            conf = new Config();
        }

        if (options.getTickTupleInterval() > 0) {
            conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, options.getTickTupleInterval());
        }

        return conf;
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat.get()) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    void flushAllWriters(boolean rollToNext)
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(rollToNext);
        }
    }

    void abortAndCloseWriters() {
        try {
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().abort();
            } catch (Exception e) {
                LOG.error("Failed to abort hive transaction batch, HiveEndPoint " + entry.getValue() + " due to exception ", e);
            }
        }
    }

    /**
     * Closes all writers and remove them from cache.
     */
    private void closeAllWriters() {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().close();
            } catch (Exception e) {
                LOG.warn("unable to close writers. ", e);
            }
        }
        //2) Clear cache
        allWriters.clear();
    }

    @VisibleForTesting
    HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.debug("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    LOG.info("cached HiveEndPoint size {} exceeded maxOpenConnections {} ", allWriters.size(),
                             options.getMaxOpenConnections());
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
                HiveUtils.logAllHiveEndPoints(allWriters);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }
    }

    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        LOG.info("Attempting close eldest writers");
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        LOG.info("Attempting close idle writers");
        int count = 0;
        long now = System.currentTimeMillis();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retire(entry.getKey());
            }
        }
        return count;
    }

    private void retire(HiveEndPoint ep) {
        try {
            HiveWriter writer = allWriters.remove(ep);
            if (writer != null) {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                writer.flushAndClose();
            }
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
        }
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.trident;

import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.IMetricsContext;
import org.apache.storm.topology.FailedException;
import org.apache.storm.trident.operation.TridentCollector;
import org.apache.storm.trident.state.State;
import org.apache.storm.trident.tuple.TridentTuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveState implements State {
    private static final Logger LOG = LoggerFactory.getLogger(HiveState.class);
    private HiveOptions options;
    private Integer currentBatchSize;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
    private Boolean sendHeartBeat = true;
    private UserGroupInformation ugi = null;
    private Boolean kerberosEnabled = false;
    private Map<HiveEndPoint, HiveWriter> allWriters;
    private boolean tokenAuthEnabled;

    public HiveState(HiveOptions options) {
        this.options = options;
        this.currentBatchSize = 0;
    }


    @Override
    public void beginCommit(Long txId) {
    }

    @Override
    public void commit(Long txId) {
        try {
            flushAllWriters();
            currentBatchSize = 0;
        } catch (HiveWriter.TxnFailure | InterruptedException | HiveWriter.CommitFailure | HiveWriter.TxnBatchFailure ex) {
            LOG.warn("Commit failed. Failing the batch.", ex);
            throw new FailedException(ex);
        }
    }

    public void prepare(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
            heartBeatTimer = new Timer("hive-hb-timer", true);
            setupHeartBeatTimer();
        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
        try {
            writeTuples(tuples);
        } catch (Exception e) {
            abortAndCloseWriters();
            LOG.warn("hive streaming failed.", e);
            throw new FailedException(e);
        }
    }

    private void writeTuples(List<TridentTuple> tuples)
        throws Exception {
        for (TridentTuple tuple : tuples) {
            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
            HiveWriter writer = getOrCreateWriter(endPoint);
            writer.write(options.getMapper().mapRecord(tuple));
            currentBatchSize++;
            if (currentBatchSize >= options.getBatchSize()) {
                flushAllWriters();
                currentBatchSize = 0;
            }
        }
    }

    private void abortAndCloseWriters() {
        try {
            sendHeartBeat = false;
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().abort();
        }
    }


    /**
     * Closes all writers and remove them from cache.
     * @return number of writers retired
     */
    private void closeAllWriters() throws InterruptedException, IOException {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().close();
        }
        //2) Clear cache
        allWriters.clear();
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void flushAllWriters()
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(true);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.info("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }

    }


    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        int count = 0;
        long now = System.currentTimeMillis();
        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retirees.add(entry.getKey());
            }
        }
        //2) Retire them
        for (HiveEndPoint ep : retirees) {
            try {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                allWriters.remove(ep).flushAndClose();
            } catch (IOException e) {
                LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
            } catch (InterruptedException e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
                Thread.currentThread().interrupt();
            } catch (Exception e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            }
        }
        return count;
    }

    public void cleanup() {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                sendHeartBeat = false;
                HiveWriter w = entry.getValue();
                LOG.info("Flushing writer to {}", w);
                w.flush(false);
                LOG.info("Closing writer to {}", w);
                w.close();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }
        heartBeatTimer.cancel();
        callTimeoutPool = null;
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.trident;

import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.IMetricsContext;
import org.apache.storm.topology.FailedException;
import org.apache.storm.trident.operation.TridentCollector;
import org.apache.storm.trident.state.State;
import org.apache.storm.trident.tuple.TridentTuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveState implements State {
    private static final Logger LOG = LoggerFactory.getLogger(HiveState.class);
    private HiveOptions options;
    private Integer currentBatchSize;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
    private Boolean sendHeartBeat = true;
    private UserGroupInformation ugi = null;
    private Boolean kerberosEnabled = false;
    private Map<HiveEndPoint, HiveWriter> allWriters;
    private boolean tokenAuthEnabled;

    public HiveState(HiveOptions options) {
        this.options = options;
        this.currentBatchSize = 0;
    }


    @Override
    public void beginCommit(Long txId) {
    }

    @Override
    public void commit(Long txId) {
        try {
            flushAllWriters();
            currentBatchSize = 0;
        } catch (HiveWriter.TxnFailure | InterruptedException | HiveWriter.CommitFailure | HiveWriter.TxnBatchFailure ex) {
            LOG.warn("Commit failed. Failing the batch.", ex);
            throw new FailedException(ex);
        }
    }

    public void prepare(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
            heartBeatTimer = new Timer("hive-hb-timer", true);
            setupHeartBeatTimer();
        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
        try {
            writeTuples(tuples);
        } catch (Exception e) {
            abortAndCloseWriters();
            LOG.warn("hive streaming failed.", e);
            throw new FailedException(e);
        }
    }

    private void writeTuples(List<TridentTuple> tuples)
        throws Exception {
        for (TridentTuple tuple : tuples) {
            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
            HiveWriter writer = getOrCreateWriter(endPoint);
            writer.write(options.getMapper().mapRecord(tuple));
            currentBatchSize++;
            if (currentBatchSize >= options.getBatchSize()) {
                flushAllWriters();
                currentBatchSize = 0;
            }
        }
    }

    private void abortAndCloseWriters() {
        try {
            sendHeartBeat = false;
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().abort();
        }
    }


    /**
     * Closes all writers and remove them from cache.
     * @return number of writers retired
     */
    private void closeAllWriters() throws InterruptedException, IOException {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().close();
        }
        //2) Clear cache
        allWriters.clear();
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void flushAllWriters()
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(true);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.info("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }

    }


    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        int count = 0;
        long now = System.currentTimeMillis();
        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retirees.add(entry.getKey());
            }
        }
        //2) Retire them
        for (HiveEndPoint ep : retirees) {
            try {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                allWriters.remove(ep).flushAndClose();
            } catch (IOException e) {
                LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
            } catch (InterruptedException e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
                Thread.currentThread().interrupt();
            } catch (Exception e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            }
        }
        return count;
    }

    public void cleanup() {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                sendHeartBeat = false;
                HiveWriter w = entry.getValue();
                LOG.info("Flushing writer to {}", w);
                w.flush(false);
                LOG.info("Closing writer to {}", w);
                w.close();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }
        heartBeatTimer.cancel();
        callTimeoutPool = null;
    }

}

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The provided diff file introduces a new file, HiveUtils.java, which includes functionality for authenticating with a Hive database using Kerberos. The authentication method, authenticate, uses a keytab file and a principal to authenticate against the Kerberos server. This is directly related to security functionalities as it deals with the authentication mechanism of the system. The use of Kerberos for authentication is a critical security component, ensuring that the system is communicating securely with the Hive database. The implementation checks for the existence and readability of the keytab file, throws an AuthenticationFailed exception if the keytab file is not found or not readable, and attempts to log in to Kerberos using the provided principal and keytab file. This change could potentially improve the system's security by ensuring secure authentication to the Hive database, assuming the implementation correctly follows Kerberos authentication best practices. However, any flaws in the implementation, handling of keytab files, or configuration could introduce security vulnerabilities.",
  "confidence": 9
}
```

Diff number: 24
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveWriter.java b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveWriter.java
new file mode 100644
index 000000000..726b8e8c5
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveWriter.java
@@ -0,0 +1,420 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.common;
+
+import java.io.IOException;
+import java.util.concurrent.Callable;
+import java.util.concurrent.CancellationException;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+import org.apache.hadoop.security.UserGroupInformation;
+import org.apache.hive.hcatalog.streaming.*;
+import org.apache.storm.hive.bolt.mapper.HiveMapper;
+import backtype.storm.tuple.Tuple;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+public class HiveWriter {
+
+    private static final Logger LOG = LoggerFactory
+        .getLogger(HiveWriter.class);
+
+    private final HiveEndPoint endPoint;
+    private final StreamingConnection connection;
+    private final int txnsPerBatch;
+    private final RecordWriter recordWriter;
+    private TransactionBatch txnBatch;
+    private final ExecutorService callTimeoutPool;
+    private final long callTimeout;
+
+    private long lastUsed; // time of last flush on this writer
+    protected boolean closed; // flag indicating HiveWriter was closed
+    private boolean autoCreatePartitions;
+    private boolean heartBeatNeeded = false;
+    private UserGroupInformation ugi;
+
+    public HiveWriter(HiveEndPoint endPoint, int txnsPerBatch,
+                      boolean autoCreatePartitions, long callTimeout,
+                      ExecutorService callTimeoutPool, HiveMapper mapper, UserGroupInformation ugi)
+        throws InterruptedException, ConnectFailure {
+        try {
+            this.autoCreatePartitions = autoCreatePartitions;
+            this.callTimeout = callTimeout;
+            this.callTimeoutPool = callTimeoutPool;
+            this.endPoint = endPoint;
+            this.ugi = ugi;
+            this.connection = newConnection(ugi);
+            this.txnsPerBatch = txnsPerBatch;
+            this.recordWriter = mapper.createRecordWriter(endPoint);
+            this.txnBatch = nextTxnBatch(recordWriter);
+            this.closed = false;
+            this.lastUsed = System.currentTimeMillis();
+        } catch(InterruptedException e) {
+            throw e;
+        } catch(RuntimeException e) {
+            throw e;
+        } catch(Exception e) {
+            throw new ConnectFailure(endPoint, e);
+        }
+    }
+
+    @Override
+    public String toString() {
+        return endPoint.toString();
+    }
+
+    public void setHeartBeatNeeded() {
+        heartBeatNeeded = true;
+    }
+
+    /**
+     * Write data <br />
+     *
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    public synchronized void write(final byte[] record)
+        throws WriteFailure, InterruptedException {
+        if (closed) {
+            throw new IllegalStateException("This hive streaming writer was closed " +
+                                            "and thus no longer able to write : " + endPoint);
+        }
+        // write the tuple
+        try {
+            LOG.debug("Writing event to {}", endPoint);
+            callWithTimeout(new CallRunner<Void>() {
+                    @Override
+                    public Void call() throws StreamingException, InterruptedException {
+                        txnBatch.write(record);
+                        return null;
+                    }
+                });
+        } catch(StreamingException e) {
+            throw new WriteFailure(endPoint, txnBatch.getCurrentTxnId(), e);
+        } catch(TimeoutException e) {
+            throw new WriteFailure(endPoint, txnBatch.getCurrentTxnId(), e);
+        }
+    }
+
+    /**
+     * Commits the current Txn.
+     * If 'rollToNext' is true, will switch to next Txn in batch or to a
+     *       new TxnBatch if current Txn batch is exhausted
+     * TODO: see what to do when there are errors in each IO call stage
+     */
+    public void flush(boolean rollToNext)
+        throws CommitFailure, TxnBatchFailure, TxnFailure, InterruptedException {
+        if(heartBeatNeeded) {
+            heartBeatNeeded = false;
+            heartBeat();
+        }
+        lastUsed = System.currentTimeMillis();
+        try {
+            commitTxn();
+            if(txnBatch.remainingTransactions() == 0) {
+                closeTxnBatch();
+                txnBatch = null;
+                if(rollToNext) {
+                    txnBatch = nextTxnBatch(recordWriter);
+                }
+            }
+            if(rollToNext) {
+                LOG.debug("Switching to next Txn for {}", endPoint);
+                txnBatch.beginNextTransaction(); // does not block
+            }
+        } catch(StreamingException e) {
+            throw new TxnFailure(txnBatch, e);
+        }
+    }
+
+    /** Queues up a heartbeat request on the current and remaining txns using the
+     *  heartbeatThdPool and returns immediately
+     */
+    public void heartBeat() throws InterruptedException {
+        // 1) schedule the heartbeat on one thread in pool
+        try {
+            callWithTimeout(new CallRunner<Void>() {
+                    @Override
+                        public Void call() throws Exception {
+                        try {
+                            LOG.debug("Sending heartbeat on batch " + txnBatch);
+                            txnBatch.heartbeat();
+                        } catch (StreamingException e) {
+                            LOG.warn("Heartbeat error on batch " + txnBatch, e);
+                        }
+                        return null;
+                    }
+                });
+        } catch (InterruptedException e) {
+            throw e;
+        } catch (Exception e) {
+            LOG.warn("Unable to send heartbeat on Txn Batch " + txnBatch, e);
+            // Suppressing exceptions as we don't care for errors on heartbeats
+        }
+    }
+
+    /**
+     * Close the Transaction Batch and connection
+     * @throws IOException
+     * @throws InterruptedException
+     */
+    public void close() throws IOException, InterruptedException {
+        closeTxnBatch();
+        closeConnection();
+        closed = true;
+    }
+
+    private void closeConnection() throws InterruptedException {
+        LOG.info("Closing connection to end point : {}", endPoint);
+        try {
+            callWithTimeout(new CallRunner<Void>() {
+                    @Override
+                    public Void call() throws Exception {
+                        connection.close(); // could block
+                        return null;
+                    }
+                });
+        } catch(Exception e) {
+            LOG.warn("Error closing connection to EndPoint : " + endPoint, e);
+            // Suppressing exceptions as we don't care for errors on connection close
+        }
+    }
+
+    private void commitTxn() throws CommitFailure, InterruptedException {
+        LOG.debug("Committing Txn id {} to {}", txnBatch.getCurrentTxnId() , endPoint);
+        try {
+            callWithTimeout(new CallRunner<Void>() {
+                    @Override
+                    public Void call() throws Exception {
+                        txnBatch.commit(); // could block
+                        return null;
+                    }
+                });
+        } catch (StreamingException e) {
+            throw new CommitFailure(endPoint, txnBatch.getCurrentTxnId(), e);
+        } catch (TimeoutException e) {
+            throw new CommitFailure(endPoint, txnBatch.getCurrentTxnId(), e);
+        }
+    }
+
+    private StreamingConnection newConnection(final UserGroupInformation ugi)
+        throws InterruptedException, ConnectFailure {
+        try {
+            return  callWithTimeout(new CallRunner<StreamingConnection>() {
+                    @Override
+                    public StreamingConnection call() throws Exception {
+                        return endPoint.newConnection(autoCreatePartitions, null, ugi); // could block
+                    }
+                });
+        } catch(StreamingException e) {
+            throw new ConnectFailure(endPoint, e);
+        } catch(TimeoutException e) {
+            throw new ConnectFailure(endPoint, e);
+        }
+    }
+
+    private TransactionBatch nextTxnBatch(final RecordWriter recordWriter)
+        throws InterruptedException, TxnBatchFailure {
+        LOG.debug("Fetching new Txn Batch for {}", endPoint);
+        TransactionBatch batch = null;
+        try {
+            batch = callWithTimeout(new CallRunner<TransactionBatch>() {
+                @Override
+                public TransactionBatch call() throws Exception {
+                    return connection.fetchTransactionBatch(txnsPerBatch, recordWriter); // could block
+                }
+            });
+        LOG.debug("Acquired {}. Switching to first txn", batch);
+        batch.beginNextTransaction();
+        } catch(TimeoutException e) {
+            throw new TxnBatchFailure(endPoint, e);
+        } catch(StreamingException e) {
+            throw new TxnBatchFailure(endPoint, e);
+        }
+        return batch;
+    }
+
+    private void closeTxnBatch() throws  InterruptedException {
+        try {
+            LOG.debug("Closing Txn Batch {}", txnBatch);
+            callWithTimeout(new CallRunner<Void>() {
+                    @Override
+                        public Void call() throws Exception {
+                        if(txnBatch != null) {
+                            txnBatch.close(); // could block
+                        }
+                        return null;
+                    }
+                });
+        } catch(InterruptedException e) {
+            throw e;
+        } catch(Exception e) {
+            LOG.warn("Error closing txn batch "+ txnBatch, e);
+        }
+    }
+
+    /**
+     * Aborts the current Txn and switches to next Txn.
+     * @throws StreamingException if could not get new Transaction Batch, or switch to next Txn
+     */
+    public void abort() throws InterruptedException {
+        abortTxn();
+    }
+
+    private void abortTxn() throws InterruptedException {
+        LOG.info("Aborting Txn id {} on End Point {}", txnBatch.getCurrentTxnId(), endPoint);
+        try {
+            callWithTimeout(new CallRunner<Void>() {
+                    @Override
+                        public Void call() throws StreamingException, InterruptedException {
+                        txnBatch.abort(); // could block
+                        return null;
+                    }
+                });
+        } catch (InterruptedException e) {
+            throw e;
+        } catch (TimeoutException e) {
+            LOG.warn("Timeout while aborting Txn " + txnBatch.getCurrentTxnId() + " on EndPoint: " + endPoint, e);
+        } catch (Exception e) {
+            LOG.warn("Error aborting Txn " + txnBatch.getCurrentTxnId() + " on EndPoint: " + endPoint, e);
+            // Suppressing exceptions as we don't care for errors on abort
+        }
+    }
+
+
+    /**
+     * If the current thread has been interrupted, then throws an
+     * exception.
+     * @throws InterruptedException
+     */
+    private static void checkAndThrowInterruptedException()
+        throws InterruptedException {
+        if (Thread.currentThread().interrupted()) {
+            throw new InterruptedException("Timed out before Hive call was made. "
+                                           + "Your callTimeout might be set too low or Hive calls are "
+                                           + "taking too long.");
+        }
+    }
+
+    /**
+     * Execute the callable on a separate thread and wait for the completion
+     * for the specified amount of time in milliseconds. In case of timeout
+     * cancel the callable and throw an IOException
+     */
+    private <T> T callWithTimeout(final CallRunner<T> callRunner)
+        throws TimeoutException, StreamingException, InterruptedException {
+        Future<T> future = callTimeoutPool.submit(new Callable<T>() {
+                @Override
+                public T call() throws Exception {
+                    return callRunner.call();
+                }
+            });
+        try {
+            if (callTimeout > 0) {
+                return future.get(callTimeout, TimeUnit.MILLISECONDS);
+            } else {
+                return future.get();
+            }
+        } catch (TimeoutException eT) {
+            future.cancel(true);
+            throw eT;
+        } catch (ExecutionException e1) {
+            Throwable cause = e1.getCause();
+            if (cause instanceof IOException) {
+                throw new StreamingIOFailure("I/O Failure", (IOException) cause);
+            } else if (cause instanceof StreamingException) {
+                throw (StreamingException) cause;
+            } else if (cause instanceof InterruptedException) {
+                throw (InterruptedException) cause;
+            } else if (cause instanceof RuntimeException) {
+                throw (RuntimeException) cause;
+            } else if (cause instanceof TimeoutException) {
+                throw new StreamingException("Operation Timed Out.", (TimeoutException) cause);
+            } else {
+                throw new RuntimeException(e1);
+            }
+        }
+    }
+
+    public long getLastUsed() {
+        return lastUsed;
+    }
+
+    private byte[] generateRecord(Tuple tuple) {
+        StringBuilder buf = new StringBuilder();
+        for (Object o: tuple.getValues()) {
+            buf.append(o);
+            buf.append(",");
+        }
+        return buf.toString().getBytes();
+    }
+
+    /**
+     * Simple interface whose <tt>call</tt> method is called by
+     * {#callWithTimeout} in a new thread inside a
+     * {@linkplain java.security.PrivilegedExceptionAction#run()} call.
+     * @param <T>
+     */
+    private interface CallRunner<T> {
+        T call() throws Exception;
+    }
+
+    public static class Failure extends Exception {
+        public Failure(String message, Throwable cause) {
+            super(message, cause);
+        }
+    }
+
+    public static class WriteFailure extends Failure {
+        public WriteFailure(HiveEndPoint endPoint, Long currentTxnId, Throwable cause) {
+            super("Failed writing to : " + endPoint + ". TxnID : " + currentTxnId, cause);
+        }
+    }
+
+    public static class CommitFailure extends Failure {
+        public CommitFailure(HiveEndPoint endPoint, Long txnID, Throwable cause) {
+            super("Commit of Txn " + txnID + " failed on EndPoint: " + endPoint, cause);
+        }
+    }
+
+    public static class ConnectFailure extends Failure {
+        public ConnectFailure(HiveEndPoint ep, Throwable cause) {
+            super("Failed connecting to EndPoint " + ep, cause);
+        }
+    }
+
+    public static class TxnBatchFailure extends Failure {
+        public TxnBatchFailure(HiveEndPoint ep, Throwable cause) {
+            super("Failed acquiring Transaction Batch from EndPoint: " + ep, cause);
+        }
+    }
+
+    public static class TxnFailure extends Failure {
+        public TxnFailure(TransactionBatch txnBatch, Throwable cause) {
+            super("Failed switching to next Txn in TxnBatch " + txnBatch, cause);
+        }
+    }
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The provided diff introduces a new file, HiveWriter.java, which is responsible for managing connections, transactions, and data writing to a Hive endpoint. The security relevance of these changes primarily revolves around the handling of user authentication (via UserGroupInformation ugi), transaction management, and the potential for data exposure or manipulation through the write operations. The use of UserGroupInformation suggests that the code interacts with secure Hadoop components, which implies that proper authentication and authorization are critical for preventing unauthorized access. Additionally, the transaction management and data writing operations could impact the integrity and confidentiality of the data being processed. However, without explicit details on how authentication credentials are managed, how data is encrypted during transit, or how access control is enforced beyond the use of UserGroupInformation, a definitive assessment of the security impact cannot be made. Therefore, further analysis is required to understand the full scope of security implications, including a review of related components that interact with this class.",
  "confidence": 7
}
```

Diff number: 25
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java
new file mode 100644
index 000000000..6050aa886
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java
@@ -0,0 +1,306 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.trident;
+
+import storm.trident.operation.TridentCollector;
+import storm.trident.state.State;
+import storm.trident.tuple.TridentTuple;
+import backtype.storm.task.IMetricsContext;
+import backtype.storm.topology.FailedException;
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.hive.hcatalog.streaming.*;
+import org.apache.storm.hive.common.HiveOptions;
+import org.apache.storm.hive.common.HiveUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.Map.Entry;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class HiveState implements State {
+    private static final Logger LOG = LoggerFactory.getLogger(HiveState.class);
+    private HiveOptions options;
+    private Integer currentBatchSize;
+    private ExecutorService callTimeoutPool;
+    private transient Timer heartBeatTimer;
+    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
+    private UserGroupInformation ugi = null;
+    private Boolean kerberosEnabled = false;
+    HashMap<HiveEndPoint, HiveWriter> allWriters;
+
+    public HiveState(HiveOptions options) {
+        this.options = options;
+        this.currentBatchSize = 0;
+    }
+
+
+    @Override
+    public void beginCommit(Long txId) {
+    }
+
+    @Override
+    public void commit(Long txId) {
+    }
+
+    public void prepare(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions)  {
+        try {
+            if(options.getKerberosPrincipal() == null && options.getKerberosKeytab() == null) {
+                kerberosEnabled = false;
+            } else if(options.getKerberosPrincipal() != null && options.getKerberosKeytab() != null) {
+                kerberosEnabled = true;
+            } else {
+                throw new IllegalArgumentException("To enable Kerberos, need to set both KerberosPrincipal " +
+                                                   " & KerberosKeytab");
+            }
+
+            if (kerberosEnabled) {
+                try {
+                    ugi = HiveUtils.authenticate(options.getKerberosKeytab(), options.getKerberosPrincipal());
+                } catch(HiveUtils.AuthenticationFailed ex) {
+                    LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
+                    throw new IllegalArgumentException(ex);
+                }
+            }
+
+            allWriters = new HashMap<HiveEndPoint,HiveWriter>();
+            String timeoutName = "hive-bolt-%d";
+            this.callTimeoutPool = Executors.newFixedThreadPool(1,
+                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
+            heartBeatTimer= new Timer();
+            setupHeartBeatTimer();
+        } catch(Exception e) {
+            LOG.warn("unable to make connection to hive ",e);
+        }
+    }
+
+    public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
+        try {
+            writeTuples(tuples);
+        } catch (Exception e) {
+            abortAndCloseWriters();
+            LOG.warn("hive streaming failed.",e);
+            throw new FailedException(e);
+        }
+    }
+
+    private void writeTuples(List<TridentTuple> tuples)
+        throws Exception {
+        if(timeToSendHeartBeat.compareAndSet(true, false)) {
+            enableHeartBeatOnAllWriters();
+        }
+        for (TridentTuple tuple : tuples) {
+            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
+            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
+            HiveWriter writer = getOrCreateWriter(endPoint);
+            writer.write(options.getMapper().mapRecord(tuple));
+            currentBatchSize++;
+            if(currentBatchSize >= options.getBatchSize()) {
+                flushAllWriters();
+                currentBatchSize = 0;
+            }
+        }
+    }
+
+    private void abortAndCloseWriters() {
+        try {
+            abortAllWriters();
+            closeAllWriters();
+        } catch(InterruptedException e) {
+            LOG.warn("unable to close hive connections. ", e);
+        } catch(IOException ie) {
+            LOG.warn("unable to close hive connections. ", ie);
+        }
+    }
+
+    /**
+     * Abort current Txn on all writers
+     * @return number of writers retired
+     */
+    private void abortAllWriters() throws InterruptedException {
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            entry.getValue().abort();
+        }
+    }
+
+
+    /**
+     * Closes all writers and remove them from cache
+     * @return number of writers retired
+     */
+    private void closeAllWriters() throws InterruptedException, IOException {
+        //1) Retire writers
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            entry.getValue().close();
+        }
+        //2) Clear cache
+        allWriters.clear();
+    }
+
+    private void setupHeartBeatTimer() {
+        if(options.getHeartBeatInterval()>0) {
+            heartBeatTimer.schedule(new TimerTask() {
+                    @Override
+                    public void run() {
+                        timeToSendHeartBeat.set(true);
+                        setupHeartBeatTimer();
+                    }
+                }, options.getHeartBeatInterval() * 1000);
+        }
+    }
+
+    private void flushAllWriters()
+        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
+        for(HiveWriter writer: allWriters.values()) {
+            writer.flush(true);
+        }
+    }
+
+    private void enableHeartBeatOnAllWriters() {
+        for (HiveWriter writer : allWriters.values()) {
+            writer.setHeartBeatNeeded();
+        }
+    }
+
+    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
+        throws HiveWriter.ConnectFailure, InterruptedException {
+        try {
+            HiveWriter writer = allWriters.get( endPoint );
+            if( writer == null ) {
+                LOG.info("Creating Writer to Hive end point : " + endPoint);
+                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options);
+                if(allWriters.size() > options.getMaxOpenConnections()){
+                    int retired = retireIdleWriters();
+                    if(retired==0) {
+                        retireEldestWriter();
+                    }
+                }
+                allWriters.put(endPoint, writer);
+            }
+            return writer;
+        } catch (HiveWriter.ConnectFailure e) {
+            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
+            throw e;
+        }
+
+    }
+
+
+
+    /**
+     * Locate writer that has not been used for longest time and retire it
+     */
+    private void retireEldestWriter() {
+        long oldestTimeStamp = System.currentTimeMillis();
+        HiveEndPoint eldest = null;
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(entry.getValue().getLastUsed() < oldestTimeStamp) {
+                eldest = entry.getKey();
+                oldestTimeStamp = entry.getValue().getLastUsed();
+            }
+        }
+        try {
+            LOG.info("Closing least used Writer to Hive end point : " + eldest);
+            allWriters.remove(eldest).close();
+        } catch (IOException e) {
+            LOG.warn("Failed to close writer for end point: " + eldest, e);
+        } catch (InterruptedException e) {
+            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
+            Thread.currentThread().interrupt();
+        }
+    }
+
+    /**
+     * Locate all writers past idle timeout and retire them
+     * @return number of writers retired
+     */
+    private int retireIdleWriters() {
+        int count = 0;
+        long now = System.currentTimeMillis();
+        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();
+
+        //1) Find retirement candidates
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
+                ++count;
+                retirees.add(entry.getKey());
+            }
+        }
+        //2) Retire them
+        for(HiveEndPoint ep : retirees) {
+            try {
+                LOG.info("Closing idle Writer to Hive end point : {}", ep);
+                allWriters.remove(ep).close();
+            } catch (IOException e) {
+                LOG.warn("Failed to close writer for end point: {}. Error: "+ ep, e);
+            } catch (InterruptedException e) {
+                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
+                Thread.currentThread().interrupt();
+            }
+        }
+        return count;
+    }
+
+    public void cleanup() {
+        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
+            try {
+                HiveWriter w = entry.getValue();
+                LOG.info("Flushing writer to {}", w);
+                w.flush(false);
+                LOG.info("Closing writer to {}", w);
+                w.close();
+            } catch (Exception ex) {
+                LOG.warn("Error while closing writer to " + entry.getKey() +
+                         ". Exception follows.", ex);
+                if (ex instanceof InterruptedException) {
+                    Thread.currentThread().interrupt();
+                }
+            }
+        }
+
+        ExecutorService toShutdown[] = {callTimeoutPool};
+        for (ExecutorService execService : toShutdown) {
+            execService.shutdown();
+            try {
+                while (!execService.isTerminated()) {
+                    execService.awaitTermination(
+                                                 options.getCallTimeOut(), TimeUnit.MILLISECONDS);
+                }
+            } catch (InterruptedException ex) {
+                LOG.warn("shutdown interrupted on " + execService, ex);
+            }
+        }
+        callTimeoutPool = null;
+    }
+
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.trident;

import java.util.Map;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.task.IMetricsContext;
import org.apache.storm.trident.state.State;
import org.apache.storm.trident.state.StateFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class HiveStateFactory implements StateFactory {
    private static final Logger LOG = LoggerFactory.getLogger(HiveStateFactory.class);
    private HiveOptions options;

    public HiveStateFactory() {}

    /**
     * The options for connecting to Hive.
     */
    public HiveStateFactory withOptions(HiveOptions options) {
        if (options.getTickTupleInterval() != HiveOptions.DEFAULT_TICK_TUPLE_INTERVAL_SECS) {
            LOG.error("Tick tuple interval will be ignored for trident."
                    + " The Hive writers are flushed after each batch.");
        }
        this.options = options;
        return this;
    }

    @Override
    public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
        LOG.info("makeState(partitonIndex={}, numpartitions={}", partitionIndex, numPartitions);
        HiveState state = new HiveState(this.options);
        state.prepare(conf, metrics, partitionIndex, numPartitions);
        return state;
    }
}

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The diff file introduces a new file, HiveState.java, which is part of the Apache Storm project and is specifically designed for integration with Apache Hive. The security relevance of these changes primarily revolves around the handling of Kerberos authentication and the management of Hive connections and writers. The code includes logic to check for Kerberos credentials (kerberosPrincipal and kerberosKeytab) and attempts to authenticate using these credentials if present. This is directly related to security as it involves authentication mechanisms. Additionally, the management of Hive connections, including opening, closing, and flushing writers, could impact the security of data flows within the system by affecting how data is transmitted to and stored in Hive. Incorrect handling or failures in these areas could potentially lead to unauthorized access or data leakage. However, without evidence of specific vulnerabilities being introduced or mitigated by these changes, the impact on the system's overall security posture is not definitively positive or negative but warrants further analysis, especially in the context of how these authentication and connection management processes are implemented and used within the broader system.",
  "confidence": 7
}
```

Diff number: 26
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java
new file mode 100644
index 000000000..8f3b9e94d
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java
@@ -0,0 +1,31 @@
+package org.apache.storm.hive.trident;
+
+import backtype.storm.task.IMetricsContext;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import storm.trident.state.State;
+import storm.trident.state.StateFactory;
+import org.apache.storm.hive.common.HiveOptions;
+
+import java.util.Map;
+
+
+public class HiveStateFactory implements StateFactory {
+    private static final Logger LOG = LoggerFactory.getLogger(HiveStateFactory.class);
+    private HiveOptions options;
+
+    public HiveStateFactory(){}
+
+    public HiveStateFactory withOptions(HiveOptions options){
+        this.options = options;
+        return this;
+    }
+
+    @Override
+    public State makeState(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        LOG.info("makeState(partitonIndex={}, numpartitions={}", partitionIndex, numPartitions);
+        HiveState state = new HiveState(this.options);
+        state.prepare(conf, metrics, partitionIndex, numPartitions);
+        return state;
+    }
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_not_security_relevant",
  "reason": "The provided diff file introduces a new file, HiveStateFactory.java, which is part of the Apache Storm project, specifically for integration with Hive. The changes involve the creation of a factory class for generating HiveState instances, which are likely used for managing state information in interactions with a Hive database. The modifications include basic setup and configuration methods, such as constructors and a method for setting options (`withOptions`), and a method (`makeState`) for creating a HiveState instance. The logging statement within `makeState` is for informational purposes and does not directly impact security functionalities such as authentication, encryption, access control, or logging of security events. However, since the class deals with database state management, improper handling or misconfiguration could potentially lead to indirect security implications, such as unauthorized access or data leakage, if the state management is tied to security-sensitive operations. Nonetheless, without explicit modifications to authentication flows, encryption mechanisms, access control procedures, or security-specific logging levels, the direct impact on system security appears minimal.",
  "confidence": 7
}
```

Diff number: 27
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java
new file mode 100644
index 000000000..b0b32f10f
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java
@@ -0,0 +1,14 @@
+package org.apache.storm.hive.trident;
+
+import storm.trident.operation.TridentCollector;
+import storm.trident.state.BaseStateUpdater;
+import storm.trident.tuple.TridentTuple;
+
+import java.util.List;
+
+public class HiveUpdater extends BaseStateUpdater<HiveState>{
+    @Override
+    public void updateState(HiveState state, List<TridentTuple> tuples, TridentCollector collector) {
+        state.updateState(tuples, collector);
+    }
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new Java class, HiveUpdater, which extends BaseStateUpdater and overrides the updateState method to interact with a HiveState object. The primary functionality of this class, as indicated by the code, is to facilitate the updating of state in a Hive database through the Trident API in Apache Storm. The changes are focused on data processing and state management within the context of a distributed computing framework and do not directly modify or interact with authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, these changes are unrelated to the core security functionalities of the system. The introduction of a new class for state updating in a database does not inherently affect the system's security posture without further context on how data validation, authentication, and authorization are handled within or around this new class.",
  "confidence": 9
}
```

Diff number: 28
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveSetupUtil.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveSetupUtil.java
new file mode 100644
index 000000000..d49281993
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveSetupUtil.java
@@ -0,0 +1,220 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.RawLocalFileSystem;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.hadoop.hive.metastore.IMetaStoreClient;
+import org.apache.hadoop.hive.metastore.TableType;
+import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
+import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;
+import org.apache.hadoop.hive.ql.io.orc.OrcSerde;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.thrift.TException;
+
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+public class HiveSetupUtil {
+    public static class RawFileSystem extends RawLocalFileSystem {
+        private static final URI NAME;
+        static {
+            try {
+                NAME = new URI("raw:///");
+            } catch (URISyntaxException se) {
+                throw new IllegalArgumentException("bad uri", se);
+            }
+        }
+
+        @Override
+        public URI getUri() {
+            return NAME;
+        }
+
+        @Override
+        public FileStatus getFileStatus(Path path) throws IOException {
+            File file = pathToFile(path);
+            if (!file.exists()) {
+                throw new FileNotFoundException("Can't find " + path);
+            }
+            // get close enough
+            short mod = 0;
+            if (file.canRead()) {
+                mod |= 0444;
+            }
+            if (file.canWrite()) {
+                mod |= 0200;
+            }
+            if (file.canExecute()) {
+                mod |= 0111;
+            }
+            ShimLoader.getHadoopShims();
+            return new FileStatus(file.length(), file.isDirectory(), 1, 1024,
+                                  file.lastModified(), file.lastModified(),
+                                  FsPermission.createImmutable(mod), "owen", "users", path);
+        }
+    }
+
+    private final static String txnMgr = "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager";
+
+    public static HiveConf getHiveConf() {
+        HiveConf conf = new HiveConf();
+        // String metastoreDBLocation = "jdbc:derby:databaseName=/tmp/metastore_db;create=true";
+        // conf.set("javax.jdo.option.ConnectionDriverName","org.apache.derby.jdbc.EmbeddedDriver");
+        // conf.set("javax.jdo.option.ConnectionURL",metastoreDBLocation);
+        conf.set("fs.raw.impl", RawFileSystem.class.getName());
+        conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, txnMgr);
+        conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
+        return conf;
+    }
+
+    public static void createDbAndTable(HiveConf conf, String databaseName,
+                                        String tableName, List<String> partVals,
+                                        String[] colNames, String[] colTypes,
+                                        String[] partNames, String dbLocation)
+        throws Exception {
+        IMetaStoreClient client = new HiveMetaStoreClient(conf);
+        try {
+            Database db = new Database();
+            db.setName(databaseName);
+            db.setLocationUri(dbLocation);
+            client.createDatabase(db);
+
+            Table tbl = new Table();
+            tbl.setDbName(databaseName);
+            tbl.setTableName(tableName);
+            tbl.setTableType(TableType.MANAGED_TABLE.toString());
+            StorageDescriptor sd = new StorageDescriptor();
+            sd.setCols(getTableColumns(colNames, colTypes));
+            sd.setNumBuckets(1);
+            sd.setLocation(dbLocation + Path.SEPARATOR + tableName);
+            if(partNames!=null && partNames.length!=0) {
+                tbl.setPartitionKeys(getPartitionKeys(partNames));
+            }
+
+            tbl.setSd(sd);
+
+            sd.setBucketCols(new ArrayList<String>(2));
+            sd.setSerdeInfo(new SerDeInfo());
+            sd.getSerdeInfo().setName(tbl.getTableName());
+            sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+            sd.getSerdeInfo().getParameters().put(serdeConstants.SERIALIZATION_FORMAT, "1");
+
+            sd.getSerdeInfo().setSerializationLib(OrcSerde.class.getName());
+            sd.setInputFormat(OrcInputFormat.class.getName());
+            sd.setOutputFormat(OrcOutputFormat.class.getName());
+
+            Map<String, String> tableParams = new HashMap<String, String>();
+            tbl.setParameters(tableParams);
+            client.createTable(tbl);
+            try {
+                if(partVals!=null && partVals.size() > 0) {
+                    addPartition(client, tbl, partVals);
+                }
+            } catch(AlreadyExistsException e) {
+            }
+        } finally {
+            client.close();
+        }
+    }
+
+    // delete db and all tables in it
+    public static void dropDB(HiveConf conf, String databaseName) throws HiveException, MetaException {
+        IMetaStoreClient client = new HiveMetaStoreClient(conf);
+        try {
+            for (String table : client.listTableNamesByFilter(databaseName, "", (short) -1)) {
+                client.dropTable(databaseName, table, true, true);
+            }
+            client.dropDatabase(databaseName);
+        } catch (TException e) {
+            client.close();
+        }
+    }
+
+    private static void addPartition(IMetaStoreClient client, Table tbl
+                                     , List<String> partValues)
+        throws IOException, TException {
+        Partition part = new Partition();
+        part.setDbName(tbl.getDbName());
+        part.setTableName(tbl.getTableName());
+        StorageDescriptor sd = new StorageDescriptor(tbl.getSd());
+        sd.setLocation(sd.getLocation() + Path.SEPARATOR + makePartPath(tbl.getPartitionKeys(), partValues));
+        part.setSd(sd);
+        part.setValues(partValues);
+        client.add_partition(part);
+    }
+
+    private static String makePartPath(List<FieldSchema> partKeys, List<String> partVals) {
+        if(partKeys.size()!=partVals.size()) {
+            throw new IllegalArgumentException("Partition values:" + partVals +
+                                               ", does not match the partition Keys in table :" + partKeys );
+        }
+        StringBuffer buff = new StringBuffer(partKeys.size()*20);
+        int i=0;
+        for(FieldSchema schema : partKeys) {
+            buff.append(schema.getName());
+            buff.append("=");
+            buff.append(partVals.get(i));
+            if(i!=partKeys.size()-1) {
+                buff.append(Path.SEPARATOR);
+            }
+            ++i;
+        }
+        return buff.toString();
+    }
+
+    private static List<FieldSchema> getTableColumns(String[] colNames, String[] colTypes) {
+        List<FieldSchema> fields = new ArrayList<FieldSchema>();
+        for (int i=0; i<colNames.length; ++i) {
+            fields.add(new FieldSchema(colNames[i], colTypes[i], ""));
+        }
+        return fields;
+    }
+
+    private static List<FieldSchema> getPartitionKeys(String[] partNames) {
+        List<FieldSchema> fields = new ArrayList<FieldSchema>();
+        for (int i=0; i < partNames.length; ++i) {
+           fields.add(new FieldSchema(partNames[i], serdeConstants.STRING_TYPE_NAME, ""));
+        }
+        return fields;
+    }
+
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.bolt;

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.google.common.collect.Lists;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.txn.TxnDbUtil;
import org.apache.hadoop.hive.serde.serdeConstants;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.storm.Config;
import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
import org.apache.storm.hive.bolt.mapper.JsonRecordHiveMapper;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.GeneralTopologyContext;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.TupleImpl;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.MockTupleHelpers;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.Mock;
import org.mockito.Mockito;
import org.mockito.MockitoAnnotations;
import org.mockito.invocation.InvocationOnMock;
import org.mockito.stubbing.Answer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.any;
import static org.mockito.Mockito.never;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.verifyNoInteractions;

public class TestHiveBolt {
    final static String dbName = "testdb";
    final static String tblName = "test_table";
    final static String dbName1 = "testdb1";
    final static String tblName1 = "test_table1";
    final static String PART1_NAME = "city";
    final static String PART2_NAME = "state";
    final static String[] partNames = { PART1_NAME, PART2_NAME };
    private static final String COL1 = "id";
    private static final String COL2 = "msg";
    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
    final String partitionVals = "sunnyvale,ca";
    final String[] colNames = { COL1, COL2 };
    final String[] colNames1 = { COL2, COL1 };
    final String metaStoreURI;
    private final HiveConf conf;
    private String[] colTypes = { serdeConstants.INT_TYPE_NAME, serdeConstants.STRING_TYPE_NAME };
    private Config config = new Config();
    private TestingHiveBolt bolt;
    ;
    private ObjectMapper objectMapper = new ObjectMapper();
    @Mock
    private OutputCollector collector;

    public TestHiveBolt() throws Exception {
        //metaStoreURI = "jdbc:derby:;databaseName="+System.getProperty("java.io.tmpdir") +"metastore_db;create=true";
        metaStoreURI = null;
        conf = HiveSetupUtil.getHiveConf();
        TxnDbUtil.setConfValues(conf);
        if (metaStoreURI != null) {
            conf.setVar(HiveConf.ConfVars.METASTOREURIS, metaStoreURI);
        }
    }

    @BeforeEach
    public void setup() throws Exception {
        MockitoAnnotations.initMocks(this);
    }

    @Test
    public void testWithByteArrayIdandMessage()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 100;
        String msg = "test-123";
        String city = "sunnyvale";
        String state = "ca";

        Set<Tuple> tupleSet = new HashSet<Tuple>();
        for (int i = 0; i < 4; i++) {
            Tuple tuple = generateTestTuple(id, msg, city, state);
            bolt.execute(tuple);
            tupleSet.add(tuple);
        }

        List<String> partVals = Lists.newArrayList(city, state);

        for (Tuple t : tupleSet) {
            verify(collector).ack(t);
        }

        assertEquals(4, bolt.getRecordWritten(partVals).size());

        bolt.cleanup();
    }

    @Test
    public void testWithoutPartitions()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName1, tblName1, mapper)
            .withTxnsPerBatch(2).withBatchSize(2).withAutoCreatePartitions(false);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 100;
        String msg = "test-123";
        String city = "sunnyvale";
        String state = "ca";

        Set<Tuple> tupleSet = new HashSet<Tuple>();
        for (int i = 0; i < 4; i++) {
            Tuple tuple = generateTestTuple(id, msg, city, state);
            bolt.execute(tuple);
            tupleSet.add(tuple);
        }

        List<String> partVals = Collections.emptyList();

        for (Tuple t : tupleSet) {
            verify(collector).ack(t);
        }

        List<byte[]> recordWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordWritten);
        assertEquals(4, recordWritten.size());

        bolt.cleanup();
    }

    @Test
    public void testWithTimeformat()
        throws Exception {
        String timeFormat = "yyyy/MM/dd";
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField(timeFormat);
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName1, tblName1, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(1)
            .withMaxOpenConnections(1);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 100;
        String msg = "test-123";
        Date d = new Date();
        SimpleDateFormat parseDate = new SimpleDateFormat(timeFormat);
        String today = parseDate.format(d.getTime());

        List<Tuple> tuples = new ArrayList<>();
        for (int i = 0; i < 2; i++) {
            Tuple tuple = generateTestTuple(id, msg, null, null);
            tuples.add(tuple);
            bolt.execute(tuple);
        }

        for (Tuple t : tuples) {
            verify(collector).ack(t);
        }

        List<String> partVals = Lists.newArrayList(today);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(2, recordsWritten.size());

        byte[] mapped = generateDelimiteredRecord(Lists.newArrayList(id, msg), mapper.getFieldDelimiter());

        for (byte[] record : recordsWritten) {
            assertArrayEquals(mapped, record);
        }

        bolt.cleanup();
    }

    @Test
    public void testData()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(1);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Integer id = 1;
        String msg = "SJC";
        String city = "Sunnyvale";
        String state = "CA";

        Tuple tuple1 = generateTestTuple(id, msg, city, state);

        bolt.execute(tuple1);
        verify(collector).ack(tuple1);

        List<String> partVals = Lists.newArrayList(city, state);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(1, recordsWritten.size());

        byte[] mapped = generateDelimiteredRecord(Lists.newArrayList(id, msg), mapper.getFieldDelimiter());
        assertArrayEquals(mapped, recordsWritten.get(0));

        bolt.cleanup();
    }

    @Test
    public void testJsonWriter()
        throws Exception {
        // json record doesn't need columns to be in the same order
        // as table in hive.
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(1);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 1;
        String msg = "SJC";
        String city = "Sunnyvale";
        String state = "CA";

        Tuple tuple1 = generateTestTuple(id, msg, city, state);

        bolt.execute(tuple1);
        verify(collector).ack(tuple1);

        List<String> partVals = Lists.newArrayList(city, state);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(1, recordsWritten.size());

        byte[] written = recordsWritten.get(0);

        Map<String, ?> writtenMap = objectMapper.readValue(new String(written), new TypeReference<Map<String, ?>>() {
        });

        Map<String, Object> expected = new HashMap<>();
        expected.put(COL1, id);
        expected.put(COL2, msg);

        assertEquals(expected, writtenMap);

        bolt.cleanup();
    }

    @Test
    public void testNoAcksUntilFlushed() {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Tuple tuple1 = generateTestTuple(1, "SJC", "Sunnyvale", "CA");
        Tuple tuple2 = generateTestTuple(2, "SFO", "San Jose", "CA");

        bolt.execute(tuple1);
        verifyNoInteractions(collector);

        bolt.execute(tuple2);
        verify(collector).ack(tuple1);
        verify(collector).ack(tuple2);
        bolt.cleanup();
    }

    @Test
    public void testNoAcksIfFlushFails() throws Exception {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        HiveBolt failingBolt = new FlushFailureHiveBolt(hiveOptions);

        failingBolt.prepare(config, null, new OutputCollector(collector));

        Tuple tuple1 = generateTestTuple(1, "SJC", "Sunnyvale", "CA");
        Tuple tuple2 = generateTestTuple(2, "SFO", "San Jose", "CA");

        failingBolt.execute(tuple1);
        failingBolt.execute(tuple2);

        verify(collector, never()).ack(tuple1);
        verify(collector, never()).ack(tuple2);

        failingBolt.cleanup();
    }

    @Test
    public void testTickTuple() {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Tuple tuple1 = generateTestTuple(1, "SJC", "Sunnyvale", "CA");
        Tuple tuple2 = generateTestTuple(2, "SFO", "San Jose", "CA");

        bolt.execute(tuple1);

        //The tick should cause tuple1 to be ack'd
        Tuple mockTick = MockTupleHelpers.mockTickTuple();
        bolt.execute(mockTick);
        verify(collector).ack(tuple1);

        //The second tuple should NOT be ack'd because the batch should be cleared and this will be
        //the first transaction in the new batch
        bolt.execute(tuple2);
        verify(collector, never()).ack(tuple2);

        bolt.cleanup();
    }

    @Test
    public void testNoTickEmptyBatches() throws Exception {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        //The tick should NOT cause any acks since the batch was empty except for acking itself
        Tuple mockTick = MockTupleHelpers.mockTickTuple();
        bolt.execute(mockTick);
        verifyNoInteractions(collector);

        bolt.cleanup();
    }

    @Test
    public void testMultiPartitionTuples()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(10)
            .withBatchSize(10);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Integer id = 1;
        String msg = "test";
        String city = "San Jose";
        String state = "CA";

        List<Tuple> tuples = new ArrayList<>();
        for (int i = 0; i < 100; i++) {
            Tuple tuple = generateTestTuple(id, msg, city, state);
            tuples.add(tuple);
            bolt.execute(tuple);
        }

        for (Tuple t : tuples) {
            verify(collector).ack(t);
        }

        List<String> partVals = Lists.newArrayList(city, state);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(100, recordsWritten.size());


        byte[] mapped = generateDelimiteredRecord(Lists.newArrayList(id, msg), mapper.getFieldDelimiter());

        for (byte[] record : recordsWritten) {
            assertArrayEquals(mapped, record);
        }

        bolt.cleanup();
    }

    private Tuple generateTestTuple(Object id, Object msg, Object city, Object state) {
        TopologyBuilder builder = new TopologyBuilder();
        GeneralTopologyContext topologyContext = new GeneralTopologyContext(builder.createTopology(),
                                                                            new Config(), new HashMap(), new HashMap(), new HashMap(), "") {
            @Override
            public Fields getComponentOutputFields(String componentId, String streamId) {
                return new Fields("id", "msg", "city", "state");
            }
        };
        return new TupleImpl(topologyContext, new Values(id, msg, city, state), "", 1, "");
    }

    private byte[] generateDelimiteredRecord(List<?> values, String fieldDelimiter) {
        StringBuilder builder = new StringBuilder();
        for (Object value : values) {
            builder.append(value);
            builder.append(fieldDelimiter);
        }
        return builder.toString().getBytes();
    }

    private static class TestingHiveBolt extends HiveBolt {

        protected Map<List<String>, List<byte[]>> partitionValuesToWrittenRecords = new HashMap<>();

        public TestingHiveBolt(HiveOptions options) {
            super(options);
        }

        @Override
        HiveWriter getOrCreateWriter(final HiveEndPoint endPoint)
            throws HiveWriter.ConnectFailure, InterruptedException {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                // always provide mocked HiveWriter
                writer = Mockito.mock(HiveWriter.class);
                try {
                    Mockito.doAnswer(new Answer<Void>() {
                        @Override
                        public Void answer(InvocationOnMock invocation) throws Throwable {
                            Object[] arguments = invocation.getArguments();
                            List<String> partitionVals = endPoint.partitionVals;
                            List<byte[]> writtenRecords = partitionValuesToWrittenRecords.get(partitionVals);
                            if (writtenRecords == null) {
                                writtenRecords = new ArrayList<>();
                                partitionValuesToWrittenRecords.put(partitionVals, writtenRecords);
                            }
                            writtenRecords.add((byte[]) arguments[0]);
                            return null;
                        }
                    }).when(writer).write(any(byte[].class));
                } catch (Exception exc) {
                    throw new RuntimeException(exc);
                }
            }
            return writer;
        }

        public Map<List<String>, List<byte[]>> getPartitionValuesToWrittenRecords() {
            return partitionValuesToWrittenRecords;
        }

        public List<byte[]> getRecordWritten(List<String> partitionValues) {
            return partitionValuesToWrittenRecords.get(partitionValues);
        }
    }

    private static class FlushFailureHiveBolt extends TestingHiveBolt {

        public FlushFailureHiveBolt(HiveOptions options) {
            super(options);
        }

        @Override
        void flushAllWriters(boolean rollToNext) throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure,
            InterruptedException {
            if (rollToNext) {
                throw new InterruptedException();
            } else {
                super.flushAllWriters(false);
            }
        }
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.common;

import com.google.common.util.concurrent.ThreadFactoryBuilder;

import java.io.File;
import java.io.IOException;
import java.util.Arrays;
import java.util.HashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.txn.TxnDbUtil;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.RecordWriter;
import org.apache.hive.hcatalog.streaming.SerializationError;
import org.apache.hive.hcatalog.streaming.StreamingConnection;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.hive.hcatalog.streaming.TransactionBatch;
import org.apache.storm.Config;
import org.apache.storm.hive.bolt.HiveSetupUtil;
import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
import org.apache.storm.hive.bolt.mapper.HiveMapper;
import org.apache.storm.task.GeneralTopologyContext;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.TupleImpl;
import org.apache.storm.tuple.Values;
import org.junit.jupiter.api.Test;
import org.mockito.Mockito;

import static org.junit.jupiter.api.Assertions.assertEquals;

public class TestHiveWriter {
    public static final String PART1_NAME = "city";
    public static final String PART2_NAME = "state";
    public static final String[] partNames = { PART1_NAME, PART2_NAME };
    final static String dbName = "testdb";
    final static String tblName = "test_table2";
    final String[] partitionVals = { "sunnyvale", "ca" };
    final String[] colNames = { "id", "msg" };
    private final int port;
    private final String metaStoreURI;
    private final HiveConf conf;
    int timeout = 10000; // msec
    UserGroupInformation ugi = null;
    private ExecutorService callTimeoutPool;

    public TestHiveWriter() throws Exception {
        port = 9083;
        metaStoreURI = null;
        int callTimeoutPoolSize = 1;
        callTimeoutPool = Executors.newFixedThreadPool(callTimeoutPoolSize,
                                                       new ThreadFactoryBuilder().setNameFormat("hiveWriterTest").build());

        // 1) Start metastore
        conf = HiveSetupUtil.getHiveConf();
        TxnDbUtil.setConfValues(conf);
        if (metaStoreURI != null) {
            conf.setVar(HiveConf.ConfVars.METASTOREURIS, metaStoreURI);
        }
    }

    @Test
    public void testInstantiate() throws Exception {
        DelimitedRecordHiveMapper mapper = new MockedDelemiteredRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
        TestingHiveWriter writer = new TestingHiveWriter(endPoint, 10, true, timeout
            , callTimeoutPool, mapper, ugi, false);
        writer.close();
    }

    @Test
    public void testWriteBasic() throws Exception {
        DelimitedRecordHiveMapper mapper = new MockedDelemiteredRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
        TestingHiveWriter writer = new TestingHiveWriter(endPoint, 10, true, timeout
            , callTimeoutPool, mapper, ugi, false);
        writeTuples(writer, mapper, 3);
        writer.flush(false);
        writer.close();
        Mockito.verify(writer.getMockedTxBatch(), Mockito.times(3)).write(Mockito.any(byte[].class));
    }

    @Test
    public void testWriteMultiFlush() throws Exception {
        DelimitedRecordHiveMapper mapper = new MockedDelemiteredRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));

        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
        TestingHiveWriter writer = new TestingHiveWriter(endPoint, 10, true, timeout
            , callTimeoutPool, mapper, ugi, false);
        Tuple tuple = generateTestTuple("1", "abc");
        writer.write(mapper.mapRecord(tuple));
        tuple = generateTestTuple("2", "def");
        writer.write(mapper.mapRecord(tuple));
        assertEquals(writer.getTotalRecords(), 2);
        Mockito.verify(writer.getMockedTxBatch(), Mockito.times(2)).write(Mockito.any(byte[].class));
        Mockito.verify(writer.getMockedTxBatch(), Mockito.never()).commit();
        writer.flush(true);
        assertEquals(writer.getTotalRecords(), 0);
        Mockito.verify(writer.getMockedTxBatch(), Mockito.atLeastOnce()).commit();

        tuple = generateTestTuple("3", "ghi");
        writer.write(mapper.mapRecord(tuple));
        writer.flush(true);

        tuple = generateTestTuple("4", "klm");
        writer.write(mapper.mapRecord(tuple));
        writer.flush(true);
        writer.close();
        Mockito.verify(writer.getMockedTxBatch(), Mockito.times(4)).write(Mockito.any(byte[].class));
    }

    private Tuple generateTestTuple(Object id, Object msg) {
        TopologyBuilder builder = new TopologyBuilder();
        GeneralTopologyContext topologyContext = new GeneralTopologyContext(builder.createTopology(),
                                                                            new Config(), new HashMap(), new HashMap(), new HashMap(), "") {
            @Override
            public Fields getComponentOutputFields(String componentId, String streamId) {
                return new Fields("id", "msg");
            }
        };
        return new TupleImpl(topologyContext, new Values(id, msg), "", 1, "");
    }

    private void writeTuples(HiveWriter writer, HiveMapper mapper, int count)
        throws HiveWriter.WriteFailure, InterruptedException, SerializationError {
        Integer id = 100;
        String msg = "test-123";
        for (int i = 1; i <= count; i++) {
            Tuple tuple = generateTestTuple(id, msg);
            writer.write(mapper.mapRecord(tuple));
        }
    }

    private static class TestingHiveWriter extends HiveWriter {

        private StreamingConnection mockedStreamingConn;
        private TransactionBatch mockedTxBatch;

        public TestingHiveWriter(HiveEndPoint endPoint, int txnsPerBatch, boolean autoCreatePartitions, long callTimeout,
                                 ExecutorService callTimeoutPool, HiveMapper mapper, UserGroupInformation ugi,
                                 boolean tokenAuthEnabled) throws InterruptedException, ConnectFailure {
            super(endPoint, txnsPerBatch, autoCreatePartitions, callTimeout, callTimeoutPool, mapper, ugi, tokenAuthEnabled);
        }

        @Override
        synchronized StreamingConnection newConnection(UserGroupInformation ugi, boolean tokenAuthEnabled) throws InterruptedException,
            ConnectFailure {
            if (mockedStreamingConn == null) {
                mockedStreamingConn = Mockito.mock(StreamingConnection.class);
                mockedTxBatch = Mockito.mock(TransactionBatch.class);

                try {
                    Mockito.when(mockedStreamingConn.fetchTransactionBatch(Mockito.anyInt(), Mockito.any(RecordWriter.class)))
                           .thenReturn(mockedTxBatch);
                } catch (StreamingException e) {
                    throw new RuntimeException(e);
                }
            }

            return mockedStreamingConn;
        }

        public TransactionBatch getMockedTxBatch() {
            return mockedTxBatch;
        }
    }

    private static class MockedDelemiteredRecordHiveMapper extends DelimitedRecordHiveMapper {
        private final RecordWriter mockedRecordWriter;

        public MockedDelemiteredRecordHiveMapper() {
            this.mockedRecordWriter = Mockito.mock(RecordWriter.class);
        }

        @Override
        public RecordWriter createRecordWriter(HiveEndPoint endPoint) throws StreamingException, IOException, ClassNotFoundException {
            return mockedRecordWriter;
        }

        public RecordWriter getMockedRecordWriter() {
            return mockedRecordWriter;
        }
    }

}
```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new file, HiveSetupUtil.java, which is primarily focused on setting up a testing environment for Hive integration within a Java application. The changes include methods for configuring Hive, creating databases and tables, managing partitions, and handling file system permissions for test purposes. These modifications are targeted towards testing functionalities rather than altering or impacting the core security mechanisms of the system, such as authentication flows, encryption algorithms, access control procedures, or logging levels related to security events. The context provided does not indicate any direct interaction with critical security components or sensitive data handling that would affect the system's security posture. Therefore, the changes are classified as not security relevant.",
  "confidence": 9
}
```

Diff number: 29
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopology.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopology.java
new file mode 100644
index 000000000..e9ecbd06f
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopology.java
@@ -0,0 +1,150 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.Config;
+import backtype.storm.LocalCluster;
+import backtype.storm.StormSubmitter;
+import backtype.storm.spout.SpoutOutputCollector;
+import backtype.storm.task.TopologyContext;
+import backtype.storm.topology.OutputFieldsDeclarer;
+import backtype.storm.topology.TopologyBuilder;
+import backtype.storm.topology.base.BaseRichSpout;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Values;
+
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.common.HiveOptions;
+
+import java.util.Map;
+import java.util.UUID;
+import java.util.concurrent.ConcurrentHashMap;
+
+
+public class HiveTopology {
+    static final String USER_SPOUT_ID = "user-spout";
+    static final String BOLT_ID = "my-hive-bolt";
+    static final String TOPOLOGY_NAME = "hive-test-topology1";
+
+    public static void main(String[] args) throws Exception {
+        String metaStoreURI = args[0];
+        String dbName = args[1];
+        String tblName = args[2];
+        String[] colNames = {"id","name","phone","street","city","state"};
+        Config config = new Config();
+        config.setNumWorkers(1);
+        UserDataSpout spout = new UserDataSpout();
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames));
+        HiveOptions hiveOptions;
+        if (args.length == 6) {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(100)
+                .withIdleTimeout(10)
+                .withKerberosKeytab(args[4])
+                .withKerberosPrincipal(args[5]);
+        } else {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(100)
+                .withIdleTimeout(10);
+        }
+
+        HiveBolt hiveBolt = new HiveBolt(hiveOptions);
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.setSpout(USER_SPOUT_ID, spout, 1);
+        // SentenceSpout --> MyBolt
+        builder.setBolt(BOLT_ID, hiveBolt, 1)
+                .shuffleGrouping(USER_SPOUT_ID);
+        if (args.length == 3) {
+            LocalCluster cluster = new LocalCluster();
+            cluster.submitTopology(TOPOLOGY_NAME, config, builder.createTopology());
+            waitForSeconds(20);
+            cluster.killTopology(TOPOLOGY_NAME);
+            System.out.println("cluster begin to shutdown");
+            cluster.shutdown();
+            System.out.println("cluster shutdown");
+            System.exit(0);
+        } else if(args.length >= 4) {
+            StormSubmitter.submitTopology(args[3], config, builder.createTopology());
+        } else {
+            System.out.println("Usage: HiveTopology metastoreURI dbName tableName [topologyNamey] [keytab file] [principal name]");
+        }
+    }
+
+    public static void waitForSeconds(int seconds) {
+        try {
+            Thread.sleep(seconds * 1000);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    public static class UserDataSpout extends BaseRichSpout {
+        private ConcurrentHashMap<UUID, Values> pending;
+        private SpoutOutputCollector collector;
+        private String[] sentences = {
+                "1,user1,123456,street1,sunnyvale,ca",
+                "2,user2,123456,street2,sunnyvale,ca",
+                "3,user3,123456,street3,san jose,ca",
+                "4,user4,123456,street4,san jose,ca",
+        };
+        private int index = 0;
+        private int count = 0;
+        private long total = 0L;
+
+        public void declareOutputFields(OutputFieldsDeclarer declarer) {
+            declarer.declare(new Fields("id","name","phone","street","city","state"));
+        }
+
+        public void open(Map config, TopologyContext context,
+                         SpoutOutputCollector collector) {
+            this.collector = collector;
+            this.pending = new ConcurrentHashMap<UUID, Values>();
+        }
+
+        public void nextTuple() {
+            String[] user = sentences[index].split(",");
+            Values values = new Values(Integer.parseInt(user[0]),user[1],user[2],user[3],user[4],user[5]);
+            UUID msgId = UUID.randomUUID();
+            this.pending.put(msgId, values);
+            this.collector.emit(values, msgId);
+            index++;
+            if (index >= sentences.length) {
+                index = 0;
+            }
+            count++;
+            total++;
+            if(count > 1000){
+                count = 0;
+                System.out.println("Pending count: " + this.pending.size() + ", total: " + this.total);
+            }
+            Thread.yield();
+        }
+
+        public void ack(Object msgId) {
+            this.pending.remove(msgId);
+        }
+
+        public void fail(Object msgId) {
+            System.out.println("**** RESENDING FAILED TUPLE");
+            this.collector.emit(this.pending.get(msgId), msgId);
+        }
+    }
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The code changes introduce a new HiveTopology class that includes functionality for connecting to a Hive database, potentially with Kerberos authentication if the necessary arguments are provided. The inclusion of Kerberos authentication options (keytab file and principal name) indicates an impact on the system's security posture, as it directly relates to how authentication is handled. However, without seeing the implementation details of how the authentication process is managed and the security measures in place for the transmission of authentication credentials (e.g., whether credentials are transmitted securely), it's challenging to fully assess the security implications. Additionally, the UserDataSpout class emits user data, including potentially sensitive information (e.g., phone numbers, addresses), which could raise concerns about data protection and privacy if not handled securely. The security relevance of these changes hinges on the specifics of how authentication credentials are managed, the security of the data transmission, and the handling of the emitted user data.",
  "confidence": 7
}
```

Diff number: 30
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopologyPartitioned.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopologyPartitioned.java
new file mode 100644
index 000000000..c3197c22f
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopologyPartitioned.java
@@ -0,0 +1,153 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.Config;
+import backtype.storm.LocalCluster;
+import backtype.storm.StormSubmitter;
+import backtype.storm.spout.SpoutOutputCollector;
+import backtype.storm.task.TopologyContext;
+import backtype.storm.topology.OutputFieldsDeclarer;
+import backtype.storm.topology.TopologyBuilder;
+import backtype.storm.topology.base.BaseRichSpout;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Values;
+import backtype.storm.utils.Utils;
+
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.common.HiveOptions;
+
+import java.util.Map;
+import java.util.UUID;
+import java.util.concurrent.ConcurrentHashMap;
+
+
+public class HiveTopologyPartitioned {
+    static final String USER_SPOUT_ID = "hive-user-spout-partitioned";
+    static final String BOLT_ID = "my-hive-bolt-partitioned";
+    static final String TOPOLOGY_NAME = "hive-test-topology-partitioned";
+
+    public static void main(String[] args) throws Exception {
+        String metaStoreURI = args[0];
+        String dbName = args[1];
+        String tblName = args[2];
+        String[] partNames = {"city","state"};
+        String[] colNames = {"id","name","phone","street"};
+        Config config = new Config();
+        config.setNumWorkers(1);
+        UserDataSpout spout = new UserDataSpout();
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions;
+        if (args.length == 6) {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(1000)
+                .withIdleTimeout(10)
+                .withKerberosKeytab(args[4])
+                .withKerberosPrincipal(args[5]);
+        } else {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(1000)
+                .withIdleTimeout(10);
+        }
+
+        HiveBolt hiveBolt = new HiveBolt(hiveOptions);
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.setSpout(USER_SPOUT_ID, spout, 1);
+        // SentenceSpout --> MyBolt
+        builder.setBolt(BOLT_ID, hiveBolt, 1)
+                .shuffleGrouping(USER_SPOUT_ID);
+        if (args.length == 3) {
+            LocalCluster cluster = new LocalCluster();
+            cluster.submitTopology(TOPOLOGY_NAME, config, builder.createTopology());
+            waitForSeconds(20);
+            cluster.killTopology(TOPOLOGY_NAME);
+            System.out.println("cluster begin to shutdown");
+            cluster.shutdown();
+            System.out.println("cluster shutdown");
+            System.exit(0);
+        } else if(args.length >= 4) {
+            StormSubmitter.submitTopology(args[3], config, builder.createTopology());
+        } else {
+            System.out.println("Usage: HiveTopologyPartitioned metastoreURI dbName tableName [topologyNamey] [keytab file] [principal name]");
+        }
+    }
+
+    public static void waitForSeconds(int seconds) {
+        try {
+            Thread.sleep(seconds * 1000);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    public static class UserDataSpout extends BaseRichSpout {
+        private ConcurrentHashMap<UUID, Values> pending;
+        private SpoutOutputCollector collector;
+        private String[] sentences = {
+                "1,user1,123456,street1,sunnyvale,ca",
+                "2,user2,123456,street2,sunnyvale,ca",
+                "3,user3,123456,street3,san jose,ca",
+                "4,user4,123456,street4,san jose,ca",
+        };
+        private int index = 0;
+        private int count = 0;
+        private long total = 0L;
+
+        public void declareOutputFields(OutputFieldsDeclarer declarer) {
+            declarer.declare(new Fields("id","name","phone","street","city","state"));
+        }
+
+        public void open(Map config, TopologyContext context,
+                         SpoutOutputCollector collector) {
+            this.collector = collector;
+            this.pending = new ConcurrentHashMap<UUID, Values>();
+        }
+
+        public void nextTuple() {
+            String[] user = sentences[index].split(",");
+            Values values = new Values(Integer.parseInt(user[0]),user[1],user[2],user[3],user[4],user[5]);
+            UUID msgId = UUID.randomUUID();
+            this.pending.put(msgId, values);
+            this.collector.emit(values, msgId);
+            index++;
+            if (index >= sentences.length) {
+                index = 0;
+            }
+            count++;
+            total++;
+            if(count > 1000){
+		Utils.sleep(1000);
+                count = 0;
+                System.out.println("Pending count: " + this.pending.size() + ", total: " + this.total);
+            }
+        }
+
+        public void ack(Object msgId) {
+            this.pending.remove(msgId);
+        }
+
+        public void fail(Object msgId) {
+            System.out.println("**** RESENDING FAILED TUPLE");
+            this.collector.emit(this.pending.get(msgId), msgId);
+        }
+    }
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The diff file introduces a new file that includes the implementation of a Hive topology with partitioned data processing capabilities. The security relevance of these changes primarily revolves around the handling of Kerberos authentication parameters ('withKerberosKeytab', 'withKerberosPrincipal') for secure access to Hive. Kerberos is a critical security component for authenticating service requests in distributed systems. The correct handling and implementation of Kerberos authentication can significantly impact the security posture of the system, ensuring that only authorized users and services can access sensitive data stored in Hive. Additionally, the code manages user data and emits tuples potentially containing sensitive information, which could impact data confidentiality if not properly secured. However, without explicit security vulnerabilities being introduced or modified in the provided diff, the potential security relevance requires further analysis to determine if the implementation adheres to best practices for secure Kerberos authentication and data handling.",
  "confidence": 7
}
```

Diff number: 31
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/TestHiveBolt.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/TestHiveBolt.java
new file mode 100644
index 000000000..e7e875e25
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/TestHiveBolt.java
@@ -0,0 +1,330 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.Config;
+import backtype.storm.task.GeneralTopologyContext;
+import backtype.storm.task.IOutputCollector;
+import backtype.storm.task.OutputCollector;
+import backtype.storm.topology.TopologyBuilder;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Tuple;
+import backtype.storm.tuple.TupleImpl;
+import backtype.storm.tuple.Values;
+
+import org.apache.storm.hive.common.HiveOptions;
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.bolt.mapper.JsonRecordHiveMapper;
+
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.CommandNeedRetryException;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.metastore.txn.TxnDbUtil;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.mockito.Mock;
+import org.mockito.MockitoAnnotations;
+import static org.junit.Assert.assertEquals;
+import static org.mockito.Mockito.verify;
+import junit.framework.Assert;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.ArrayList;
+import java.io.IOException;
+import java.util.Date;
+import java.text.SimpleDateFormat;
+
+
+import org.apache.hive.hcatalog.streaming.*;
+
+public class TestHiveBolt {
+    final static String dbName = "testdb";
+    final static String tblName = "test_table";
+    final static String dbName1 = "testdb1";
+    final static String tblName1 = "test_table1";
+    final static String PART1_NAME = "city";
+    final static String PART2_NAME = "state";
+    final static String[] partNames = { PART1_NAME, PART2_NAME };
+    final String partitionVals = "sunnyvale,ca";
+    private static final String COL1 = "id";
+    private static final String COL2 = "msg";
+    final String[] colNames = {COL1,COL2};
+    final String[] colNames1 = {COL2,COL1};
+    private String[] colTypes = {serdeConstants.INT_TYPE_NAME, serdeConstants.STRING_TYPE_NAME};
+    private final HiveConf conf;
+    private final Driver driver;
+    private final int port ;
+    final String metaStoreURI;
+    private String dbLocation;
+    private Config config = new Config();
+    private HiveBolt bolt;
+    private final static boolean WINDOWS = System.getProperty("os.name").startsWith("Windows");
+
+    @Rule
+    public TemporaryFolder dbFolder = new TemporaryFolder();
+
+    @Mock
+    private IOutputCollector collector;
+
+
+    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
+
+    public TestHiveBolt() throws Exception {
+        port=9083;
+        dbLocation = new String();
+        //metaStoreURI = "jdbc:derby:;databaseName="+System.getProperty("java.io.tmpdir") +"metastore_db;create=true";
+        metaStoreURI = null;
+        conf = HiveSetupUtil.getHiveConf();
+        TxnDbUtil.setConfValues(conf);
+        TxnDbUtil.cleanDb();
+        TxnDbUtil.prepDb();
+        SessionState.start(new CliSessionState(conf));
+        driver = new Driver(conf);
+
+        // driver.init();
+    }
+
+    @Before
+    public void setup() throws Exception {
+        MockitoAnnotations.initMocks(this);
+        HiveSetupUtil.dropDB(conf, dbName);
+        if(WINDOWS) {
+            dbLocation = dbFolder.newFolder(dbName + ".db").getCanonicalPath();
+        } else {
+            dbLocation = "raw://" + dbFolder.newFolder(dbName + ".db").getCanonicalPath();
+        }
+        HiveSetupUtil.createDbAndTable(conf, dbName, tblName, Arrays.asList(partitionVals.split(",")),
+                colNames, colTypes, partNames, dbLocation);
+        System.out.println("done");
+    }
+
+    @Test
+    public void testEndpointConnection() throws Exception {
+        // 1) Basic
+        HiveEndPoint endPt = new HiveEndPoint(metaStoreURI, dbName, tblName
+                                              , Arrays.asList(partitionVals.split(",")));
+        StreamingConnection connection = endPt.newConnection(false, null); //shouldn't throw
+        connection.close();
+        // 2) Leave partition unspecified
+        endPt = new HiveEndPoint(metaStoreURI, dbName, tblName, null);
+        endPt.newConnection(false, null).close(); // should not throw
+    }
+
+    @Test
+    public void testWithByteArrayIdandMessage()
+        throws Exception {
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+            .withTxnsPerBatch(2)
+            .withBatchSize(2);
+        bolt = new HiveBolt(hiveOptions);
+        bolt.prepare(config,null,new OutputCollector(collector));
+        Integer id = 100;
+        String msg = "test-123";
+        String city = "sunnyvale";
+        String state = "ca";
+        checkRecordCountInTable(tblName,dbName,0);
+        for (int i=0; i < 4; i++) {
+            Tuple tuple = generateTestTuple(id,msg,city,state);
+            bolt.execute(tuple);
+            verify(collector).ack(tuple);
+        }
+        checkRecordCountInTable(tblName, dbName, 4);
+        bolt.cleanup();
+    }
+
+
+    @Test
+    public void testWithoutPartitions()
+        throws Exception {
+        HiveSetupUtil.dropDB(conf,dbName1);
+        HiveSetupUtil.createDbAndTable(conf, dbName1, tblName1,null,
+                                       colNames,colTypes,null, dbLocation);
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames));
+        HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName1,tblName1,mapper)
+            .withTxnsPerBatch(2)
+            .withBatchSize(2)
+            .withAutoCreatePartitions(false);
+        bolt = new HiveBolt(hiveOptions);
+        bolt.prepare(config,null,new OutputCollector(collector));
+        Integer id = 100;
+        String msg = "test-123";
+        String city = "sunnyvale";
+        String state = "ca";
+        checkRecordCountInTable(tblName1,dbName1,0);
+        for (int i=0; i < 4; i++) {
+            Tuple tuple = generateTestTuple(id,msg,city,state);
+            bolt.execute(tuple);
+            verify(collector).ack(tuple);
+        }
+        bolt.cleanup();
+        checkRecordCountInTable(tblName1, dbName1, 4);
+    }
+
+    @Test
+    public void testWithTimeformat()
+        throws Exception {
+        String[] partNames1 = {"date"};
+        String timeFormat = "yyyy/MM/dd";
+        HiveSetupUtil.dropDB(conf,dbName1);
+        HiveSetupUtil.createDbAndTable(conf, dbName1, tblName1,null,
+                                       colNames,colTypes,partNames1, dbLocation);
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withTimeAsPartitionField(timeFormat);
+        HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName1,tblName1,mapper)
+            .withTxnsPerBatch(2)
+            .withBatchSize(1);
+        bolt = new HiveBolt(hiveOptions);
+        bolt.prepare(config,null,new OutputCollector(collector));
+        Integer id = 100;
+        String msg = "test-123";
+        Date d = new Date();
+        SimpleDateFormat parseDate = new SimpleDateFormat(timeFormat);
+        String today=parseDate.format(d.getTime());
+        checkRecordCountInTable(tblName1,dbName1,0);
+        for (int i=0; i < 2; i++) {
+            Tuple tuple = generateTestTuple(id,msg,null,null);
+            bolt.execute(tuple);
+            verify(collector).ack(tuple);
+        }
+        checkDataWritten(tblName1, dbName1, "100,test-123,"+today, "100,test-123,"+today);
+        bolt.cleanup();
+    }
+
+    @Test
+    public void testData()
+        throws Exception {
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+            .withTxnsPerBatch(2)
+            .withBatchSize(1);
+        bolt = new HiveBolt(hiveOptions);
+        bolt.prepare(config,null,new OutputCollector(collector));
+        Tuple tuple1 = generateTestTuple(1,"SJC","Sunnyvale","CA");
+        //Tuple tuple2 = generateTestTuple(2,"SFO","San Jose","CA");
+        bolt.execute(tuple1);
+        verify(collector).ack(tuple1);
+        //bolt.execute(tuple2);
+        //verify(collector).ack(tuple2);
+        checkDataWritten(tblName, dbName, "1,SJC,Sunnyvale,CA");
+        bolt.cleanup();
+    }
+
+    @Test
+    public void testJsonWriter()
+        throws Exception {
+        // json record doesn't need columns to be in the same order
+        // as table in hive.
+        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
+            .withColumnFields(new Fields(colNames1))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+            .withTxnsPerBatch(2)
+            .withBatchSize(1);
+        bolt = new HiveBolt(hiveOptions);
+        bolt.prepare(config,null,new OutputCollector(collector));
+        Tuple tuple1 = generateTestTuple(1,"SJC","Sunnyvale","CA");
+        //Tuple tuple2 = generateTestTuple(2,"SFO","San Jose","CA");
+        bolt.execute(tuple1);
+        verify(collector).ack(tuple1);
+        //bolt.execute(tuple2);
+        //verify(collector).ack(tuple2);
+        checkDataWritten(tblName, dbName, "1,SJC,Sunnyvale,CA");
+        bolt.cleanup();
+    }
+
+
+    @Test
+    public void testMultiPartitionTuples()
+        throws Exception {
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+            .withTxnsPerBatch(10)
+            .withBatchSize(10);
+        bolt = new HiveBolt(hiveOptions);
+        bolt.prepare(config,null,new OutputCollector(collector));
+        Integer id = 1;
+        String msg = "test";
+        String city = "San Jose";
+        String state = "CA";
+        checkRecordCountInTable(tblName,dbName,0);
+        for(int i=0; i < 100; i++) {
+            Tuple tuple = generateTestTuple(id,msg,city,state);
+            bolt.execute(tuple);
+            verify(collector).ack(tuple);
+        }
+        checkRecordCountInTable(tblName, dbName, 100);
+        bolt.cleanup();
+    }
+
+    private void checkRecordCountInTable(String tableName,String dbName,int expectedCount)
+        throws CommandNeedRetryException, IOException {
+        int count = listRecordsInTable(tableName,dbName).size();
+        Assert.assertEquals(expectedCount, count);
+    }
+
+    private  ArrayList<String> listRecordsInTable(String tableName,String dbName)
+        throws CommandNeedRetryException, IOException {
+        driver.compile("select * from " + dbName + "." + tableName);
+        ArrayList<String> res = new ArrayList<String>();
+        driver.getResults(res);
+        return res;
+    }
+
+    private void checkDataWritten(String tableName,String dbName,String... row)
+        throws CommandNeedRetryException, IOException {
+        ArrayList<String> results = listRecordsInTable(tableName,dbName);
+        for(int i = 0; i < row.length && results.size() > 0; i++) {
+            String resultRow = results.get(i).replace("\t",",");
+            System.out.println(resultRow);
+            assertEquals(row[i],resultRow);
+        }
+    }
+
+    private Tuple generateTestTuple(Object id, Object msg,Object city,Object state) {
+        TopologyBuilder builder = new TopologyBuilder();
+        GeneralTopologyContext topologyContext = new GeneralTopologyContext(builder.createTopology(),
+                                                                             new Config(), new HashMap(), new HashMap(), new HashMap(), "") {
+                @Override
+                public Fields getComponentOutputFields(String componentId, String streamId) {
+                    return new Fields("id", "msg","city","state");
+                }
+            };
+        return new TupleImpl(topologyContext, new Values(id, msg,city,state), 1, "");
+    }
+
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new test class for the HiveBolt component within the Apache Storm project. This class, TestHiveBolt, includes various test methods aimed at verifying the functionality of the HiveBolt component, such as its ability to connect to Hive endpoints, handle data with or without partitions, and correctly process both delimited and JSON formatted data. The changes are focused on testing the data processing capabilities of the HiveBolt component and do not directly modify or interact with critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, these changes are not directly related to the system's security functionalities but are rather aimed at ensuring the reliability and correctness of the HiveBolt's data processing features.",
  "confidence": 9
}
```

Diff number: 32
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/common/TestHiveWriter.java b/external/storm-hive/src/test/java/org/apache/storm/hive/common/TestHiveWriter.java
new file mode 100644
index 000000000..63b194918
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/common/TestHiveWriter.java
@@ -0,0 +1,193 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.common;
+
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import junit.framework.Assert;
+import org.apache.hadoop.hive.cli.CliSessionState;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.ql.CommandNeedRetryException;
+import org.apache.hadoop.hive.ql.Driver;
+import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hive.hcatalog.streaming.HiveEndPoint;
+import org.apache.hive.hcatalog.streaming.StreamingException;
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.bolt.mapper.HiveMapper;
+import org.apache.storm.hive.bolt.HiveSetupUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import backtype.storm.Config;
+import backtype.storm.task.GeneralTopologyContext;
+import backtype.storm.topology.TopologyBuilder;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Tuple;
+import backtype.storm.tuple.TupleImpl;
+import backtype.storm.tuple.Values;
+import org.apache.hadoop.hive.metastore.txn.TxnDbUtil;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.ArrayList;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.HashMap;
+
+public class TestHiveWriter {
+    final static String dbName = "testdb";
+    final static String tblName = "test_table2";
+
+    public static final String PART1_NAME = "city";
+    public static final String PART2_NAME = "state";
+    public static final String[] partNames = { PART1_NAME, PART2_NAME };
+    final String[] partitionVals = {"sunnyvale","ca"};
+    final String[] colNames = {"id","msg"};
+    private String[] colTypes = { "int", "string" };
+    private final int port;
+    private final String metaStoreURI;
+    private final HiveConf conf;
+    private ExecutorService callTimeoutPool;
+    private final Driver driver;
+    int timeout = 10000; // msec
+    UserGroupInformation ugi = null;
+
+    @Rule
+    public TemporaryFolder dbFolder = new TemporaryFolder();
+
+
+    public TestHiveWriter() throws Exception {
+        port = 9083;
+        metaStoreURI = null;
+        int callTimeoutPoolSize = 1;
+        callTimeoutPool = Executors.newFixedThreadPool(callTimeoutPoolSize,
+                                                       new ThreadFactoryBuilder().setNameFormat("hiveWriterTest").build());
+
+        // 1) Start metastore
+        conf = HiveSetupUtil.getHiveConf();
+        TxnDbUtil.setConfValues(conf);
+        TxnDbUtil.cleanDb();
+        TxnDbUtil.prepDb();
+
+        if(metaStoreURI!=null) {
+            conf.setVar(HiveConf.ConfVars.METASTOREURIS, metaStoreURI);
+        }
+        SessionState.start(new CliSessionState(conf));
+        driver = new Driver(conf);
+        driver.init();
+    }
+
+    @Before
+    public void setUp() throws Exception {
+        // 1) Setup tables
+        HiveSetupUtil.dropDB(conf, dbName);
+        String dbLocation = dbFolder.newFolder(dbName).getCanonicalPath() + ".db";
+        HiveSetupUtil.createDbAndTable(conf, dbName, tblName, Arrays.asList(partitionVals),
+                                       colNames,colTypes, partNames, dbLocation);
+    }
+
+    @Test
+    public void testInstantiate() throws Exception {
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
+        HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout
+                                           ,callTimeoutPool, mapper, ugi);
+        writer.close();
+    }
+
+    @Test
+    public void testWriteBasic() throws Exception {
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
+        HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout
+                                           , callTimeoutPool, mapper, ugi);
+        writeTuples(writer,mapper,3);
+        writer.flush(false);
+        writer.close();
+        checkRecordCountInTable(dbName,tblName,3);
+    }
+
+    @Test
+    public void testWriteMultiFlush() throws Exception {
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+
+        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
+        HiveWriter writer = new HiveWriter(endPoint, 10, true, timeout
+                                           , callTimeoutPool, mapper, ugi);
+        Tuple tuple = generateTestTuple("1","abc");
+        writer.write(mapper.mapRecord(tuple));
+        checkRecordCountInTable(dbName,tblName,0);
+        writer.flush(true);
+
+        tuple = generateTestTuple("2","def");
+        writer.write(mapper.mapRecord(tuple));
+        writer.flush(true);
+
+        tuple = generateTestTuple("3","ghi");
+        writer.write(mapper.mapRecord(tuple));
+        writer.flush(true);
+        writer.close();
+        checkRecordCountInTable(dbName,tblName,3);
+    }
+
+    private Tuple generateTestTuple(Object id, Object msg) {
+        TopologyBuilder builder = new TopologyBuilder();
+        GeneralTopologyContext topologyContext = new GeneralTopologyContext(builder.createTopology(),
+                                                              new Config(), new HashMap(), new HashMap(), new HashMap(), "") {
+                @Override
+                public Fields getComponentOutputFields(String componentId, String streamId) {
+                    return new Fields("id", "msg");
+                }
+            };
+        return new TupleImpl(topologyContext, new Values(id, msg), 1, "");
+    }
+
+    private void writeTuples(HiveWriter writer, HiveMapper mapper, int count)
+            throws HiveWriter.WriteFailure, InterruptedException {
+        Integer id = 100;
+        String msg = "test-123";
+        for (int i = 1; i <= count; i++) {
+            Tuple tuple = generateTestTuple(id,msg);
+            writer.write(mapper.mapRecord(tuple));
+        }
+    }
+
+    private void checkRecordCountInTable(String dbName,String tableName,int expectedCount)
+        throws CommandNeedRetryException, IOException {
+        int count = listRecordsInTable(dbName,tableName).size();
+        Assert.assertEquals(expectedCount, count);
+    }
+
+    private  ArrayList<String> listRecordsInTable(String dbName,String tableName)
+        throws CommandNeedRetryException, IOException {
+        driver.compile("select * from " + dbName + "." + tableName);
+        ArrayList<String> res = new ArrayList<String>();
+        driver.getResults(res);
+        return res;
+    }
+
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_not_security_relevant",
  "reason": "The provided diff file introduces a new test class for HiveWriter functionality within a project. This class primarily focuses on setting up a testing environment for HiveWriter, including configuration of Hive, creation of a test database and table, and execution of basic write operations to Hive. The changes involve test setup procedures, instantiation of HiveWriter, and execution of write operations in a test environment. There are no direct modifications to authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. However, since the test involves interactions with Hive, which could potentially include sensitive data handling or access control configurations in a real-world scenario, there's a very indirect link to security. The primary purpose of these changes is to validate the functionality of HiveWriter in a controlled test environment, rather than altering the security posture of the system.",
  "confidence": 8
}
```

Diff number: 33
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java b/external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java
new file mode 100644
index 000000000..bc607f3f9
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java
@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.trident;
+
+
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.common.HiveOptions;
+
+import backtype.storm.Config;
+import backtype.storm.LocalCluster;
+import backtype.storm.StormSubmitter;
+import backtype.storm.generated.StormTopology;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Values;
+import backtype.storm.task.TopologyContext;
+import storm.trident.operation.TridentCollector;
+import storm.trident.spout.IBatchSpout;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import storm.trident.Stream;
+import storm.trident.TridentState;
+import storm.trident.TridentTopology;
+import storm.trident.state.StateFactory;
+
+
+public class TridentHiveTopology {
+    public static StormTopology buildTopology(String metaStoreURI, String dbName, String tblName, Object keytab, Object principal) {
+        int batchSize = 100;
+        FixedBatchSpout spout = new FixedBatchSpout(batchSize);
+        spout.setCycle(true);
+        TridentTopology topology = new TridentTopology();
+        Stream stream = topology.newStream("hiveTridentspout1",spout);
+        String[] partNames = {"city","state"};
+        String[] colNames = {"id","name","phone","street"};
+        Fields hiveFields = new Fields("id","name","phone","street","city","state");
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions;
+        if (keytab != null && principal != null) {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(batchSize)
+                .withIdleTimeout(10)
+                .withCallTimeout(30000)
+                .withKerberosKeytab((String)keytab)
+                .withKerberosPrincipal((String)principal);
+        } else  {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(batchSize)
+                .withCallTimeout(30000)
+                .withIdleTimeout(10);
+        }
+        StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
+        TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());
+        return topology.build();
+    }
+
+    public static void waitForSeconds(int seconds) {
+        try {
+            Thread.sleep(seconds * 1000);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    public static void main(String[] args) {
+        String metaStoreURI = args[0];
+        String dbName = args[1];
+        String tblName = args[2];
+        Config conf = new Config();
+        conf.setMaxSpoutPending(5);
+        if(args.length == 3) {
+            LocalCluster cluster = new LocalCluster();
+            cluster.submitTopology("tridentHiveTopology", conf, buildTopology(metaStoreURI, dbName, tblName,null,null));
+            System.out.println("waiting for 60 seconds");
+            waitForSeconds(60);
+            System.out.println("killing topology");
+            cluster.killTopology("tridenHiveTopology");
+            System.out.println("cluster shutdown");
+            cluster.shutdown();
+            System.out.println("cluster shutdown");
+            System.exit(0);
+        } else if(args.length == 4) {
+            try {
+                StormSubmitter.submitTopology(args[3], conf, buildTopology(metaStoreURI, dbName, tblName,null,null));
+            } catch(Exception e) {
+                System.out.println("Failed to submit topology "+e);
+            }
+        } else if (args.length == 6) {
+            try {
+                StormSubmitter.submitTopology(args[3], conf, buildTopology(metaStoreURI, dbName, tblName,args[4],args[5]));
+            } catch(Exception e) {
+                System.out.println("Failed to submit topology "+e);
+            }
+        } else {
+            System.out.println("Usage: TridentHiveTopology metastoreURI dbName tableName [topologyNamey]");
+        }
+    }
+
+    public static class FixedBatchSpout implements IBatchSpout {
+        int maxBatchSize;
+        HashMap<Long, List<List<Object>>> batches = new HashMap<Long, List<List<Object>>>();
+        private Values[] outputs = {
+            new Values("1","user1","123456","street1","sunnyvale","ca"),
+            new Values("2","user2","123456","street2","sunnyvale","ca"),
+            new Values("3","user3","123456","street3","san jose","ca"),
+            new Values("4","user4","123456","street4","san jose","ca"),
+        };
+        private int index = 0;
+        boolean cycle = false;
+
+        public FixedBatchSpout(int maxBatchSize) {
+            this.maxBatchSize = maxBatchSize;
+        }
+
+        public void setCycle(boolean cycle) {
+            this.cycle = cycle;
+        }
+
+        @Override
+        public Fields getOutputFields() {
+            return new Fields("id","name","phone","street","city","state");
+        }
+
+        @Override
+        public void open(Map conf, TopologyContext context) {
+            index = 0;
+        }
+
+        @Override
+        public void emitBatch(long batchId, TridentCollector collector) {
+            List<List<Object>> batch = this.batches.get(batchId);
+            if(batch == null){
+                batch = new ArrayList<List<Object>>();
+                if(index>=outputs.length && cycle) {
+                    index = 0;
+                }
+                for(int i=0; i < maxBatchSize; index++, i++) {
+                    if(index == outputs.length){
+                        index=0;
+                    }
+                    batch.add(outputs[index]);
+                }
+                this.batches.put(batchId, batch);
+            }
+            for(List<Object> list : batch){
+                collector.emit(list);
+            }
+        }
+
+        @Override
+        public void ack(long batchId) {
+            this.batches.remove(batchId);
+        }
+
+        @Override
+        public void close() {
+        }
+
+        @Override
+        public Map getComponentConfiguration() {
+            Config conf = new Config();
+            conf.setMaxTaskParallelism(1);
+            return conf;
+        }
+
+    }
+
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The code introduces a new feature that includes handling of Kerberos authentication (keytab and principal parameters) for secure interaction with a Hive database. Kerberos is a network authentication protocol designed to provide strong authentication for client/server applications by using secret-key cryptography. The correct implementation and use of Kerberos can significantly enhance the security posture of an application by ensuring that sensitive actions or data access are performed by authenticated entities. However, the security impact of these changes depends on the correct implementation of the Kerberos authentication mechanism and the secure handling of the keytab and principal information. If mishandled, it could potentially expose the system to unauthorized access or compromise the Kerberos authentication flow. Additionally, the code includes a spout that cycles through user data, which could be a vector for information leakage if the data is sensitive and not properly protected. However, without further context on how the data is used and protected throughout the system, the direct impact on security cannot be fully assessed.",
  "confidence": 7
}
```

Diff number: 34
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java b/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java
index cc723c332..7bac4c8d7 100644
--- a/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java
+++ b/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java
@@ -24,10 +24,11 @@ import java.sql.Timestamp;
 import java.sql.Types;
 
 public class Util {
-    public static String getSqlTypeName(int sqlType) {
+    private static String getSqlTypeName(int sqlType) {
         try {
+            Integer val = new Integer(sqlType);
             for (Field field : Types.class.getFields()) {
-                if (sqlType == field.get(null)) {
+                if (val.equals(field.get(null))) {
                     return field.getName();
                 }
             }

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_not_security_relevant",
  "reason": "The changes in the diff file involve modifying the access modifier of the getSqlTypeName method from public to private and changing the comparison mechanism from a direct integer comparison to an object equality check. The modification of the access modifier restricts the visibility of the method, which could potentially impact how the method is used across the system, but this change does not directly affect security functionalities such as authentication, encryption, access control, or logging. The change in comparison mechanism (from '==' to 'equals') is a refinement in how SQL types are compared, which could potentially improve the reliability of type comparisons but does not inherently impact security features or vulnerabilities. Without further context indicating that this method is directly involved in security-critical operations, these changes appear to be more related to code quality or maintainability rather than security.",
  "confidence": 7
}
```

Diff number: 35
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-jdbc/src/test/java/org/apache/storm/jdbc/common/UtilTest.java b/external/storm-jdbc/src/test/java/org/apache/storm/jdbc/common/UtilTest.java
new file mode 100644
index 000000000..8b6e8b351
--- /dev/null
+++ b/external/storm-jdbc/src/test/java/org/apache/storm/jdbc/common/UtilTest.java
@@ -0,0 +1,69 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.jdbc.common;
+
+import java.sql.Date;
+import java.sql.Time;
+import java.sql.Timestamp;
+import java.sql.Types;
+import org.junit.Test;
+
+import static org.junit.Assert.*;
+
+public class UtilTest {
+
+    @Test
+    public void testBasic() {
+        assertEquals(String.class, Util.getJavaType(Types.CHAR));
+        assertEquals(String.class, Util.getJavaType(Types.VARCHAR));
+        assertEquals(String.class, Util.getJavaType(Types.LONGVARCHAR));
+        assertEquals(byte[].class, Util.getJavaType(Types.BINARY));
+        assertEquals(byte[].class, Util.getJavaType(Types.VARBINARY));
+        assertEquals(byte[].class, Util.getJavaType(Types.LONGVARBINARY));
+        assertEquals(Boolean.class, Util.getJavaType(Types.BIT));
+        assertEquals(Short.class, Util.getJavaType(Types.TINYINT));
+        assertEquals(Short.class, Util.getJavaType(Types.SMALLINT));
+        assertEquals(Integer.class, Util.getJavaType(Types.INTEGER));
+        assertEquals(Long.class, Util.getJavaType(Types.BIGINT));
+        assertEquals(Float.class, Util.getJavaType(Types.REAL));
+        assertEquals(Double.class, Util.getJavaType(Types.DOUBLE));
+        assertEquals(Double.class, Util.getJavaType(Types.FLOAT));
+        assertEquals(Date.class, Util.getJavaType(Types.DATE));
+        assertEquals(Time.class, Util.getJavaType(Types.TIME));
+        assertEquals(Timestamp.class, Util.getJavaType(Types.TIMESTAMP));
+    }
+
+    @Test
+    public void testError() {
+        //This test is rather ugly, but it is the only way to see if the error messages are working correctly.
+        try {
+            Util.getJavaType(Types.REF);
+            fail("didn't throw like expected");
+        } catch (Exception e) {
+            assertEquals("We do not support tables with SqlType: REF", e.getMessage());
+        }
+
+        try {
+            Util.getJavaType(-1000);
+            fail("didn't throw like expected");
+        } catch (Exception e) {
+            assertEquals("Unknown sqlType -1000", e.getMessage());
+        }
+
+    }
+}

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new test class for the Apache Storm JDBC component. This class, UtilTest, contains unit tests that verify the mapping of SQL types to Java types and the handling of unsupported SQL types. The changes are focused on testing the functionality of utility methods used for type conversion and do not directly modify or interact with critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The modifications are strictly related to unit testing of existing functionality and do not introduce or alter the behavior of the system's security features. Therefore, these changes are not directly relevant to the system's security posture.",
  "confidence": 10
}
```

Diff number: 36
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
index b6cccadd2..ecb0daf6a 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
@@ -141,7 +141,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
                     new PrivilegedExceptionAction<Void>() {
                 public Void run() {
                     try {
-                        LOG.info("do as:"+ principal);
+                        LOG.debug("do as:"+ principal);
                         sasalTransport.open();
                     }
                     catch (Exception e) {

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The change in the logging level from INFO to DEBUG for the log statement that logs the action of 'do as' with a principal in the KerberosSaslTransportPlugin.java file could potentially impact the system's security. Logging at a DEBUG level often means that this information will only be recorded if the system's logging configuration is set to capture debug (or lower) level messages. This could reduce the visibility of authentication actions under normal operational conditions, where INFO level logging is more common. In environments where monitoring and alerting are configured to react to specific log entries, this change could lead to a delay or failure in detecting issues related to Kerberos authentication processes. However, without further context on how logs are monitored and the operational logging level, it's challenging to definitively assess the impact on security.",
  "confidence": 7
}
```

Diff number: 37
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/Config.java b/storm-core/src/jvm/backtype/storm/Config.java
index 1237c28f2..1655cf6ca 100644
--- a/storm-core/src/jvm/backtype/storm/Config.java
+++ b/storm-core/src/jvm/backtype/storm/Config.java
@@ -441,6 +441,14 @@ public class Config extends HashMap<String, Object> {
     public static final String NIMBUS_AUTHORIZER = "nimbus.authorizer";
     public static final Object NIMBUS_AUTHORIZER_SCHEMA = String.class;
 
+
+    /**
+     * Impersonation user ACL config entries.
+     */
+    public static final String NIMBUS_IMPERSONATION_AUTHORIZER = "nimbus.impersonation.authorizer";
+    public static final Object NIMBUS_IMPERSONATION_AUTHORIZER_SCHEMA = String.class;
+
+
     /**
      * Impersonation user ACL config entries.
      */

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The addition of the 'NIMBUS_IMPERSONATION_AUTHORIZER' configuration parameter directly impacts the security functionalities of the system. This parameter appears to be related to the authorization process for user impersonation within the system. Impersonation features, if not properly secured and monitored, can be exploited by attackers to escalate privileges or perform actions on behalf of legitimate users without authorization. The introduction of this parameter suggests that the system is either introducing or modifying its capabilities around user impersonation, which is a critical security component. The security implications of this change depend on how this new configuration parameter is used within the system, including how authorization checks are implemented and enforced. However, the mere presence of this change indicates a direct impact on the system's security posture, particularly in the areas of authentication and authorization.",
  "confidence": 9
}
```

Diff number: 38
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
index 1e947ae67..d6431be56 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
@@ -13,6 +13,7 @@ import java.util.*;
 
 public class ImpersonationAuthorizer implements IAuthorizer {
     private static final Logger LOG = LoggerFactory.getLogger(ImpersonationAuthorizer.class);
+    protected static final String WILD_CARD = "*";
 
     protected Map<String, ImpersonationACL> userImpersonationACL;
     protected IPrincipalToLocal _ptol;
@@ -97,12 +98,17 @@ public class ImpersonationAuthorizer implements IAuthorizer {
     }
 
     private boolean isAllowedToImpersonateFromHost(Set<String> authorizedHosts, InetAddress remoteAddress) {
-        return authorizedHosts.contains(remoteAddress.getCanonicalHostName()) ||
+        return authorizedHosts.contains(WILD_CARD) ||
+                authorizedHosts.contains(remoteAddress.getCanonicalHostName()) ||
                 authorizedHosts.contains(remoteAddress.getHostName()) ||
                 authorizedHosts.contains(remoteAddress.getHostAddress());
     }
 
     private boolean isAllowedToImpersonateUser(Set<String> authorizedGroups, String userBeingImpersonated) {
+        if(authorizedGroups.contains(WILD_CARD)) {
+            return true;
+        }
+
         Set<String> groups = null;
         try {
             groups = _groupMappingProvider.getGroups(userBeingImpersonated);

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes introduced in the ImpersonationAuthorizer.java file directly impact the system's security by modifying the authentication mechanism related to impersonation authorization. Specifically, the addition of a wildcard ('*') as a valid entry for both authorized hosts and groups significantly lowers the security posture of the system. This change means that if the wildcard is present in the list of authorized hosts or groups, any host or any user, respectively, would be allowed to impersonate another user. This effectively bypasses the intended restrictions and controls around impersonation, potentially allowing unauthorized access to resources or the execution of actions as another user without proper authorization. Such a modification could lead to a wide range of security vulnerabilities, including privilege escalation and unauthorized data access, making it a direct impact on the system's security.",
  "confidence": 9
}
```

Diff number: 39
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
index 25114b959..888eefb4f 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
@@ -50,7 +50,6 @@ public class SimpleACLAuthorizer implements IAuthorizer {
     protected Set<String> _supervisors;
     protected IPrincipalToLocal _ptol;
     protected IGroupMappingServiceProvider _groupMappingProvider;
-    protected ImpersonationAuthorizer _impersonationAuthorizer;
     /**
      * Invoked once immediately after construction
      * @param conf Storm configuration
@@ -69,8 +68,6 @@ public class SimpleACLAuthorizer implements IAuthorizer {
 
         _ptol = AuthUtils.GetPrincipalToLocalPlugin(conf);
         _groupMappingProvider = AuthUtils.GetGroupMappingServiceProviderPlugin(conf);
-        _impersonationAuthorizer = new ImpersonationAuthorizer();
-        _impersonationAuthorizer.prepare(conf);
     }
 
     /**
@@ -91,10 +88,6 @@ public class SimpleACLAuthorizer implements IAuthorizer {
         String principal = context.principal().getName();
         String user = _ptol.toLocal(context.principal());
 
-        if(!_impersonationAuthorizer.permit(context, operation, topology_conf)) {
-            return false;
-        }
-
         if (_admins.contains(principal) || _admins.contains(user)) {
             return true;
         }

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file involve the removal of an ImpersonationAuthorizer component from the SimpleACLAuthorizer class in a security authentication module. The ImpersonationAuthorizer likely played a role in checking if a given operation or access request could be performed under the guise of another user (impersonation), which is a critical security control in preventing unauthorized access and ensuring that users can only perform actions they are explicitly authorized to do. By removing the initialization and the call to _impersonationAuthorizer.permit(...) method, the system's ability to enforce impersonation checks for authorization decisions is directly impacted. This could potentially allow unauthorized actions if the system relies on this mechanism as a part of its security posture to prevent privilege escalation or unauthorized access to resources. Therefore, this change directly impacts security functionalities related to authentication and authorization mechanisms within the system.",
  "confidence": 9
}
```

Diff number: 40
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
index 0ff3d35bd..1788dab08 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
@@ -112,7 +112,7 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private void handleAuthorizeCallback(AuthorizeCallback ac) {
         String authenticationID = ac.getAuthenticationID();
-        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID + " authorizationID= " + ac.getAuthorizationID());
+        LOG.info("Successfully authenticated client: authenticationID = " + authenticationID + " authorizationID = " + ac.getAuthorizationID());
 
         //if authorizationId is not set, set it to authenticationId.
         if(ac.getAuthorizationID() == null) {
@@ -122,6 +122,7 @@ public class ServerCallbackHandler implements CallbackHandler {
         //When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We
         //add the authNid as the real user in reqContext's subject which will be used during authorization.
         if(!authenticationID.equals(ac.getAuthorizationID())) {
+            LOG.info("Impersonation attempt  authenticationID = " + ac.getAuthenticationID() + " authorizationID = " + ac.getAuthorizationID());
             ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
         }
 

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The changes in the diff file are related to the logging of authentication events within an authentication mechanism, specifically in the context of handling authorization callbacks. The modification includes an adjustment to the logging format for successful authentication events and the addition of a new log entry for impersonation attempts. Logging of authentication and authorization events is crucial for security monitoring and incident response. The addition of explicit logging for impersonation attempts enhances the system's ability to detect and respond to potential unauthorized access attempts, which could indicate an attack or a misconfiguration allowing privilege escalation. However, without further context on how these logs are monitored, alerted, and acted upon, or the overall security architecture of the system, it's challenging to fully assess the direct impact on the system's security posture. Therefore, while the changes are related to security functionalities, their impact requires further analysis to fully understand.",
  "confidence": 8
}
```

Diff number: 41
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
index 86455580a..59c5b065d 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
@@ -72,16 +72,24 @@ public class DefaultHttpCredentialsPlugin implements IHttpCredentialsPlugin {
     public ReqContext populateContext(ReqContext context,
             HttpServletRequest req) {
         String userName = getUserName(req);
-        Principal p = null;
-        if (userName != null) {
-            p = new SingleUserPrincipal(userName);
+
+        String doAsUser = req.getHeader("doAsUser");
+        if(doAsUser == null) {
+            doAsUser = req.getParameter("doAsUser");
         }
-        Set<Principal> principals = new HashSet<Principal>(1);
-        if (p != null) {
-            principals.add(p);
+
+        if(doAsUser != null) {
+            context.setRealPrincipal(new SingleUserPrincipal(userName));
+            userName = doAsUser;
         }
-        Subject s = new Subject(true, principals, new HashSet(), new HashSet());
-        context.setSubject(s);
+
+        if(userName != null) {
+            Subject s = new Subject();
+            Principal p = new SingleUserPrincipal(userName);
+            s.getPrincipals().add(p);
+            context.setSubject(s);
+        }
+
         return context;
     }
 }

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The code changes in the diff file directly impact the authentication mechanism of the system by introducing the ability to specify a 'doAsUser' parameter in HTTP requests, which can alter the user context the system operates under. This change allows for the specification of a different user than the one authenticated by the initial login process, potentially enabling privilege escalation or unauthorized access if not properly validated and logged. The introduction of 'doAsUser' handling without explicit checks or logging mechanisms raises concerns about accountability and traceability of actions performed under altered user contexts. Furthermore, the change affects how the 'Subject' and 'Principal' objects are constructed and associated with the 'ReqContext', which is central to security decisions in the application. The modification could have implications for access control decisions, potentially affecting the enforcement of security policies based on user identity.",
  "confidence": 9
}
```

Diff number: 42
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/StormSubmitter.java b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
index 63cc5a146..a4ccf5f87 100644
--- a/storm-core/src/jvm/backtype/storm/StormSubmitter.java
+++ b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
@@ -268,7 +268,7 @@ public class StormSubmitter {
     @SuppressWarnings("unchecked")
     public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
              ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
-
+        submitTopologyAs(name, stormConf, topology, opts, progressListener, null);
     }
 
     /**

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_not_security_relevant",
  "reason": "The code change involves a modification in the method call within the StormSubmitter class, specifically changing the 'submitTopology' method to internally call 'submitTopologyAs' with an additional parameter set to null. This change does not directly modify any authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. However, without further context on what 'submitTopologyAs' does, especially regarding the last parameter now being passed as null, it's challenging to definitively assess the impact on security. The change seems to be more about internal method handling rather than altering security functionalities directly. Therefore, while the change itself does not appear to impact security components, the lack of context about the behavior of 'submitTopologyAs' and the significance of the null parameter necessitates a cautious approach.",
  "confidence": 7
}
```

Diff number: 43
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java b/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
index b6c3de487..cf169dc40 100644
--- a/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
+++ b/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
@@ -101,11 +101,10 @@ public class KafkaBolt<K, V> extends BaseRichBolt {
             } else {
                 LOG.warn("skipping key = " + key + ", topic selector returned null.");
             }
-        } catch (Exception ex) {
-            LOG.error("Could not send message with key = " + key
-                    + " and value = " + message + " to topic = " + topic, ex);
-        } finally {
             collector.ack(input);
+        } catch (Exception ex) {
+            collector.reportError(ex);
+            collector.fail(input);
         }
     }
 

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "potentially_security_relevant",
  "reason": "The changes in the diff file modify the error handling behavior of a KafkaBolt component in a Storm application. Previously, when an exception occurred, the system would log an error message with details about the key, value, and topic of the message that failed to be sent. The updated code removes the detailed logging on error and instead reports the error to the collector and marks the input as failed without logging specific details about the message. This modification could potentially impact the system's security in two ways: 1) By removing detailed error logs, it may hinder the ability to diagnose or detect malicious activities or misconfigurations that cause message failures. Detailed logs are often crucial for forensic analysis and identifying security breaches. 2) Reporting the error and failing the input explicitly could improve the system's resilience by ensuring that message failures are handled more transparently, potentially aiding in the detection of denial-of-service attacks or other malicious activities that could lead to an unusually high rate of message failures. However, without more context about how errors are handled downstream and the sensitivity of the information previously logged, it's challenging to definitively classify the security impact.",
  "confidence": 7
}
```

Diff number: 44
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java b/external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java
index 2a56f84ec..c30cba132 100644
--- a/external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java
+++ b/external/storm-kafka/src/test/storm/kafka/bolt/KafkaBoltTest.java
@@ -143,7 +143,7 @@ public class KafkaBoltTest {
         String message = "value-234";
         Tuple tuple = generateTestTuple(message);
         bolt.execute(tuple);
-        verify(collector).ack(tuple);
+        verify(collector).fail(tuple);
     }
 
 

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The change in the diff file is within a test class for the KafkaBolt component of the Apache Storm project. Specifically, it modifies the behavior of a test case to verify a failure scenario (`verify(collector).fail(tuple);`) instead of a success scenario (`verify(collector).ack(tuple);`). Test classes and their modifications generally do not have a direct impact on the runtime security of the system, as they are not part of the application's production code. The change is focused on altering how a test validates the handling of a tuple by the KafkaBolt, switching from expecting an acknowledgment to expecting a failure. This does not affect authentication mechanisms, encryption algorithms, access control procedures, or the logging of security events within the system's operational context.",
  "confidence": 10
}
```

Diff number: 45
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will be given a context containing class(es) where the methods in the diff file were called. If there aren't any methods in the diff file, this context will be empty.
Use this extra information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'potentially_security_relevant' (if the changes could impact the system's security but require further analysis), 'potentially_not_security_relevant' (if the changes require further analysis, but likely are not related to security functionalities), 'not_security_relevant' (if the changes do not involve security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
index 59c5b065d..e2469e5d9 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
@@ -83,12 +83,13 @@ public class DefaultHttpCredentialsPlugin implements IHttpCredentialsPlugin {
             userName = doAsUser;
         }
 
+        Set<Principal> principals = new HashSet<Principal>();
         if(userName != null) {
-            Subject s = new Subject();
             Principal p = new SingleUserPrincipal(userName);
-            s.getPrincipals().add(p);
-            context.setSubject(s);
+            principals.add(p);
         }
+        Subject s = new Subject(true, principals, new HashSet(), new HashSet());
+        context.setSubject(s);
 
         return context;
     }

```
Context:
```

```
Analyze the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file are directly related to the authentication mechanism of the system. Specifically, the modifications impact how user principals are created and associated with a Subject in the authentication process. Previously, a new Subject was created and a Principal was added to it only if a userName was not null. With the new changes, a Subject is created outside of the conditional check for a non-null userName, and it is populated with a set of principals that includes the Principal created from the userName (if not null). This change could potentially alter the authentication flow by ensuring that a Subject is always created and set in the context, regardless of whether a userName is present or not. This could have implications for how authentication and subsequent authorization are handled, especially in scenarios where the presence or absence of a userName is a critical factor. The introduction of a more structured way to handle principals (using a Set) might also impact how identities are managed and verified within the system.",
  "confidence": 8
}
```

Number of diff files: 45
Number of processed files: 43
Accuracy: 0.4883720930232558
Precision: 0.9047619047619048
Recall: 0.38
F1 Score: 0.5352112676056339