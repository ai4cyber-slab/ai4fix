File: storm, pull request #448
Model: gpt-4o

Diff number: 1
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/Config.java b/storm-core/src/jvm/backtype/storm/Config.java
index 73a1976ff..1237c28f2 100644
--- a/storm-core/src/jvm/backtype/storm/Config.java
+++ b/storm-core/src/jvm/backtype/storm/Config.java
@@ -441,6 +441,12 @@ public class Config extends HashMap<String, Object> {
     public static final String NIMBUS_AUTHORIZER = "nimbus.authorizer";
     public static final Object NIMBUS_AUTHORIZER_SCHEMA = String.class;
 
+    /**
+     * Impersonation user ACL config entries.
+     */
+    public static final String NIMBUS_IMPERSONATION_ACL = "nimbus.impersonation.acl";
+    public static final Object NIMBUS_IMPERSONATION_ACL_SCHEMA = ConfigValidation.MapOfStringToMapValidator;
+
     /**
      * How often nimbus should wake up to renew credentials if needed.
      */

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The diff file introduces a new configuration entry 'NIMBUS_IMPERSONATION_ACL' and its corresponding schema 'NIMBUS_IMPERSONATION_ACL_SCHEMA'. This change directly affects access control procedures by adding a new ACL (Access Control List) configuration for impersonation. Since ACLs are critical for defining and enforcing security policies, it is essential to re-run the security tests to ensure that the new configuration does not introduce any vulnerabilities or misconfigurations.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 2
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/ConfigValidation.java b/storm-core/src/jvm/backtype/storm/ConfigValidation.java
index 14394a08b..24991d783 100644
--- a/storm-core/src/jvm/backtype/storm/ConfigValidation.java
+++ b/storm-core/src/jvm/backtype/storm/ConfigValidation.java
@@ -168,12 +168,18 @@ public class ConfigValidation {
      * Validates a list of Strings.
      */
     public static Object StringsValidator = listFv(String.class, true);
-    
+
     /**
      * Validates a map of Strings to Numbers.
      */
     public static Object MapOfStringToNumberValidator = mapFv(String.class, Number.class, true);
 
+    /**
+     * Validates a map of Strings to a map of Strings to a list.
+     * {str -> {str -> [str,str]}
+     */
+    public static Object MapOfStringToMapValidator = mapFv(fv(String.class, false), mapFv(fv(String.class, false), listFv(String.class, false), false), true);
+
     /**
      * Validates is a list of Maps.
      */

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The changes in the diff file involve the addition of a new validator (MapOfStringToMapValidator) for a map structure. This change does not directly impact critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, re-running the security tests is not necessary based on these modifications.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 3
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/StormSubmitter.java b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
index 54cc70100..63cc5a146 100644
--- a/storm-core/src/jvm/backtype/storm/StormSubmitter.java
+++ b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
@@ -175,22 +175,19 @@ public class StormSubmitter {
     }
 
     /**
-     * Submits a topology to run on the cluster. A topology runs forever or until
-     * explicitly killed.
      *
-     *
-     * @param name the name of the storm.
-     * @param stormConf the topology-specific configuration. See {@link Config}.
-     * @param topology the processing to execute.
-     * @param opts to manipulate the starting of the topology
-     * @param progressListener to track the progress of the jar upload process
-     * @throws AlreadyAliveException if a topology with this name is already running
-     * @throws InvalidTopologyException if an invalid topology was submitted
-     * @throws AuthorizationException if authorization is failed
+     * @param name
+     * @param stormConf
+     * @param topology
+     * @param opts
+     * @param progressListener
+     * @param asUser The user as which this topology should be submitted.
+     * @throws AlreadyAliveException
+     * @throws InvalidTopologyException
+     * @throws AuthorizationException
      */
-    @SuppressWarnings("unchecked")
-    public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
-             ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
+    public static void submitTopologyAs(String name, Map stormConf, StormTopology topology, SubmitOptions opts, ProgressListener progressListener, String asUser)
+            throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
         if(!Utils.isValidConf(stormConf)) {
             throw new IllegalArgumentException("Storm conf is not valid. Must be json-serializable");
         }
@@ -218,25 +215,25 @@ public class StormSubmitter {
             if(localNimbus!=null) {
                 LOG.info("Submitting topology " + name + " in local mode");
                 if(opts!=null) {
-                    localNimbus.submitTopologyWithOpts(name, stormConf, topology, opts);                    
+                    localNimbus.submitTopologyWithOpts(name, stormConf, topology, opts);
                 } else {
                     // this is for backwards compatibility
-                    localNimbus.submitTopology(name, stormConf, topology);                                            
+                    localNimbus.submitTopology(name, stormConf, topology);
                 }
             } else {
                 String serConf = JSONValue.toJSONString(stormConf);
-                NimbusClient client = NimbusClient.getConfiguredClient(conf);
-                if(topologyNameExists(conf, name)) {
+                NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser);
+                if(topologyNameExists(conf, name, asUser)) {
                     throw new RuntimeException("Topology with name `" + name + "` already exists on cluster");
                 }
-                String jar = submitJar(conf, progressListener);
+                String jar = submitJarAs(conf, System.getProperty("storm.jar"), progressListener, asUser);
                 try {
                     LOG.info("Submitting topology " +  name + " in distributed mode with conf " + serConf);
                     if(opts!=null) {
-                        client.getClient().submitTopologyWithOpts(name, jar, serConf, topology, opts);                    
+                        client.getClient().submitTopologyWithOpts(name, jar, serConf, topology, opts);
                     } else {
                         // this is for backwards compatibility
-                        client.getClient().submitTopology(name, jar, serConf, topology);                                            
+                        client.getClient().submitTopology(name, jar, serConf, topology);
                     }
                 } catch(InvalidTopologyException e) {
                     LOG.warn("Topology submission exception: "+e.get_msg());
@@ -254,6 +251,26 @@ public class StormSubmitter {
         }
     }
 
+    /**
+     * Submits a topology to run on the cluster. A topology runs forever or until
+     * explicitly killed.
+     *
+     *
+     * @param name the name of the storm.
+     * @param stormConf the topology-specific configuration. See {@link Config}.
+     * @param topology the processing to execute.
+     * @param opts to manipulate the starting of the topology
+     * @param progressListener to track the progress of the jar upload process
+     * @throws AlreadyAliveException if a topology with this name is already running
+     * @throws InvalidTopologyException if an invalid topology was submitted
+     * @throws AuthorizationException if authorization is failed
+     */
+    @SuppressWarnings("unchecked")
+    public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
+             ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
+
+    }
+
     /**
      * Submits a topology to run on the cluster with a progress bar. A topology runs forever or until
      * explicitly killed.
@@ -310,8 +327,8 @@ public class StormSubmitter {
         });
     }
 
-    private static boolean topologyNameExists(Map conf, String name) {
-        NimbusClient client = NimbusClient.getConfiguredClient(conf);
+    private static boolean topologyNameExists(Map conf, String name, String asUser) {
+        NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser);
         try {
             ClusterSummary summary = client.getClient().getClusterInfo();
             for(TopologySummary s : summary.get_topologies()) {
@@ -342,19 +359,13 @@ public class StormSubmitter {
         return submitJar(conf, localJar, null);
     }
 
-    /**
-     * Submit jar file
-     * @param conf the topology-specific configuration. See {@link Config}.
-     * @param localJar file path of the jar file to submit
-     * @param listener progress listener to track the jar file upload
-     * @return the remote location of the submitted jar
-     */
-    public static String submitJar(Map conf, String localJar, ProgressListener listener) {
+
+    public static String submitJarAs(Map conf, String localJar, ProgressListener listener, String asUser) {
         if (localJar == null) {
             throw new RuntimeException("Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
         }
 
-        NimbusClient client = NimbusClient.getConfiguredClient(conf);
+        NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser);
         try {
             String uploadLocation = client.getClient().beginFileUpload();
             LOG.info("Uploading topology jar " + localJar + " to assigned location: " + uploadLocation);
@@ -385,12 +396,23 @@ public class StormSubmitter {
             LOG.info("Successfully uploaded topology jar to assigned location: " + uploadLocation);
             return uploadLocation;
         } catch(Exception e) {
-            throw new RuntimeException(e);            
+            throw new RuntimeException(e);
         } finally {
             client.close();
         }
     }
 
+    /**
+     * Submit jar file
+     * @param conf the topology-specific configuration. See {@link Config}.
+     * @param localJar file path of the jar file to submit
+     * @param listener progress listener to track the jar file upload
+     * @return the remote location of the submitted jar
+     */
+    public static String submitJar(Map conf, String localJar, ProgressListener listener) {
+        return submitJarAs(conf,localJar, listener, null);
+    }
+
     /**
      * Interface use to track progress of file upload
      */

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.command;

import java.util.Arrays;
import java.util.Map;

import org.apache.commons.lang.ArrayUtils;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.NimbusSummary;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.ServerUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class ShellSubmission {
    private static final Logger LOG = LoggerFactory.getLogger(ShellSubmission.class);

    public static void main(String[] args) throws Exception {
        if (args.length <= 1) {
            LOG.error("Arguments should be of the form: <path_to_jar> [argument...]");
            System.exit(-1);
        }
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        try (NimbusClient client = NimbusClient.getConfiguredClient(conf)) {
            NimbusSummary ns = client.getClient().getLeader();
            String host = ns.get_host();
            int port = ns.get_port();
            String jarPath = StormSubmitter.submitJar(conf, args[0]);
            String[] newArgs = (String[]) ArrayUtils.addAll(Arrays.copyOfRange(args, 1, args.length),
                    new String[]{host, String.valueOf(port), jarPath});
            ServerUtils.execCommand(newArgs);
        }
    }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.storm;

import java.util.Map;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.generated.SubmitOptions;
import org.apache.storm.generated.TopologyInitialStatus;
import org.apache.storm.testing.TestWordCounter;
import org.apache.storm.topology.TopologyBuilder;
import org.junit.jupiter.api.Test;

import static org.junit.jupiter.api.Assertions.fail;

public class TestStormSubmitter {

    @Test
    public void invalidTopologyWithoutSpout() {
        String expectedExceptionMsgFragment = "does not have any spout";
        TopologyBuilder tb = new TopologyBuilder();
        tb.setBolt("bolt1", new TestWordCounter(), 10).shuffleGrouping("spout1");
        tb.setBolt("bolt11", new TestWordCounter(), 10).shuffleGrouping("bolt1");
        tb.setBolt("bolt12", new TestWordCounter(), 10).shuffleGrouping("bolt1");
        StormTopology topology = tb.createTopology();
        Map<String, Object> topoConf = null;
        SubmitOptions opts = new SubmitOptions(TopologyInitialStatus.INACTIVE);

        try {
            StormSubmitter.submitTopologyAs("test-topo-without-spout", topoConf, topology, opts, null, "none");
            fail("Topology without spout should fail in submission");
        } catch (InvalidTopologyException ex) {
            if (!ex.getMessage().contains(expectedExceptionMsgFragment)) {
                String err = String.format("Topology submit failure should contain string \"%s\", but is \"%s\"",
                        expectedExceptionMsgFragment, ex.getMessage());
                fail(err);
            }
        } catch (Throwable ex) {
            ex.printStackTrace();
            fail("Unexpected exception submitting topology without spout: " + ex);
        }
    }
}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.sql;

import java.io.BufferedOutputStream;
import java.io.ByteArrayOutputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.List;
import java.util.Map;
import java.util.jar.Attributes;
import java.util.jar.JarOutputStream;
import java.util.jar.Manifest;
import java.util.zip.ZipEntry;
import org.apache.calcite.sql.SqlNode;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.generated.SubmitOptions;
import org.apache.storm.sql.javac.CompilingClassLoader;
import org.apache.storm.sql.parser.SqlCreateFunction;
import org.apache.storm.sql.parser.SqlCreateTable;
import org.apache.storm.sql.parser.StormParser;

class StormSqlImpl extends StormSql {
    private final StormSqlContext sqlContext;

    StormSqlImpl() {
        sqlContext = new StormSqlContext();
    }

    @Override
    public void submit(
        String name, Iterable<String> statements, Map<String, Object> topoConf, SubmitOptions opts,
        StormSubmitter.ProgressListener progressListener, String asUser)
        throws Exception {
        for (String sql : statements) {
            StormParser parser = new StormParser(sql);
            SqlNode node = parser.impl().parseSqlStmtEof();
            if (node instanceof SqlCreateTable) {
                sqlContext.interpretCreateTable((SqlCreateTable) node);
            } else if (node instanceof SqlCreateFunction) {
                sqlContext.interpretCreateFunction((SqlCreateFunction) node);
            } else {
                AbstractStreamsProcessor processor = sqlContext.compileSql(sql);
                StormTopology topo = processor.build();

                Path jarPath = null;
                try {
                    // QueryPlanner on Streams mode configures the topology with compiled classes,
                    // so we need to add new classes into topology jar
                    // Topology will be serialized and sent to Nimbus, and deserialized and executed in workers.

                    jarPath = Files.createTempFile("storm-sql", ".jar");
                    System.setProperty("storm.jar", jarPath.toString());
                    packageTopology(jarPath, processor);
                    StormSubmitter.submitTopologyAs(name, topoConf, topo, opts, progressListener, asUser);
                } finally {
                    if (jarPath != null) {
                        Files.delete(jarPath);
                    }
                }
            }
        }
    }

    @Override
    public void explain(Iterable<String> statements) throws Exception {
        for (String sql : statements) {
            System.out.println("===========================================================");
            System.out.println("query>");
            System.out.println(sql);
            System.out.println("-----------------------------------------------------------");

            StormParser parser = new StormParser(sql);
            SqlNode node = parser.impl().parseSqlStmtEof();
            if (node instanceof SqlCreateTable) {
                sqlContext.interpretCreateTable((SqlCreateTable) node);
                System.out.println("No plan presented on DDL");
            } else if (node instanceof SqlCreateFunction) {
                sqlContext.interpretCreateFunction((SqlCreateFunction) node);
                System.out.println("No plan presented on DDL");
            } else {
                String plan = sqlContext.explain(sql);
                System.out.println("plan>");
                System.out.println(plan);
            }

            System.out.println("===========================================================");
        }
    }

    private void packageTopology(Path jar, AbstractStreamsProcessor processor) throws IOException {
        Manifest manifest = new Manifest();
        Attributes attr = manifest.getMainAttributes();
        attr.put(Attributes.Name.MANIFEST_VERSION, "1.0");
        attr.put(Attributes.Name.MAIN_CLASS, processor.getClass().getCanonicalName());
        try (JarOutputStream out = new JarOutputStream(
            new BufferedOutputStream(new FileOutputStream(jar.toFile())), manifest)) {
            List<CompilingClassLoader> classLoaders = processor.getClassLoaders();
            if (classLoaders != null && !classLoaders.isEmpty()) {
                for (CompilingClassLoader classLoader : classLoaders) {
                    for (Map.Entry<String, ByteArrayOutputStream> e : classLoader.getClasses().entrySet()) {
                        out.putNextEntry(new ZipEntry(e.getKey().replace(".", "/") + ".class"));
                        out.write(e.getValue().toByteArray());
                        out.closeEntry();
                    }
                }
            }
        }
    }
}

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file introduce a new method `submitTopologyAs` that includes an additional parameter `asUser`. This parameter is used to specify the user under which the topology should be submitted. The changes also modify existing methods to use this new parameter, which affects the authentication and authorization mechanisms. Specifically, the changes impact how the Nimbus client is configured and how topologies are submitted and checked for existence. These modifications directly affect critical security components such as authentication and authorization, making it necessary to re-run the security tests to ensure that the new functionality does not introduce any security vulnerabilities.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 5
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java b/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java
index 4033f1870..a252f85b8 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java
@@ -22,6 +22,9 @@ import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.net.InetAddress;
 import com.google.common.annotations.VisibleForTesting;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import java.security.AccessControlContext;
 import java.security.AccessController;
 import java.security.Principal;
@@ -39,6 +42,10 @@ public class ReqContext {
     private InetAddress _remoteAddr;
     private Integer _reqID;
     private Map _storm_conf;
+    private Principal realPrincipal;
+
+    private static final Logger LOG = LoggerFactory.getLogger(ReqContext.class);
+
 
     /**
      * Get a request context associated with current thread
@@ -87,7 +94,7 @@ public class ReqContext {
      * Set remote subject explicitly
      */
     public void setSubject(Subject subject) {
-        _subject = subject;	
+        _subject = subject;
     }
 
     /**
@@ -106,6 +113,24 @@ public class ReqContext {
         if (princs.size()==0) return null;
         return (Principal) (princs.toArray()[0]);
     }
+
+    public void setRealPrincipal(Principal realPrincipal) {
+        this.realPrincipal = realPrincipal;
+    }
+    /**
+     * The real principal associated with the subject.
+     */
+    public Principal realPrincipal() {
+        return this.realPrincipal;
+    }
+
+    /**
+     * Returns true if this request is an impersonation request.
+     * @return
+     */
+    public boolean isImpersonating() {
+        return this.realPrincipal != null;
+    }
     
     /**
      * request ID of this request
@@ -113,4 +138,5 @@ public class ReqContext {
     public Integer requestID() {
         return _reqID;
     }
+
 }

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}
```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file include the addition of new methods and fields related to authentication and impersonation checks. Specifically, the addition of the 'realPrincipal' field, the 'setRealPrincipal' method, the 'realPrincipal' method, and the 'isImpersonating' method directly impact the authentication and authorization mechanisms. These changes are critical to the security of the system as they introduce new ways to handle principals and impersonation, which could have significant security implications. Additionally, the introduction of logging with SLF4J could affect how security events are logged and monitored.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 6
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
index 64bec193b..532ebd726 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
@@ -116,7 +116,6 @@ public abstract class SaslTransportPlugin implements ITransportPlugin {
             TTransport trans = inProt.getTransport();
             //Sasl transport
             TSaslServerTransport saslTrans = (TSaslServerTransport)trans;
-
             //remote address
             TSocket tsocket = (TSocket)saslTrans.getUnderlyingTransport();
             Socket socket = tsocket.getSocket();
@@ -128,7 +127,7 @@ public abstract class SaslTransportPlugin implements ITransportPlugin {
             Subject remoteUser = new Subject();
             remoteUser.getPrincipals().add(new User(authId));
             req_context.setSubject(remoteUser);
-            
+
             //invoke service handler
             return wrapped.process(inProt, outProt);
         }

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The changes in the diff file are minor and do not affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The modifications are limited to the removal of a blank line and the addition of a blank line, which do not have any impact on the functionality or security of the system.",
  "confidence": 10
}
``` -> security_relevancy: not

Diff number: 7
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java
index 0e1a3652a..ab06635fb 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java
@@ -90,8 +90,11 @@ public class SimpleTransportPlugin implements ITransportPlugin {
     /**
      * Connect to the specified server via framed transport 
      * @param transport The underlying Thrift transport.
+     * @param serverHost unused.
+     * @param asUser unused.
      */
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException {
+    @Override
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException {
         //create a framed transport
         TTransport conn = new TFramedTransport(transport);
 

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The changes in the diff file involve adding an unused parameter 'asUser' to the 'connect' method and adding an @Override annotation. These modifications do not directly impact critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, re-running the security tests is not necessary.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 8
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java b/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
index eaed61ff4..f547868f8 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
@@ -40,12 +40,12 @@ public class TBackoffConnect {
                                                               retryTimes);
     }
 
-    public TTransport doConnectWithRetry(ITransportPlugin transportPlugin, TTransport underlyingTransport, String host) throws IOException {
+    public TTransport doConnectWithRetry(ITransportPlugin transportPlugin, TTransport underlyingTransport, String host, String asUser) throws IOException {
         boolean connected = false;
         TTransport transportResult = null;
         while(!connected) {
             try {
-                transportResult = transportPlugin.connect(underlyingTransport, host);
+                transportResult = transportPlugin.connect(underlyingTransport, host, asUser);
                 connected = true;
             } catch (TTransportException ex) {
                 retryNext(ex);

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.util.Map;
import javax.security.auth.login.Configuration;
import org.apache.storm.Config;
import org.apache.storm.thrift.protocol.TBinaryProtocol;
import org.apache.storm.thrift.protocol.TProtocol;
import org.apache.storm.thrift.transport.TSocket;
import org.apache.storm.thrift.transport.TTransport;
import org.apache.storm.utils.ObjectReader;

public class ThriftClient implements AutoCloseable {
    protected TProtocol protocol;
    protected boolean retryForever = false;
    private TTransport transport;
    private String host;
    private Integer port;
    private Integer timeout;
    private Map conf;
    private ThriftConnectionType type;
    private String asUser;

    public ThriftClient(Map<String, Object> topoConf, ThriftConnectionType type, String host) {
        this(topoConf, type, host, null, null, null);
    }

    public ThriftClient(Map<String, Object> topoConf, ThriftConnectionType type, String host, Integer port, Integer timeout) {
        this(topoConf, type, host, port, timeout, null);
    }

    public ThriftClient(Map<String, Object> topoConf, ThriftConnectionType type, String host, Integer port, Integer timeout,
                        String asUser) {
        //create a socket with server
        if (host == null) {
            throw new IllegalArgumentException("host is not set");
        }

        if (port == null) {
            port = type.getPort(topoConf);
        }

        if (timeout == null) {
            timeout = type.getSocketTimeOut(topoConf);
        }

        if (port <= 0 && !type.isFake()) {
            throw new IllegalArgumentException("invalid port: " + port);
        }

        this.host = host;
        this.port = port;
        this.timeout = timeout;
        conf = topoConf;
        this.type = type;
        this.asUser = asUser;
        if (!type.isFake()) {
            reconnect();
        }
    }

    public synchronized TTransport transport() {
        return transport;
    }

    public synchronized void reconnect() {
        close();
        TSocket socket = null;
        try {
            socket = new TSocket(host, port);
            if (timeout != null) {
                socket.setTimeout(timeout);
            }

            //construct a transport plugin
            ITransportPlugin transportPlugin = ClientAuthUtils.getTransportPlugin(type, conf);

            //TODO get this from type instead of hardcoding to Nimbus.
            //establish client-server transport via plugin
            //do retries if the connect fails
            TBackoffConnect connectionRetry
                = new TBackoffConnect(
                ObjectReader.getInt(conf.get(Config.STORM_NIMBUS_RETRY_TIMES)),
                ObjectReader.getInt(conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL)),
                ObjectReader.getInt(conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)),
                    retryForever);
            transport = connectionRetry.doConnectWithRetry(transportPlugin, socket, host, asUser);
        } catch (Exception ex) {
            // close the socket, which releases connection if it has created any.
            if (socket != null) {
                try {
                    socket.close();
                } catch (Exception e) {
                    //ignore
                }
            }
            throw new RuntimeException(ex);
        }
        protocol = null;
        if (transport != null) {
            protocol = new TBinaryProtocol(transport);
        }
    }

    @Override
    public synchronized void close() {
        if (transport != null) {
            transport.close();
            transport = null;
            protocol = null;
        }
    }
}

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file modify the method signature of `doConnectWithRetry` to include an additional parameter `asUser`. This parameter is then passed to the `connect` method of the `ITransportPlugin` interface. Given that this change affects the authentication mechanism by potentially altering how user identities are handled during the connection process, it is critical to re-run the security tests. This ensures that the new parameter does not introduce any vulnerabilities or bypass existing security controls.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 9
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java b/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java
index ec337d990..9f77ab92d 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java
@@ -39,12 +39,17 @@ public class ThriftClient {
     private Integer _timeout;
     private Map _conf;
     private ThriftConnectionType _type;
+    private String _asUser;
 
     public ThriftClient(Map storm_conf, ThriftConnectionType type, String host) {
-        this(storm_conf, type, host, null, null);
+        this(storm_conf, type, host, null, null, null);
     }
 
-    public ThriftClient(Map storm_conf, ThriftConnectionType type, String host, Integer port, Integer timeout) {
+    public ThriftClient(Map storm_conf, ThriftConnectionType type, String host, Integer port, Integer timeout){
+        this(storm_conf, type, host, port, timeout, null);
+    }
+
+    public ThriftClient(Map storm_conf, ThriftConnectionType type, String host, Integer port, Integer timeout, String asUser) {
         //create a socket with server
         if (host==null) {
             throw new IllegalArgumentException("host is not set");
@@ -63,6 +68,7 @@ public class ThriftClient {
         _timeout = timeout;
         _conf = storm_conf;
         _type = type;
+        _asUser = asUser;
         reconnect();
     }
 
@@ -94,7 +100,7 @@ public class ThriftClient {
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_TIMES)),
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL)),
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)));
-            _transport = connectionRetry.doConnectWithRetry(transportPlugin, underlyingTransport, _host);
+            _transport = connectionRetry.doConnectWithRetry(transportPlugin, underlyingTransport, _host, _asUser);
         } catch (IOException ex) {
             throw new RuntimeException(ex);
         }

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file introduce a new parameter '_asUser' to the ThriftClient class, which is used in the connection retry mechanism. This parameter likely affects the authentication process, as it appears to specify the user context for the connection. Modifications to authentication mechanisms are critical security components and warrant re-running security tests to ensure there are no vulnerabilities or unintended side effects introduced by these changes.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 10
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
new file mode 100644
index 000000000..1e947ae67
--- /dev/null
+++ b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
@@ -0,0 +1,148 @@
+package backtype.storm.security.auth.authorizer;
+
+import backtype.storm.Config;
+import backtype.storm.security.auth.*;
+import com.google.common.collect.ImmutableSet;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.*;
+
+
+public class ImpersonationAuthorizer implements IAuthorizer {
+    private static final Logger LOG = LoggerFactory.getLogger(ImpersonationAuthorizer.class);
+
+    protected Map<String, ImpersonationACL> userImpersonationACL;
+    protected IPrincipalToLocal _ptol;
+    protected IGroupMappingServiceProvider _groupMappingProvider;
+
+    @Override
+    public void prepare(Map conf) {
+        userImpersonationACL = new HashMap<String, ImpersonationACL>();
+
+        Map<String, Map<String, List<String>>> userToHostAndGroup = (Map<String, Map<String, List<String>>>) conf.get(Config.NIMBUS_IMPERSONATION_ACL);
+
+        if (userToHostAndGroup != null) {
+            for (String user : userToHostAndGroup.keySet()) {
+                Set<String> groups = ImmutableSet.copyOf(userToHostAndGroup.get(user).get("groups"));
+                Set<String> hosts = ImmutableSet.copyOf(userToHostAndGroup.get(user).get("hosts"));
+                userImpersonationACL.put(user, new ImpersonationACL(user, groups, hosts));
+            }
+        }
+
+        _ptol = AuthUtils.GetPrincipalToLocalPlugin(conf);
+        _groupMappingProvider = AuthUtils.GetGroupMappingServiceProviderPlugin(conf);
+    }
+
+    @Override
+    public boolean permit(ReqContext context, String operation, Map topology_conf) {
+        if (!context.isImpersonating()) {
+            LOG.debug("Not an impersonation attempt.");
+            return true;
+        }
+
+        String impersonatingPrincipal = context.realPrincipal().getName();
+        String impersonatingUser = _ptol.toLocal(context.realPrincipal());
+        String userBeingImpersonated = _ptol.toLocal(context.principal());
+        InetAddress remoteAddress = context.remoteAddress();
+
+        LOG.info("user = {}, principal = {} is attmepting to impersonate user = {} for operation = {} from host = {}",
+                impersonatingUser, impersonatingPrincipal, userBeingImpersonated, operation, remoteAddress);
+
+        /**
+         * no config is present for impersonating principal or user, do not permit impersonation.
+         */
+        if (!userImpersonationACL.containsKey(impersonatingPrincipal) && !userImpersonationACL.containsKey(impersonatingUser)) {
+            LOG.info("user = {}, principal = {} is trying to impersonate user {}, but config {} does not have entry for impersonating user or principal." +
+                    "Please see SECURITY.MD to learn how to configure users for impersonation."
+                    , impersonatingUser, impersonatingPrincipal, userBeingImpersonated, Config.NIMBUS_IMPERSONATION_ACL);
+            return false;
+        }
+
+        ImpersonationACL principalACL = userImpersonationACL.get(impersonatingPrincipal);
+        ImpersonationACL userACL = userImpersonationACL.get(impersonatingUser);
+
+        Set<String> authorizedHosts = new HashSet<String>();
+        Set<String> authorizedGroups = new HashSet<String>();
+
+        if (principalACL != null) {
+            authorizedHosts.addAll(principalACL.authorizedHosts);
+            authorizedGroups.addAll(principalACL.authorizedGroups);
+        }
+
+        if (userACL != null) {
+            authorizedHosts.addAll(userACL.authorizedHosts);
+            authorizedGroups.addAll(userACL.authorizedGroups);
+        }
+
+        LOG.debug("user = {}, principal = {} is allowed to impersonate groups = {} from hosts = {} ",
+                impersonatingUser, impersonatingPrincipal, authorizedGroups, authorizedHosts);
+
+        if (!isAllowedToImpersonateFromHost(authorizedHosts, remoteAddress)) {
+            LOG.info("user = {}, principal = {} is not allowed to impersonate from host {} ",
+                    impersonatingUser, impersonatingPrincipal, remoteAddress);
+            return false;
+        }
+
+        if (!isAllowedToImpersonateUser(authorizedGroups, userBeingImpersonated)) {
+            LOG.info("user = {}, principal = {} is not allowed to impersonate any group that user {} is part of.",
+                    impersonatingUser, impersonatingPrincipal, userBeingImpersonated);
+            return false;
+        }
+
+        LOG.info("Allowing impersonation of user {} by user {}", userBeingImpersonated, impersonatingUser);
+        return true;
+    }
+
+    private boolean isAllowedToImpersonateFromHost(Set<String> authorizedHosts, InetAddress remoteAddress) {
+        return authorizedHosts.contains(remoteAddress.getCanonicalHostName()) ||
+                authorizedHosts.contains(remoteAddress.getHostName()) ||
+                authorizedHosts.contains(remoteAddress.getHostAddress());
+    }
+
+    private boolean isAllowedToImpersonateUser(Set<String> authorizedGroups, String userBeingImpersonated) {
+        Set<String> groups = null;
+        try {
+            groups = _groupMappingProvider.getGroups(userBeingImpersonated);
+        } catch (IOException e) {
+            throw new RuntimeException("failed to get groups for user " + userBeingImpersonated);
+        }
+
+        if (groups == null || groups.isEmpty()) {
+            return false;
+        }
+
+        for (String group : groups) {
+            if (authorizedGroups.contains(group)) {
+                return true;
+            }
+        }
+
+        return false;
+    }
+
+    protected class ImpersonationACL {
+        public String impersonatingUser;
+        //Groups this user is authorized to impersonate.
+        public Set<String> authorizedGroups;
+        //Hosts this user is authorized to impersonate from.
+        public Set<String> authorizedHosts;
+
+        private ImpersonationACL(String impersonatingUser, Set<String> authorizedGroups, Set<String> authorizedHosts) {
+            this.impersonatingUser = impersonatingUser;
+            this.authorizedGroups = authorizedGroups;
+            this.authorizedHosts = authorizedHosts;
+        }
+
+        @Override
+        public String toString() {
+            return "ImpersonationACL{" +
+                    "impersonatingUser='" + impersonatingUser + '\'' +
+                    ", authorizedGroups=" + authorizedGroups +
+                    ", authorizedHosts=" + authorizedHosts +
+                    '}';
+        }
+    }
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}
```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The diff file introduces a new class `ImpersonationAuthorizer` which directly impacts the access control mechanism by adding a new authorization method for impersonation. This class includes methods for preparing user impersonation ACLs, checking permissions for impersonation attempts, and logging these events. Given that this change affects a critical security component (access control), it is essential to re-run the security tests to ensure that the new authorization logic is correctly implemented and does not introduce any security vulnerabilities.",
  "confidence": 10
}
``` -> security_relevancy: security

Diff number: 12
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
index cb6857906..ad642d8b3 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
@@ -51,10 +51,10 @@ public class DigestSaslTransportPlugin extends SaslTransportPlugin {
     }
 
     @Override
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException, IOException {
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {
         ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);
-        TSaslClientTransport wrapper_transport = new TSaslClientTransport(DIGEST, 
-                null, 
+        TSaslClientTransport wrapper_transport = new TSaslClientTransport(DIGEST,
+                null,
                 AuthUtils.SERVICE, 
                 serverHost,
                 null,

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file modify the method signature of the 'connect' method in the DigestSaslTransportPlugin class by adding a new parameter 'asUser'. This method is part of the authentication mechanism, which is a critical security component. Any changes to authentication mechanisms can have significant security implications, and therefore, it is necessary to re-run the security tests to ensure that the modifications do not introduce vulnerabilities or affect the security posture of the system.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 13
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
index a0e4839df..0ff3d35bd 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
@@ -20,8 +20,13 @@ package backtype.storm.security.auth.digest;
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
+
+import backtype.storm.security.auth.ReqContext;
+import backtype.storm.security.auth.SaslTransportPlugin;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
+import javax.security.auth.Subject;
 import javax.security.auth.callback.Callback;
 import javax.security.auth.callback.CallbackHandler;
 import javax.security.auth.callback.NameCallback;
@@ -107,8 +112,19 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private void handleAuthorizeCallback(AuthorizeCallback ac) {
         String authenticationID = ac.getAuthenticationID();
-        LOG.debug("Successfully authenticated client: authenticationID=" + authenticationID);
-        ac.setAuthorizedID(authenticationID);
+        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID + " authorizationID= " + ac.getAuthorizationID());
+
+        //if authorizationId is not set, set it to authenticationId.
+        if(ac.getAuthorizationID() == null) {
+            ac.setAuthorizedID(authenticationID);
+        }
+
+        //When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We
+        //add the authNid as the real user in reqContext's subject which will be used during authorization.
+        if(!authenticationID.equals(ac.getAuthorizationID())) {
+            ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        }
+
         ac.setAuthorized(true);
     }
 }

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file directly affect the authentication mechanism. Specifically, the logging level for successful authentication has been changed from DEBUG to INFO, which could expose sensitive information in logs. Additionally, new logic has been introduced to handle cases where the authentication ID and authorization ID are not equal, including setting the real user in the request context's subject. These changes have significant implications for security, particularly in terms of logging sensitive information and handling impersonation scenarios.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 14
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
index 451f87bb4..b6cccadd2 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
@@ -32,6 +32,8 @@ import javax.security.auth.login.AppConfigurationEntry;
 import javax.security.auth.login.Configuration;
 import javax.security.auth.login.LoginException;
 import javax.security.sasl.Sasl;
+
+import org.apache.commons.lang.StringUtils;
 import org.apache.thrift.transport.TSaslClientTransport;
 import org.apache.thrift.transport.TSaslServerTransport;
 import org.apache.thrift.transport.TTransport;
@@ -51,7 +53,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
 
     public TTransportFactory getServerTransportFactory() throws IOException {
         //create an authentication callback handler
-        CallbackHandler server_callback_handler = new ServerCallbackHandler(login_conf);
+        CallbackHandler server_callback_handler = new ServerCallbackHandler(login_conf, storm_conf);
         
         //login our principal
         Subject subject = null;
@@ -92,7 +94,8 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
         return wrapFactory;
     }
 
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException, IOException {
+    @Override
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {
         //create an authentication callback handler
         ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);
         
@@ -114,7 +117,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
                         +AuthUtils.LOGIN_CONTEXT_CLIENT+"\" in login configuration file "+ login_conf);
         }
 
-        final String principal = getPrincipal(subject); 
+        final String principal = StringUtils.isBlank(asUser) ? getPrincipal(subject) : asUser;
         String serviceName = AuthUtils.get(login_conf, AuthUtils.LOGIN_CONTEXT_CLIENT, "serviceName");
         if (serviceName == null) {
             serviceName = AuthUtils.SERVICE; 
@@ -138,7 +141,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
                     new PrivilegedExceptionAction<Void>() {
                 public Void run() {
                     try {
-                        LOG.debug("do as:"+ principal);
+                        LOG.info("do as:"+ principal);
                         sasalTransport.open();
                     }
                     catch (Exception e) {

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file directly affect the authentication mechanism and logging levels, which are critical security components. Specifically, the modifications include adding a new parameter 'storm_conf' to the ServerCallbackHandler, changing the method signature of 'connect' to include 'asUser', and altering the logging level from DEBUG to INFO. These changes could have significant implications on the security posture of the system, warranting a re-run of the security tests to ensure no vulnerabilities have been introduced.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 15
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
index 9dc75c4db..7b143f0d6 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
@@ -18,22 +18,19 @@
 
 package backtype.storm.security.auth.kerberos;
 
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
+import backtype.storm.security.auth.AuthUtils;
+import backtype.storm.security.auth.ReqContext;
+import backtype.storm.security.auth.SaslTransportPlugin;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import javax.security.auth.callback.Callback;
-import javax.security.auth.callback.CallbackHandler;
-import javax.security.auth.callback.NameCallback;
-import javax.security.auth.callback.PasswordCallback;
-import javax.security.auth.callback.UnsupportedCallbackException;
+
+import javax.security.auth.Subject;
+import javax.security.auth.callback.*;
 import javax.security.auth.login.AppConfigurationEntry;
 import javax.security.auth.login.Configuration;
 import javax.security.sasl.AuthorizeCallback;
-import javax.security.sasl.RealmCallback;
-
-import backtype.storm.security.auth.AuthUtils;
+import java.io.IOException;
+import java.util.Map;
 
 /**
  * SASL server side callback handler
@@ -43,7 +40,7 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private String userName;
 
-    public ServerCallbackHandler(Configuration configuration) throws IOException {
+    public ServerCallbackHandler(Configuration configuration, Map stormConf) throws IOException {
         if (configuration==null) return;
 
         AppConfigurationEntry configurationEntries[] = configuration.getAppConfigurationEntry(AuthUtils.LOGIN_CONTEXT_SERVER);
@@ -52,6 +49,7 @@ public class ServerCallbackHandler implements CallbackHandler {
             LOG.error(errorMessage);
             throw new IOException(errorMessage);
         }
+
     }
 
     public void handle(Callback[] callbacks) throws UnsupportedCallbackException {
@@ -78,9 +76,19 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private void handleAuthorizeCallback(AuthorizeCallback ac) {
         String authenticationID = ac.getAuthenticationID();
-        LOG.debug("Successfully authenticated client: authenticationID=" + authenticationID);
-        ac.setAuthorized(true);
+        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID + " authorizationID= " + ac.getAuthorizationID());
+
+        //if authorizationId is not set, set it to authenticationId.
+        if(ac.getAuthorizationID() == null) {
+            ac.setAuthorizedID(authenticationID);
+        }
 
-        ac.setAuthorizedID(authenticationID);
+        //When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We
+        //add the authNid as the real user in reqContext's subject which will be used during authorization.
+        if(!ac.getAuthenticationID().equals(ac.getAuthorizationID())) {
+            ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        }
+
+        ac.setAuthorized(true);
     }
 }

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file directly affect the authentication mechanism and authorization process. Specifically, the modifications in the `handleAuthorizeCallback` method introduce new logic for handling authorization IDs and potential impersonation scenarios. These changes are critical as they impact how users are authenticated and authorized, which are fundamental security components. Additionally, the constructor of `ServerCallbackHandler` has been modified to accept a new parameter, which could affect the initialization and configuration of the authentication handler. Given the importance of these changes, re-running the security tests is necessary to ensure that the new logic does not introduce any vulnerabilities or regressions.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 16
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java b/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
index 3218e4994..b2a2a7d85 100644
--- a/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
+++ b/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
@@ -41,7 +41,7 @@ public class DRPCClient extends ThriftClient implements DistributedRPC.Iface {
     }
 
     public DRPCClient(Map conf, String host, int port, Integer timeout) throws TTransportException {
-        super(conf, ThriftConnectionType.DRPC, host, port, timeout);
+        super(conf, ThriftConnectionType.DRPC, host, port, timeout, null);
         this.host = host;
         this.port = port;
         this.client = new DistributedRPC.Client(_protocol);

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The change in the constructor of the DRPCClient class modifies the parameters passed to the superclass constructor. Specifically, an additional parameter 'null' is added. This change could potentially affect the initialization of the ThriftClient, which may have implications for the security of the authentication mechanism, connection handling, or other critical security components managed by the superclass. Given the potential impact on the security of the system, it is prudent to re-run the security tests to ensure that no vulnerabilities have been introduced.",
  "confidence": 8
}
``` -> security_relevancy: security

Diff number: 17
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/utils/NimbusClient.java b/storm-core/src/jvm/backtype/storm/utils/NimbusClient.java
index 273e232fb..b17135378 100644
--- a/storm-core/src/jvm/backtype/storm/utils/NimbusClient.java
+++ b/storm-core/src/jvm/backtype/storm/utils/NimbusClient.java
@@ -30,6 +30,7 @@ public class NimbusClient extends ThriftClient {
     private Nimbus.Client _client;
     private static final Logger LOG = LoggerFactory.getLogger(NimbusClient.class);
 
+
     public static NimbusClient getConfiguredClient(Map conf) {
         try {
             String nimbusHost = (String) conf.get(Config.NIMBUS_HOST);
@@ -39,17 +40,31 @@ public class NimbusClient extends ThriftClient {
         }
     }
 
+    public static NimbusClient getConfiguredClientAs(Map conf, String asUser) {
+        try {
+            String nimbusHost = (String) conf.get(Config.NIMBUS_HOST);
+            return new NimbusClient(conf, nimbusHost, null, null, asUser);
+        } catch (TTransportException ex) {
+            throw new RuntimeException(ex);
+        }
+    }
+
     public NimbusClient(Map conf, String host, int port) throws TTransportException {
         this(conf, host, port, null);
     }
 
     public NimbusClient(Map conf, String host, int port, Integer timeout) throws TTransportException {
-        super(conf, ThriftConnectionType.NIMBUS, host, port, timeout);
+        super(conf, ThriftConnectionType.NIMBUS, host, port, timeout, null);
+        _client = new Nimbus.Client(_protocol);
+    }
+
+    public NimbusClient(Map conf, String host, Integer port, Integer timeout, String asUser) throws TTransportException {
+        super(conf, ThriftConnectionType.NIMBUS, host, port, timeout, asUser);
         _client = new Nimbus.Client(_protocol);
     }
 
     public NimbusClient(Map conf, String host) throws TTransportException {
-        super(conf, ThriftConnectionType.NIMBUS, host, null, null);
+        super(conf, ThriftConnectionType.NIMBUS, host, null, null, null);
         _client = new Nimbus.Client(_protocol);
     }
 

```
Context:
```
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.storm;

import java.io.File;
import java.nio.ByteBuffer;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.storm.dependency.DependencyPropertiesParser;
import org.apache.storm.dependency.DependencyUploader;
import org.apache.storm.generated.AlreadyAliveException;
import org.apache.storm.generated.AuthorizationException;
import org.apache.storm.generated.Credentials;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.generated.NotAliveException;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.generated.SubmitOptions;
import org.apache.storm.generated.TopologyInfo;
import org.apache.storm.generated.TopologyInitialStatus;
import org.apache.storm.hooks.SubmitterHookException;
import org.apache.storm.security.auth.ClientAuthUtils;
import org.apache.storm.security.auth.IAutoCredentials;
import org.apache.storm.shade.net.minidev.json.JSONValue;
import org.apache.storm.shade.org.apache.commons.lang.StringUtils;
import org.apache.storm.thrift.TException;
import org.apache.storm.utils.BufferFileInputStream;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Utils;
import org.apache.storm.utils.WrappedInvalidTopologyException;
import org.apache.storm.validation.ConfigValidation;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Use this class to submit topologies to run on the Storm cluster. You should run your program with the "storm jar" command from the
 * command-line, and then use this class to submit your topologies.
 */
public class StormSubmitter {
    public static final Logger LOG = LoggerFactory.getLogger(StormSubmitter.class);
    public static final Pattern zkDigestPattern = Pattern.compile("\\S+:\\S+");
    private static final int THRIFT_CHUNK_SIZE_BYTES = 307200;

    private static String generateZookeeperDigestSecretPayload() {
        return Utils.secureRandomLong() + ":" + Utils.secureRandomLong();
    }

    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
    public static boolean validateZKDigestPayload(String payload) {
        if (payload != null) {
            Matcher m = zkDigestPattern.matcher(payload);
            return m.matches();
        }
        return false;
    }

    public static Map<String, Object> prepareZookeeperAuthentication(Map<String, Object> conf) {
        Map<String, Object> toRet = new HashMap<>();
        String secretPayload = (String) conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);
        // Is the topology ZooKeeper authentication configuration unset?
        if (!conf.containsKey(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD)
                || conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD) == null
                || !validateZKDigestPayload((String) conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD))) {
            secretPayload = generateZookeeperDigestSecretPayload();
            LOG.info("Generated ZooKeeper secret payload for MD5-digest: " + secretPayload);
        }
        toRet.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD, secretPayload);
        // This should always be set to digest.
        toRet.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME, "digest");
        return toRet;
    }

    private static Map<String, String> populateCredentials(Map<String, Object> conf, Map<String, String> creds) {
        Map<String, String> ret = new HashMap<>();
        for (IAutoCredentials autoCred : ClientAuthUtils.getAutoCredentials(conf)) {
            LOG.info("Running " + autoCred);
            autoCred.populateCredentials(ret);
        }
        if (creds != null) {
            ret.putAll(creds);
        }
        return ret;
    }

    /**
     * Push a new set of credentials to the running topology.
     *
     * @param name        the name of the topology to push credentials to.
     * @param topoConf    the topology-specific configuration, if desired. See {@link Config}.
     * @param credentials the credentials to push.
     * @return whether the pushed credential collection is non-empty. Return false if empty.
     * @throws AuthorizationException   if you are not authorized ot push credentials.
     * @throws NotAliveException        if the topology is not alive
     * @throws InvalidTopologyException if any other error happens
     */
    public static boolean pushCredentials(String name, Map<String, Object> topoConf, Map<String, String> credentials)
        throws AuthorizationException, NotAliveException, InvalidTopologyException {
        return pushCredentials(name, topoConf, credentials, null);
    }

    /**
     * Push a new set of credentials to the running topology.
     * Return false if push Creds map is empty, true otherwise.
     *
     * @param name        the name of the topology to push credentials to.
     * @param topoConf    the topology-specific configuration, if desired. See {@link Config}.
     * @param credentials the credentials to push.
     * @param expectedUser the user you expect the topology to be owned by.
     * @return whether the pushed credential collection is non-empty. Return false if empty.
     * @throws AuthorizationException   if you are not authorized ot push credentials.
     * @throws NotAliveException        if the topology is not alive
     * @throws InvalidTopologyException if any other error happens
     */
    public static boolean pushCredentials(String name, Map<String, Object> topoConf, Map<String, String> credentials, String expectedUser)
        throws AuthorizationException, NotAliveException, InvalidTopologyException {
        topoConf = new HashMap<>(topoConf);
        topoConf.putAll(Utils.readCommandLineOpts());
        Map<String, Object> conf = Utils.readStormConfig();
        conf.putAll(topoConf);
        Map<String, String> fullCreds = populateCredentials(conf, credentials);
        if (fullCreds.isEmpty()) {
            LOG.warn("No credentials were found to push to " + name);
            return false;
        }
        try {
            try (NimbusClient client = NimbusClient.getConfiguredClient(conf)) {
                LOG.info("Uploading new credentials to {}", name);
                Credentials creds = new Credentials(fullCreds);
                if (expectedUser != null) {
                    creds.set_topoOwner(expectedUser);
                }
                client.getClient().uploadNewCredentials(name, creds);
            }
            LOG.info("Finished pushing creds to topology: {}", name);
        } catch (TException e) {
            throw new RuntimeException(e);
        }
        return true;
    }


    /**
     * Submits a topology to run on the cluster. A topology runs forever or until explicitly killed.
     *
     * @param name     the name of the storm.
     * @param topoConf the topology-specific configuration. See {@link Config}.
     * @param topology the processing to execute.
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    public static void submitTopology(String name, Map<String, Object> topoConf, StormTopology topology)
        throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
        submitTopology(name, topoConf, topology, null, null);
    }

    /**
     * Submits a topology to run on the cluster. A topology runs forever or until explicitly killed.
     *
     * @param name     the name of the storm.
     * @param topoConf the topology-specific configuration. See {@link Config}.
     * @param topology the processing to execute.
     * @param opts     to manipulate the starting of the topology.
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    public static void submitTopology(String name, Map<String, Object> topoConf, StormTopology topology, SubmitOptions opts)
        throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
        submitTopology(name, topoConf, topology, opts, null);
    }

    /**
     * Submits a topology to run on the cluster. A topology runs forever or until explicitly killed.
     *
     * @param name             the name of the storm.
     * @param topoConf         the topology-specific configuration. See {@link Config}.
     * @param topology         the processing to execute.
     * @param opts             to manipulate the starting of the topology
     * @param progressListener to track the progress of the jar upload process {@link ProgressListener}
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    @SuppressWarnings("unchecked")
    public static void submitTopology(String name, Map<String, Object> topoConf, StormTopology topology, SubmitOptions opts,
            ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException,
            AuthorizationException {
        submitTopologyAs(name, topoConf, topology, opts, progressListener, null);
    }

    /**
     * Submits a topology to run on the cluster as a particular user. A topology runs forever or until explicitly killed.
     *
     * @param asUser The user as which this topology should be submitted.
     * @throws IllegalArgumentException thrown if configs will yield an unschedulable topology. validateConfs validates confs
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    public static void submitTopologyAs(String name, Map<String, Object> topoConf, StormTopology topology, SubmitOptions opts,
                                        ProgressListener progressListener, String asUser)
        throws AlreadyAliveException, InvalidTopologyException, AuthorizationException, IllegalArgumentException {

        //validate topology name first; nothing else should be done if it's invalid.
        Utils.validateTopologyName(name);

        if (!Utils.isValidConf(topoConf)) {
            throw new IllegalArgumentException("Storm conf is not valid. Must be json-serializable");
        }

        if (topology.get_spouts_size() == 0) {
            throw new WrappedInvalidTopologyException("Topology " + name + " does not have any spout");
        }

        topoConf = new HashMap<>(topoConf);
        topoConf.putAll(Utils.readCommandLineOpts());
        Map<String, Object> conf = Utils.readStormConfig();
        conf.putAll(topoConf);
        topoConf.putAll(prepareZookeeperAuthentication(conf));

        validateConfs(conf);

        try {
            Utils.validateCycleFree(topology, name);
        } catch (InvalidTopologyException ex) {
            LOG.warn("", ex);
        }

        Map<String, String> passedCreds = new HashMap<>();
        if (opts != null) {
            Credentials tmpCreds = opts.get_creds();
            if (tmpCreds != null) {
                passedCreds = tmpCreds.get_creds();
            }
        }
        Map<String, String> fullCreds = populateCredentials(conf, passedCreds);
        if (!fullCreds.isEmpty()) {
            if (opts == null) {
                opts = new SubmitOptions(TopologyInitialStatus.ACTIVE);
            }
            opts.set_creds(new Credentials(fullCreds));
        }
        try {
            String serConf = JSONValue.toJSONString(topoConf);
            try (NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser)) {
                if (!isTopologyNameAllowed(name, client)) {
                    throw new RuntimeException("Topology name " + name + " is either not allowed or it already exists on the cluster");
                }

                // Dependency uploading only makes sense for distributed mode
                List<String> jarsBlobKeys = Collections.emptyList();
                List<String> artifactsBlobKeys;

                DependencyUploader uploader = new DependencyUploader();
                try {
                    uploader.init();

                    jarsBlobKeys = uploadDependencyJarsToBlobStore(uploader);

                    artifactsBlobKeys = uploadDependencyArtifactsToBlobStore(uploader);
                } catch (Throwable e) {
                    // remove uploaded jars blobs, not artifacts since they're shared across the cluster
                    uploader.deleteBlobs(jarsBlobKeys);
                    uploader.shutdown();
                    throw e;
                }

                try {
                    setDependencyBlobsToTopology(topology, jarsBlobKeys, artifactsBlobKeys);
                    submitTopologyInDistributeMode(name, topology, opts, progressListener, asUser, conf, serConf, client);
                } catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) {
                    // remove uploaded jars blobs, not artifacts since they're shared across the cluster
                    // Note that we don't handle TException to delete jars blobs
                    // because it's safer to leave some blobs instead of topology not running
                    uploader.deleteBlobs(jarsBlobKeys);
                    throw e;
                } finally {
                    uploader.shutdown();
                }
            }
        } catch (TException e) {
            throw new RuntimeException(e);
        }
        invokeSubmitterHook(name, asUser, conf, topology);

    }

    private static List<String> uploadDependencyJarsToBlobStore(DependencyUploader uploader) {
        LOG.info("Uploading dependencies - jars...");

        DependencyPropertiesParser propertiesParser = new DependencyPropertiesParser();

        String depJarsProp = System.getProperty("storm.dependency.jars", "");
        List<File> depJars = propertiesParser.parseJarsProperties(depJarsProp);

        try {
            return uploader.uploadFiles(depJars, true);
        } catch (Throwable e) {
            throw new RuntimeException(e);
        }
    }

    private static List<String> uploadDependencyArtifactsToBlobStore(DependencyUploader uploader) {
        LOG.info("Uploading dependencies - artifacts...");

        DependencyPropertiesParser propertiesParser = new DependencyPropertiesParser();

        String depArtifactsProp = System.getProperty("storm.dependency.artifacts", "{}");
        Map<String, File> depArtifacts = propertiesParser.parseArtifactsProperties(depArtifactsProp);

        try {
            return uploader.uploadArtifacts(depArtifacts);
        } catch (Throwable e) {
            throw new RuntimeException(e);
        }
    }

    private static void setDependencyBlobsToTopology(StormTopology topology, List<String> jarsBlobKeys, List<String> artifactsBlobKeys) {
        LOG.info("Dependency Blob keys - jars : {} / artifacts : {}", jarsBlobKeys, artifactsBlobKeys);
        topology.set_dependency_jars(jarsBlobKeys);
        topology.set_dependency_artifacts(artifactsBlobKeys);
    }

    private static void submitTopologyInDistributeMode(String name, StormTopology topology, SubmitOptions opts,
                                                       ProgressListener progressListener, String asUser, Map<String, Object> conf,
                                                       String serConf, NimbusClient client) throws TException {
        try {
            String jar = submitJarAs(conf, System.getProperty("storm.jar"), progressListener, client);
            LOG.info("Submitting topology {} in distributed mode with conf {}", name, serConf);
            Utils.addVersions(topology);
            if (opts != null) {
                client.getClient().submitTopologyWithOpts(name, jar, serConf, topology, opts);
            } else {
                // this is for backwards compatibility
                client.getClient().submitTopology(name, jar, serConf, topology);
            }
            LOG.info("Finished submitting topology: {}", name);
        } catch (InvalidTopologyException e) {
            LOG.error("Topology submission exception: {}", e.get_msg());
            throw e;
        } catch (AlreadyAliveException e) {
            LOG.error("Topology already alive exception", e);
            throw e;
        }
    }

    /**
     * Invoke submitter hook.
     * @throws SubmitterHookException This is thrown when any Exception occurs during initialization or invocation of registered {@link
     *     ISubmitterHook}
     */
    private static void invokeSubmitterHook(String name, String asUser, Map<String, Object> topoConf, StormTopology topology) {
        String submissionNotifierClassName = null;
        try {
            if (topoConf.containsKey(Config.STORM_TOPOLOGY_SUBMISSION_NOTIFIER_PLUGIN)) {
                submissionNotifierClassName = topoConf.get(Config.STORM_TOPOLOGY_SUBMISSION_NOTIFIER_PLUGIN).toString();
                LOG.info("Initializing the registered ISubmitterHook [{}]", submissionNotifierClassName);

                if (submissionNotifierClassName == null || submissionNotifierClassName.isEmpty()) {
                    throw new IllegalArgumentException(
                        Config.STORM_TOPOLOGY_SUBMISSION_NOTIFIER_PLUGIN + " property must be a non empty string.");
                }

                ISubmitterHook submitterHook = (ISubmitterHook) Class.forName(submissionNotifierClassName).newInstance();
                TopologyInfo topologyInfo = Utils.getTopologyInfo(name, asUser, topoConf);
                LOG.info("Invoking the registered ISubmitterHook [{}]", submissionNotifierClassName);
                submitterHook.notify(topologyInfo, topoConf, topology);
            }
        } catch (Exception e) {
            LOG.warn("Error occurred in invoking submitter hook:[{}] ", submissionNotifierClassName, e);
            throw new SubmitterHookException(e);
        }
    }

    /**
     * Submits a topology to run on the cluster with a progress bar. A topology runs forever or until explicitly killed.
     *
     * @param name     the name of the storm.
     * @param topoConf the topology-specific configuration. See {@link Config}.
     * @param topology the processing to execute.
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     */

    public static void submitTopologyWithProgressBar(String name, Map<String, Object> topoConf, StormTopology topology) throws
        AlreadyAliveException, InvalidTopologyException, AuthorizationException {
        submitTopologyWithProgressBar(name, topoConf, topology, null);
    }

    /**
     * Submits a topology to run on the cluster with a progress bar. A topology runs forever or until explicitly killed.
     *
     * @param name     the name of the storm.
     * @param topoConf the topology-specific configuration. See {@link Config}.
     * @param topology the processing to execute.
     * @param opts     to manipulate the starting of the topology
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    public static void submitTopologyWithProgressBar(String name, Map<String, Object> topoConf, StormTopology topology,
                                                     SubmitOptions opts) throws AlreadyAliveException, InvalidTopologyException,
        AuthorizationException {
        // show a progress bar so we know we're not stuck (especially on slow connections)
        submitTopology(name, topoConf, topology, opts, new StormSubmitter.ProgressListener() {
            @Override
            public void onStart(String srcFile, String targetFile, long totalBytes) {
                System.out.printf("Start uploading file '%s' to '%s' (%d bytes)\n", srcFile, targetFile, totalBytes);
            }

            @Override
            public void onProgress(String srcFile, String targetFile, long bytesUploaded, long totalBytes) {
                int length = 50;
                int p = (int) ((length * bytesUploaded) / totalBytes);
                String progress = StringUtils.repeat("=", p);
                String todo = StringUtils.repeat(" ", length - p);

                System.out.printf("\r[%s%s] %d / %d", progress, todo, bytesUploaded, totalBytes);
            }

            @Override
            public void onCompleted(String srcFile, String targetFile, long totalBytes) {
                System.out.printf("\nFile '%s' uploaded to '%s' (%d bytes)\n", srcFile, targetFile, totalBytes);
            }
        });
    }

    private static boolean isTopologyNameAllowed(String name, NimbusClient client) {
        try {
            return client.getClient().isTopologyNameAllowed(name);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    /**
     * Submit jar file.
     *
     * @param conf     the topology-specific configuration. See {@link Config}.
     * @param localJar file path of the jar file to submit
     * @return the remote location of the submitted jar
     */
    public static String submitJar(Map<String, Object> conf, String localJar) {
        return submitJar(conf, localJar, null);
    }

    /**
     * Submit jar file.
     *
     * @param conf     the topology-specific configuration. See {@link Config}.
     * @param localJar file path of the jar file to submit
     * @param listener progress listener to track the jar file upload
     * @return the remote location of the submitted jar
     */
    public static String submitJar(Map<String, Object> conf, String localJar, ProgressListener listener) {
        return submitJarAs(conf, localJar, listener, (String) null);
    }

    public static String submitJarAs(Map<String, Object> conf, String localJar, ProgressListener listener, NimbusClient client) {
        if (localJar == null) {
            throw new RuntimeException(
                "Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
        }

        try {
            String uploadLocation = client.getClient().beginFileUpload();
            LOG.info("Uploading topology jar " + localJar + " to assigned location: " + uploadLocation);
            BufferFileInputStream is = new BufferFileInputStream(localJar, THRIFT_CHUNK_SIZE_BYTES);

            long totalSize = new File(localJar).length();
            if (listener != null) {
                listener.onStart(localJar, uploadLocation, totalSize);
            }

            long bytesUploaded = 0;
            while (true) {
                byte[] toSubmit = is.read();
                bytesUploaded += toSubmit.length;
                if (listener != null) {
                    listener.onProgress(localJar, uploadLocation, bytesUploaded, totalSize);
                }

                if (toSubmit.length == 0) {
                    break;
                }
                client.getClient().uploadChunk(uploadLocation, ByteBuffer.wrap(toSubmit));
            }
            client.getClient().finishFileUpload(uploadLocation);

            if (listener != null) {
                listener.onCompleted(localJar, uploadLocation, totalSize);
            }

            LOG.info("Successfully uploaded topology jar to assigned location: " + uploadLocation);
            return uploadLocation;
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    public static String submitJarAs(Map<String, Object> conf, String localJar, ProgressListener listener, String asUser) {
        if (localJar == null) {
            throw new RuntimeException(
                "Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
        }

        try (NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser)) {
            return submitJarAs(conf, localJar, listener, client);
        }
    }

    private static void validateConfs(Map<String, Object> topoConf) throws IllegalArgumentException,
        InvalidTopologyException, AuthorizationException {
        ConfigValidation.validateTopoConf(topoConf);
        Utils.validateTopologyBlobStoreMap(topoConf);
    }

    /**
     * Interface use to track progress of file upload.
     */
    public interface ProgressListener {
        /**
         * called before file is uploaded.
         *
         * @param srcFile    - jar file to be uploaded
         * @param targetFile - destination file
         * @param totalBytes - total number of bytes of the file
         */
        void onStart(String srcFile, String targetFile, long totalBytes);

        /**
         * called whenever a chunk of bytes is uploaded.
         *
         * @param srcFile       - jar file to be uploaded
         * @param targetFile    - destination file
         * @param bytesUploaded - number of bytes transferred so far
         * @param totalBytes    - total number of bytes of the file
         */
        void onProgress(String srcFile, String targetFile, long bytesUploaded, long totalBytes);

        /**
         * called when the file is uploaded.
         *
         * @param srcFile    - jar file to be uploaded
         * @param targetFile - destination file
         * @param totalBytes - total number of bytes of the file
         */
        void onCompleted(String srcFile, String targetFile, long totalBytes);
    }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.storm;

import java.io.File;
import java.nio.ByteBuffer;
import java.util.Collections;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.regex.Matcher;
import java.util.regex.Pattern;

import org.apache.storm.dependency.DependencyPropertiesParser;
import org.apache.storm.dependency.DependencyUploader;
import org.apache.storm.generated.AlreadyAliveException;
import org.apache.storm.generated.AuthorizationException;
import org.apache.storm.generated.Credentials;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.generated.NotAliveException;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.generated.SubmitOptions;
import org.apache.storm.generated.TopologyInfo;
import org.apache.storm.generated.TopologyInitialStatus;
import org.apache.storm.hooks.SubmitterHookException;
import org.apache.storm.security.auth.ClientAuthUtils;
import org.apache.storm.security.auth.IAutoCredentials;
import org.apache.storm.shade.net.minidev.json.JSONValue;
import org.apache.storm.shade.org.apache.commons.lang.StringUtils;
import org.apache.storm.thrift.TException;
import org.apache.storm.utils.BufferFileInputStream;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Utils;
import org.apache.storm.utils.WrappedInvalidTopologyException;
import org.apache.storm.validation.ConfigValidation;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

/**
 * Use this class to submit topologies to run on the Storm cluster. You should run your program with the "storm jar" command from the
 * command-line, and then use this class to submit your topologies.
 */
public class StormSubmitter {
    public static final Logger LOG = LoggerFactory.getLogger(StormSubmitter.class);
    public static final Pattern zkDigestPattern = Pattern.compile("\\S+:\\S+");
    private static final int THRIFT_CHUNK_SIZE_BYTES = 307200;

    private static String generateZookeeperDigestSecretPayload() {
        return Utils.secureRandomLong() + ":" + Utils.secureRandomLong();
    }

    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
    public static boolean validateZKDigestPayload(String payload) {
        if (payload != null) {
            Matcher m = zkDigestPattern.matcher(payload);
            return m.matches();
        }
        return false;
    }

    public static Map<String, Object> prepareZookeeperAuthentication(Map<String, Object> conf) {
        Map<String, Object> toRet = new HashMap<>();
        String secretPayload = (String) conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);
        // Is the topology ZooKeeper authentication configuration unset?
        if (!conf.containsKey(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD)
                || conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD) == null
                || !validateZKDigestPayload((String) conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD))) {
            secretPayload = generateZookeeperDigestSecretPayload();
            LOG.info("Generated ZooKeeper secret payload for MD5-digest: " + secretPayload);
        }
        toRet.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD, secretPayload);
        // This should always be set to digest.
        toRet.put(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME, "digest");
        return toRet;
    }

    private static Map<String, String> populateCredentials(Map<String, Object> conf, Map<String, String> creds) {
        Map<String, String> ret = new HashMap<>();
        for (IAutoCredentials autoCred : ClientAuthUtils.getAutoCredentials(conf)) {
            LOG.info("Running " + autoCred);
            autoCred.populateCredentials(ret);
        }
        if (creds != null) {
            ret.putAll(creds);
        }
        return ret;
    }

    /**
     * Push a new set of credentials to the running topology.
     *
     * @param name        the name of the topology to push credentials to.
     * @param topoConf    the topology-specific configuration, if desired. See {@link Config}.
     * @param credentials the credentials to push.
     * @return whether the pushed credential collection is non-empty. Return false if empty.
     * @throws AuthorizationException   if you are not authorized ot push credentials.
     * @throws NotAliveException        if the topology is not alive
     * @throws InvalidTopologyException if any other error happens
     */
    public static boolean pushCredentials(String name, Map<String, Object> topoConf, Map<String, String> credentials)
        throws AuthorizationException, NotAliveException, InvalidTopologyException {
        return pushCredentials(name, topoConf, credentials, null);
    }

    /**
     * Push a new set of credentials to the running topology.
     * Return false if push Creds map is empty, true otherwise.
     *
     * @param name        the name of the topology to push credentials to.
     * @param topoConf    the topology-specific configuration, if desired. See {@link Config}.
     * @param credentials the credentials to push.
     * @param expectedUser the user you expect the topology to be owned by.
     * @return whether the pushed credential collection is non-empty. Return false if empty.
     * @throws AuthorizationException   if you are not authorized ot push credentials.
     * @throws NotAliveException        if the topology is not alive
     * @throws InvalidTopologyException if any other error happens
     */
    public static boolean pushCredentials(String name, Map<String, Object> topoConf, Map<String, String> credentials, String expectedUser)
        throws AuthorizationException, NotAliveException, InvalidTopologyException {
        topoConf = new HashMap<>(topoConf);
        topoConf.putAll(Utils.readCommandLineOpts());
        Map<String, Object> conf = Utils.readStormConfig();
        conf.putAll(topoConf);
        Map<String, String> fullCreds = populateCredentials(conf, credentials);
        if (fullCreds.isEmpty()) {
            LOG.warn("No credentials were found to push to " + name);
            return false;
        }
        try {
            try (NimbusClient client = NimbusClient.getConfiguredClient(conf)) {
                LOG.info("Uploading new credentials to {}", name);
                Credentials creds = new Credentials(fullCreds);
                if (expectedUser != null) {
                    creds.set_topoOwner(expectedUser);
                }
                client.getClient().uploadNewCredentials(name, creds);
            }
            LOG.info("Finished pushing creds to topology: {}", name);
        } catch (TException e) {
            throw new RuntimeException(e);
        }
        return true;
    }


    /**
     * Submits a topology to run on the cluster. A topology runs forever or until explicitly killed.
     *
     * @param name     the name of the storm.
     * @param topoConf the topology-specific configuration. See {@link Config}.
     * @param topology the processing to execute.
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    public static void submitTopology(String name, Map<String, Object> topoConf, StormTopology topology)
        throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
        submitTopology(name, topoConf, topology, null, null);
    }

    /**
     * Submits a topology to run on the cluster. A topology runs forever or until explicitly killed.
     *
     * @param name     the name of the storm.
     * @param topoConf the topology-specific configuration. See {@link Config}.
     * @param topology the processing to execute.
     * @param opts     to manipulate the starting of the topology.
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    public static void submitTopology(String name, Map<String, Object> topoConf, StormTopology topology, SubmitOptions opts)
        throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
        submitTopology(name, topoConf, topology, opts, null);
    }

    /**
     * Submits a topology to run on the cluster. A topology runs forever or until explicitly killed.
     *
     * @param name             the name of the storm.
     * @param topoConf         the topology-specific configuration. See {@link Config}.
     * @param topology         the processing to execute.
     * @param opts             to manipulate the starting of the topology
     * @param progressListener to track the progress of the jar upload process {@link ProgressListener}
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    @SuppressWarnings("unchecked")
    public static void submitTopology(String name, Map<String, Object> topoConf, StormTopology topology, SubmitOptions opts,
            ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException,
            AuthorizationException {
        submitTopologyAs(name, topoConf, topology, opts, progressListener, null);
    }

    /**
     * Submits a topology to run on the cluster as a particular user. A topology runs forever or until explicitly killed.
     *
     * @param asUser The user as which this topology should be submitted.
     * @throws IllegalArgumentException thrown if configs will yield an unschedulable topology. validateConfs validates confs
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    public static void submitTopologyAs(String name, Map<String, Object> topoConf, StormTopology topology, SubmitOptions opts,
                                        ProgressListener progressListener, String asUser)
        throws AlreadyAliveException, InvalidTopologyException, AuthorizationException, IllegalArgumentException {

        //validate topology name first; nothing else should be done if it's invalid.
        Utils.validateTopologyName(name);

        if (!Utils.isValidConf(topoConf)) {
            throw new IllegalArgumentException("Storm conf is not valid. Must be json-serializable");
        }

        if (topology.get_spouts_size() == 0) {
            throw new WrappedInvalidTopologyException("Topology " + name + " does not have any spout");
        }

        topoConf = new HashMap<>(topoConf);
        topoConf.putAll(Utils.readCommandLineOpts());
        Map<String, Object> conf = Utils.readStormConfig();
        conf.putAll(topoConf);
        topoConf.putAll(prepareZookeeperAuthentication(conf));

        validateConfs(conf);

        try {
            Utils.validateCycleFree(topology, name);
        } catch (InvalidTopologyException ex) {
            LOG.warn("", ex);
        }

        Map<String, String> passedCreds = new HashMap<>();
        if (opts != null) {
            Credentials tmpCreds = opts.get_creds();
            if (tmpCreds != null) {
                passedCreds = tmpCreds.get_creds();
            }
        }
        Map<String, String> fullCreds = populateCredentials(conf, passedCreds);
        if (!fullCreds.isEmpty()) {
            if (opts == null) {
                opts = new SubmitOptions(TopologyInitialStatus.ACTIVE);
            }
            opts.set_creds(new Credentials(fullCreds));
        }
        try {
            String serConf = JSONValue.toJSONString(topoConf);
            try (NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser)) {
                if (!isTopologyNameAllowed(name, client)) {
                    throw new RuntimeException("Topology name " + name + " is either not allowed or it already exists on the cluster");
                }

                // Dependency uploading only makes sense for distributed mode
                List<String> jarsBlobKeys = Collections.emptyList();
                List<String> artifactsBlobKeys;

                DependencyUploader uploader = new DependencyUploader();
                try {
                    uploader.init();

                    jarsBlobKeys = uploadDependencyJarsToBlobStore(uploader);

                    artifactsBlobKeys = uploadDependencyArtifactsToBlobStore(uploader);
                } catch (Throwable e) {
                    // remove uploaded jars blobs, not artifacts since they're shared across the cluster
                    uploader.deleteBlobs(jarsBlobKeys);
                    uploader.shutdown();
                    throw e;
                }

                try {
                    setDependencyBlobsToTopology(topology, jarsBlobKeys, artifactsBlobKeys);
                    submitTopologyInDistributeMode(name, topology, opts, progressListener, asUser, conf, serConf, client);
                } catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) {
                    // remove uploaded jars blobs, not artifacts since they're shared across the cluster
                    // Note that we don't handle TException to delete jars blobs
                    // because it's safer to leave some blobs instead of topology not running
                    uploader.deleteBlobs(jarsBlobKeys);
                    throw e;
                } finally {
                    uploader.shutdown();
                }
            }
        } catch (TException e) {
            throw new RuntimeException(e);
        }
        invokeSubmitterHook(name, asUser, conf, topology);

    }

    private static List<String> uploadDependencyJarsToBlobStore(DependencyUploader uploader) {
        LOG.info("Uploading dependencies - jars...");

        DependencyPropertiesParser propertiesParser = new DependencyPropertiesParser();

        String depJarsProp = System.getProperty("storm.dependency.jars", "");
        List<File> depJars = propertiesParser.parseJarsProperties(depJarsProp);

        try {
            return uploader.uploadFiles(depJars, true);
        } catch (Throwable e) {
            throw new RuntimeException(e);
        }
    }

    private static List<String> uploadDependencyArtifactsToBlobStore(DependencyUploader uploader) {
        LOG.info("Uploading dependencies - artifacts...");

        DependencyPropertiesParser propertiesParser = new DependencyPropertiesParser();

        String depArtifactsProp = System.getProperty("storm.dependency.artifacts", "{}");
        Map<String, File> depArtifacts = propertiesParser.parseArtifactsProperties(depArtifactsProp);

        try {
            return uploader.uploadArtifacts(depArtifacts);
        } catch (Throwable e) {
            throw new RuntimeException(e);
        }
    }

    private static void setDependencyBlobsToTopology(StormTopology topology, List<String> jarsBlobKeys, List<String> artifactsBlobKeys) {
        LOG.info("Dependency Blob keys - jars : {} / artifacts : {}", jarsBlobKeys, artifactsBlobKeys);
        topology.set_dependency_jars(jarsBlobKeys);
        topology.set_dependency_artifacts(artifactsBlobKeys);
    }

    private static void submitTopologyInDistributeMode(String name, StormTopology topology, SubmitOptions opts,
                                                       ProgressListener progressListener, String asUser, Map<String, Object> conf,
                                                       String serConf, NimbusClient client) throws TException {
        try {
            String jar = submitJarAs(conf, System.getProperty("storm.jar"), progressListener, client);
            LOG.info("Submitting topology {} in distributed mode with conf {}", name, serConf);
            Utils.addVersions(topology);
            if (opts != null) {
                client.getClient().submitTopologyWithOpts(name, jar, serConf, topology, opts);
            } else {
                // this is for backwards compatibility
                client.getClient().submitTopology(name, jar, serConf, topology);
            }
            LOG.info("Finished submitting topology: {}", name);
        } catch (InvalidTopologyException e) {
            LOG.error("Topology submission exception: {}", e.get_msg());
            throw e;
        } catch (AlreadyAliveException e) {
            LOG.error("Topology already alive exception", e);
            throw e;
        }
    }

    /**
     * Invoke submitter hook.
     * @throws SubmitterHookException This is thrown when any Exception occurs during initialization or invocation of registered {@link
     *     ISubmitterHook}
     */
    private static void invokeSubmitterHook(String name, String asUser, Map<String, Object> topoConf, StormTopology topology) {
        String submissionNotifierClassName = null;
        try {
            if (topoConf.containsKey(Config.STORM_TOPOLOGY_SUBMISSION_NOTIFIER_PLUGIN)) {
                submissionNotifierClassName = topoConf.get(Config.STORM_TOPOLOGY_SUBMISSION_NOTIFIER_PLUGIN).toString();
                LOG.info("Initializing the registered ISubmitterHook [{}]", submissionNotifierClassName);

                if (submissionNotifierClassName == null || submissionNotifierClassName.isEmpty()) {
                    throw new IllegalArgumentException(
                        Config.STORM_TOPOLOGY_SUBMISSION_NOTIFIER_PLUGIN + " property must be a non empty string.");
                }

                ISubmitterHook submitterHook = (ISubmitterHook) Class.forName(submissionNotifierClassName).newInstance();
                TopologyInfo topologyInfo = Utils.getTopologyInfo(name, asUser, topoConf);
                LOG.info("Invoking the registered ISubmitterHook [{}]", submissionNotifierClassName);
                submitterHook.notify(topologyInfo, topoConf, topology);
            }
        } catch (Exception e) {
            LOG.warn("Error occurred in invoking submitter hook:[{}] ", submissionNotifierClassName, e);
            throw new SubmitterHookException(e);
        }
    }

    /**
     * Submits a topology to run on the cluster with a progress bar. A topology runs forever or until explicitly killed.
     *
     * @param name     the name of the storm.
     * @param topoConf the topology-specific configuration. See {@link Config}.
     * @param topology the processing to execute.
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     */

    public static void submitTopologyWithProgressBar(String name, Map<String, Object> topoConf, StormTopology topology) throws
        AlreadyAliveException, InvalidTopologyException, AuthorizationException {
        submitTopologyWithProgressBar(name, topoConf, topology, null);
    }

    /**
     * Submits a topology to run on the cluster with a progress bar. A topology runs forever or until explicitly killed.
     *
     * @param name     the name of the storm.
     * @param topoConf the topology-specific configuration. See {@link Config}.
     * @param topology the processing to execute.
     * @param opts     to manipulate the starting of the topology
     * @throws AlreadyAliveException    if a topology with this name is already running
     * @throws InvalidTopologyException if an invalid topology was submitted
     * @throws AuthorizationException   if authorization is failed
     * @throws SubmitterHookException if any Exception occurs during initialization or invocation of registered {@link ISubmitterHook}
     */
    public static void submitTopologyWithProgressBar(String name, Map<String, Object> topoConf, StormTopology topology,
                                                     SubmitOptions opts) throws AlreadyAliveException, InvalidTopologyException,
        AuthorizationException {
        // show a progress bar so we know we're not stuck (especially on slow connections)
        submitTopology(name, topoConf, topology, opts, new StormSubmitter.ProgressListener() {
            @Override
            public void onStart(String srcFile, String targetFile, long totalBytes) {
                System.out.printf("Start uploading file '%s' to '%s' (%d bytes)\n", srcFile, targetFile, totalBytes);
            }

            @Override
            public void onProgress(String srcFile, String targetFile, long bytesUploaded, long totalBytes) {
                int length = 50;
                int p = (int) ((length * bytesUploaded) / totalBytes);
                String progress = StringUtils.repeat("=", p);
                String todo = StringUtils.repeat(" ", length - p);

                System.out.printf("\r[%s%s] %d / %d", progress, todo, bytesUploaded, totalBytes);
            }

            @Override
            public void onCompleted(String srcFile, String targetFile, long totalBytes) {
                System.out.printf("\nFile '%s' uploaded to '%s' (%d bytes)\n", srcFile, targetFile, totalBytes);
            }
        });
    }

    private static boolean isTopologyNameAllowed(String name, NimbusClient client) {
        try {
            return client.getClient().isTopologyNameAllowed(name);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    /**
     * Submit jar file.
     *
     * @param conf     the topology-specific configuration. See {@link Config}.
     * @param localJar file path of the jar file to submit
     * @return the remote location of the submitted jar
     */
    public static String submitJar(Map<String, Object> conf, String localJar) {
        return submitJar(conf, localJar, null);
    }

    /**
     * Submit jar file.
     *
     * @param conf     the topology-specific configuration. See {@link Config}.
     * @param localJar file path of the jar file to submit
     * @param listener progress listener to track the jar file upload
     * @return the remote location of the submitted jar
     */
    public static String submitJar(Map<String, Object> conf, String localJar, ProgressListener listener) {
        return submitJarAs(conf, localJar, listener, (String) null);
    }

    public static String submitJarAs(Map<String, Object> conf, String localJar, ProgressListener listener, NimbusClient client) {
        if (localJar == null) {
            throw new RuntimeException(
                "Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
        }

        try {
            String uploadLocation = client.getClient().beginFileUpload();
            LOG.info("Uploading topology jar " + localJar + " to assigned location: " + uploadLocation);
            BufferFileInputStream is = new BufferFileInputStream(localJar, THRIFT_CHUNK_SIZE_BYTES);

            long totalSize = new File(localJar).length();
            if (listener != null) {
                listener.onStart(localJar, uploadLocation, totalSize);
            }

            long bytesUploaded = 0;
            while (true) {
                byte[] toSubmit = is.read();
                bytesUploaded += toSubmit.length;
                if (listener != null) {
                    listener.onProgress(localJar, uploadLocation, bytesUploaded, totalSize);
                }

                if (toSubmit.length == 0) {
                    break;
                }
                client.getClient().uploadChunk(uploadLocation, ByteBuffer.wrap(toSubmit));
            }
            client.getClient().finishFileUpload(uploadLocation);

            if (listener != null) {
                listener.onCompleted(localJar, uploadLocation, totalSize);
            }

            LOG.info("Successfully uploaded topology jar to assigned location: " + uploadLocation);
            return uploadLocation;
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    public static String submitJarAs(Map<String, Object> conf, String localJar, ProgressListener listener, String asUser) {
        if (localJar == null) {
            throw new RuntimeException(
                "Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
        }

        try (NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser)) {
            return submitJarAs(conf, localJar, listener, client);
        }
    }

    private static void validateConfs(Map<String, Object> topoConf) throws IllegalArgumentException,
        InvalidTopologyException, AuthorizationException {
        ConfigValidation.validateTopoConf(topoConf);
        Utils.validateTopologyBlobStoreMap(topoConf);
    }

    /**
     * Interface use to track progress of file upload.
     */
    public interface ProgressListener {
        /**
         * called before file is uploaded.
         *
         * @param srcFile    - jar file to be uploaded
         * @param targetFile - destination file
         * @param totalBytes - total number of bytes of the file
         */
        void onStart(String srcFile, String targetFile, long totalBytes);

        /**
         * called whenever a chunk of bytes is uploaded.
         *
         * @param srcFile       - jar file to be uploaded
         * @param targetFile    - destination file
         * @param bytesUploaded - number of bytes transferred so far
         * @param totalBytes    - total number of bytes of the file
         */
        void onProgress(String srcFile, String targetFile, long bytesUploaded, long totalBytes);

        /**
         * called when the file is uploaded.
         *
         * @param srcFile    - jar file to be uploaded
         * @param targetFile - destination file
         * @param totalBytes - total number of bytes of the file
         */
        void onCompleted(String srcFile, String targetFile, long totalBytes);
    }
}
/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.storm.utils;

import java.io.BufferedReader;
import java.io.ByteArrayInputStream;
import java.io.ByteArrayOutputStream;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.OutputStreamWriter;
import java.io.Serializable;
import java.io.UnsupportedEncodingException;
import java.lang.Thread.UncaughtExceptionHandler;
import java.lang.management.ManagementFactory;
import java.lang.management.ThreadInfo;
import java.net.InetAddress;
import java.net.ServerSocket;
import java.net.URL;
import java.net.URLDecoder;
import java.net.URLEncoder;
import java.net.UnknownHostException;
import java.nio.ByteBuffer;
import java.nio.charset.StandardCharsets;
import java.nio.file.Files;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Base64;
import java.util.Collection;
import java.util.Collections;
import java.util.Enumeration;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.NavigableMap;
import java.util.Set;
import java.util.Stack;
import java.util.TreeMap;
import java.util.UUID;
import java.util.concurrent.Callable;
import java.util.jar.JarFile;
import java.util.regex.Matcher;
import java.util.regex.Pattern;
import java.util.stream.Collectors;
import java.util.zip.GZIPInputStream;
import java.util.zip.GZIPOutputStream;
import java.util.zip.ZipEntry;
import java.util.zip.ZipFile;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.blobstore.BlobStore;
import org.apache.storm.blobstore.ClientBlobStore;
import org.apache.storm.blobstore.NimbusBlobStore;
import org.apache.storm.generated.AuthorizationException;
import org.apache.storm.generated.ComponentCommon;
import org.apache.storm.generated.ComponentObject;
import org.apache.storm.generated.GlobalStreamId;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.generated.KeyNotFoundException;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.NotAliveException;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.generated.TopologyInfo;
import org.apache.storm.generated.TopologySummary;
import org.apache.storm.security.auth.ReqContext;
import org.apache.storm.serialization.SerializationDelegate;
import org.apache.storm.shade.com.google.common.annotations.VisibleForTesting;
import org.apache.storm.shade.com.google.common.collect.Lists;
import org.apache.storm.shade.com.google.common.collect.MapDifference;
import org.apache.storm.shade.com.google.common.collect.Maps;
import org.apache.storm.shade.net.minidev.json.JSONValue;
import org.apache.storm.shade.net.minidev.json.parser.ParseException;
import org.apache.storm.shade.org.apache.commons.io.FileUtils;
import org.apache.storm.shade.org.apache.commons.io.input.ClassLoaderObjectInputStream;
import org.apache.storm.shade.org.apache.commons.lang.StringUtils;
import org.apache.storm.shade.org.apache.zookeeper.ZooDefs;
import org.apache.storm.shade.org.apache.zookeeper.data.ACL;
import org.apache.storm.shade.org.apache.zookeeper.data.Id;
import org.apache.storm.shade.org.yaml.snakeyaml.LoaderOptions;
import org.apache.storm.shade.org.yaml.snakeyaml.Yaml;
import org.apache.storm.shade.org.yaml.snakeyaml.constructor.SafeConstructor;
import org.apache.storm.thrift.TBase;
import org.apache.storm.thrift.TDeserializer;
import org.apache.storm.thrift.TException;
import org.apache.storm.thrift.TSerializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class Utils {
    public static final Logger LOG = LoggerFactory.getLogger(Utils.class);
    public static final String DEFAULT_STREAM_ID = "default";
    private static final Set<Class<?>> defaultAllowedExceptions = Collections.emptySet();
    private static final List<String> LOCALHOST_ADDRESSES = Lists.newArrayList("localhost", "127.0.0.1", "0:0:0:0:0:0:0:1");
    static SerializationDelegate serializationDelegate;
    private static ThreadLocal<TSerializer> threadSer = new ThreadLocal<TSerializer>();
    private static ThreadLocal<TDeserializer> threadDes = new ThreadLocal<TDeserializer>();
    private static ClassLoader cl = null;
    private static Map<String, Object> localConf;
    // A singleton instance allows us to mock delegated static methods in our
    // tests by subclassing.
    private static Utils _instance = new Utils();
    private static String memoizedLocalHostnameString = null;
    public static final Pattern BLOB_KEY_PATTERN =
            Pattern.compile("^[\\w \\t\\._-]+$", Pattern.UNICODE_CHARACTER_CLASS);
    private static final Pattern TOPOLOGY_NAME_REGEX = Pattern.compile("^[^/.:\\\\]+$");

    static {
        localConf = readStormConfig();
        serializationDelegate = getSerializationDelegate(localConf);
    }

    /**
     * Provide an instance of this class for delegates to use.  To mock out delegated methods, provide an instance of a subclass that
     * overrides the implementation of the delegated method.
     *
     * @param u a Utils instance
     * @return the previously set instance
     */
    public static Utils setInstance(Utils u) {
        Utils oldInstance = _instance;
        _instance = u;
        return oldInstance;
    }

    @VisibleForTesting
    public static void setClassLoaderForJavaDeSerialize(ClassLoader cl) {
        Utils.cl = cl;
    }

    @VisibleForTesting
    public static void resetClassLoaderForJavaDeSerialize() {
        Utils.cl = ClassLoader.getSystemClassLoader();
    }

    public static List<URL> findResources(String name) {
        try {
            Enumeration<URL> resources = Thread.currentThread().getContextClassLoader().getResources(name);
            List<URL> ret = new ArrayList<URL>();
            while (resources.hasMoreElements()) {
                ret.add(resources.nextElement());
            }
            return ret;
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    public static Map<String, Object> findAndReadConfigFile(String name, boolean mustExist) {
        InputStream in = null;
        boolean confFileEmpty = false;
        try {
            in = getConfigFileInputStream(name);
            if (null != in) {
                Yaml yaml = new Yaml(new SafeConstructor(new LoaderOptions()));
                @SuppressWarnings("unchecked")
                Map<String, Object> ret = (Map<String, Object>) yaml.load(new InputStreamReader(in));
                if (null != ret) {
                    return new HashMap<>(ret);
                } else {
                    confFileEmpty = true;
                }
            }

            if (mustExist) {
                if (confFileEmpty) {
                    throw new RuntimeException("Config file " + name + " doesn't have any valid storm configs");
                } else {
                    throw new RuntimeException("Could not find config file on classpath " + name);
                }
            } else {
                return new HashMap<>();
            }
        } catch (IOException e) {
            throw new RuntimeException(e);
        } finally {
            if (null != in) {
                try {
                    in.close();
                } catch (IOException e) {
                    throw new RuntimeException(e);
                }
            }
        }
    }

    public static Map<String, Object> findAndReadConfigFile(String name) {
        return findAndReadConfigFile(name, true);
    }

    private static InputStream getConfigFileInputStream(String configFilePath)
        throws IOException {
        if (null == configFilePath) {
            throw new IOException(
                "Could not find config file, name not specified");
        }

        HashSet<URL> resources = new HashSet<URL>(findResources(configFilePath));
        if (resources.isEmpty()) {
            File configFile = new File(configFilePath);
            if (configFile.exists()) {
                return new FileInputStream(configFile);
            }
        } else if (resources.size() > 1) {
            throw new IOException(
                "Found multiple " + configFilePath
                + " resources. You're probably bundling the Storm jars with your topology jar. "
                + resources);
        } else {
            LOG.debug("Using " + configFilePath + " from resources");
            URL resource = resources.iterator().next();
            return resource.openStream();
        }
        return null;
    }

    public static Map<String, Object> readDefaultConfig() {
        return findAndReadConfigFile("defaults.yaml", true);
    }

    /**
     * URL encode the given string using the UTF-8 charset. Once Storm is baselined to Java 11, we can use URLEncoder.encode(String,
     * Charset) instead, which obsoletes this method.
     */
    public static String urlEncodeUtf8(String s) {
        try {
            return URLEncoder.encode(s, StandardCharsets.UTF_8.name());
        } catch (UnsupportedEncodingException e) {
            //This cannot happen since we're using a standard charset
            throw Utils.wrapInRuntime(e);
        }
    }
    
    /**
     * URL decode the given string using the UTF-8 charset. Once Storm is baselined to Java 11, we can use URLDecoder.decode(String,
     * Charset) instead, which obsoletes this method.
     */
    public static String urlDecodeUtf8(String s) {
        try {
            //Once Storm is baselined to Java 11, we can use URLDecoder.decode(String, Charset) instead, which obsoletes this method.
            return URLDecoder.decode(s, StandardCharsets.UTF_8.name());
        } catch (UnsupportedEncodingException e) {
            //This cannot happen since we're using a standard charset
            throw Utils.wrapInRuntime(e);
        }
    }
    
    public static Map<String, Object> readCommandLineOpts() {
        Map<String, Object> ret = new HashMap<>();
        String commandOptions = System.getProperty("storm.options");
        if (commandOptions != null) {
            /*
             Below regex uses negative lookahead to not split in the middle of json objects '{}'
             or json arrays '[]'. This is needed to parse valid json object/arrays passed as options
             via 'storm.cmd' in windows. This is not an issue while using 'storm.py' since it url-encodes
             the options and the below regex just does a split on the commas that separates each option.

             Note:- This regex handles only valid json strings and could produce invalid results
             if the options contain un-encoded invalid json or strings with unmatched '[, ], { or }'. We can
             replace below code with split(",") once 'storm.cmd' is fixed to send url-encoded options.
              */
            String[] configs = commandOptions.split(",(?![^\\[\\]{}]*(]|}))");
            for (String config : configs) {
                config = urlDecodeUtf8(config);
                String[] options = config.split("=", 2);
                if (options.length == 2) {
                    Object val = options[1];
                    try {
                        val = JSONValue.parseWithException(options[1]);
                    } catch (ParseException ignored) {
                        //fall back to string, which is already set
                    }
                    ret.put(options[0], val);
                }
            }
        }
        return ret;
    }

    public static Map<String, Object> readStormConfig() {
        Map<String, Object> ret = readDefaultConfig();
        String confFile = System.getProperty("storm.conf.file");
        Map<String, Object> storm;
        if (confFile == null || confFile.equals("")) {
            storm = findAndReadConfigFile("storm.yaml", false);
        } else {
            storm = findAndReadConfigFile(confFile, true);
        }
        ret.putAll(storm);
        ret.putAll(readCommandLineOpts());
        return ret;
    }

    public static long bitXorVals(List<Long> coll) {
        long result = 0;
        for (Long val : coll) {
            result ^= val;
        }
        return result;
    }

    public static long bitXor(Long a, Long b) {
        return a ^ b;
    }

    /**
     * Adds the user supplied function as a shutdown hook for cleanup. Also adds a function that sleeps for a second and then halts the
     * runtime to avoid any zombie process in case cleanup function hangs.
     */
    public static void addShutdownHookWithForceKillIn1Sec(Runnable func) {
        addShutdownHookWithDelayedForceKill(func, 1);
    }

    /**
     * Adds the user supplied function as a shutdown hook for cleanup. Also adds a function that sleeps for numSecs and then halts the
     * runtime to avoid any zombie process in case cleanup function hangs.
     */
    public static void addShutdownHookWithDelayedForceKill(Runnable func, int numSecs) {
        final Thread sleepKill = new Thread(() -> {
            try {
                LOG.info("Halting after {} seconds", numSecs);
                Time.sleepSecs(numSecs);
                LOG.warn("Forcing Halt... {}", Utils.threadDump());
                Runtime.getRuntime().halt(20);
            } catch (InterruptedException ie) {
                //Ignored/expected...
            } catch (Exception e) {
                LOG.warn("Exception in the ShutDownHook", e);
            }
        }, "ShutdownHook-sleepKill-" + numSecs + "s");
        sleepKill.setDaemon(true);
        Thread shutdownFunc = new Thread(() -> {
            func.run();
            sleepKill.interrupt();
        }, "ShutdownHook-shutdownFunc");
        Runtime.getRuntime().addShutdownHook(shutdownFunc);
        Runtime.getRuntime().addShutdownHook(sleepKill);
    }

    public static boolean isSystemId(String id) {
        return id.startsWith("__");
    }

    /**
     * Creates a thread that calls the given code repeatedly, sleeping for an interval of seconds equal to the return value of the previous
     * call.
     *
     * <p>The given afn may be a callable that returns the number of seconds to sleep, or it may be a Callable that returns another Callable
     * that in turn returns the number of seconds to sleep. In the latter case isFactory.
     *
     * @param afn              the code to call on each iteration
     * @param isDaemon         whether the new thread should be a daemon thread
     * @param eh               code to call when afn throws an exception
     * @param priority         the new thread's priority
     * @param isFactory        whether afn returns a callable instead of sleep seconds
     * @param startImmediately whether to start the thread before returning
     * @param threadName       a suffix to be appended to the thread name
     * @return the newly created thread
     *
     * @see Thread
     */
    public static SmartThread asyncLoop(final Callable afn, boolean isDaemon, final Thread.UncaughtExceptionHandler eh,
                                        int priority, final boolean isFactory, boolean startImmediately,
                                        String threadName) {
        SmartThread thread = new SmartThread(new Runnable() {
            @Override
            public void run() {
                try {
                    final Callable<Long> fn = isFactory ? (Callable<Long>) afn.call() : afn;
                    while (true) {
                        if (Thread.interrupted()) {
                            throw new InterruptedException();
                        }
                        final Long s = fn.call();
                        if (s == null) { // then stop running it
                            break;
                        }
                        if (s > 0) {
                            Time.sleep(s);
                        }
                    }
                } catch (Throwable t) {
                    if (Utils.exceptionCauseIsInstanceOf(
                        InterruptedException.class, t)) {
                        LOG.info("Async loop interrupted!");
                        return;
                    }
                    LOG.error("Async loop died!", t);
                    throw new RuntimeException(t);
                }
            }
        });
        if (eh != null) {
            thread.setUncaughtExceptionHandler(eh);
        } else {
            thread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {
                @Override
                public void uncaughtException(Thread t, Throwable e) {
                    LOG.error("Async loop died!", e);
                    Utils.exitProcess(1, "Async loop died!");
                }
            });
        }
        thread.setDaemon(isDaemon);
        thread.setPriority(priority);
        if (threadName != null && !threadName.isEmpty()) {
            thread.setName(thread.getName() + "-" + threadName);
        }
        if (startImmediately) {
            thread.start();
        }
        return thread;
    }

    /**
     * Convenience method used when only the function and name suffix are given.
     *
     * @param afn        the code to call on each iteration
     * @param threadName a suffix to be appended to the thread name
     * @return the newly created thread
     *
     * @see Thread
     */
    public static SmartThread asyncLoop(final Callable afn, String threadName, final Thread.UncaughtExceptionHandler eh) {
        return asyncLoop(afn, false, eh, Thread.NORM_PRIORITY, false, true,
                         threadName);
    }

    /**
     * Convenience method used when only the function is given.
     *
     * @param afn the code to call on each iteration
     * @return the newly created thread
     */
    public static SmartThread asyncLoop(final Callable afn) {
        return asyncLoop(afn, false, null, Thread.NORM_PRIORITY, false, true,
                         null);
    }

    /**
     * Checks if a throwable is an instance of a particular class.
     *
     * @param klass     The class you're expecting
     * @param throwable The throwable you expect to be an instance of klass
     * @return true if throwable is instance of klass, false otherwise.
     */
    public static boolean exceptionCauseIsInstanceOf(Class klass, Throwable throwable) {
        return unwrapTo(klass, throwable) != null;
    }

    public static <T extends Throwable> T unwrapTo(Class<T> klass, Throwable t) {
        while (t != null) {
            if (klass.isInstance(t)) {
                return (T) t;
            }
            t = t.getCause();
        }
        return null;
    }

    public static <T extends Throwable> void unwrapAndThrow(Class<T> klass, Throwable t) throws T {
        T ret = unwrapTo(klass, t);
        if (ret != null) {
            throw ret;
        }
    }

    public static RuntimeException wrapInRuntime(Exception e) {
        if (e instanceof RuntimeException) {
            return (RuntimeException) e;
        } else {
            return new RuntimeException(e);
        }
    }

    public static long secureRandomLong() {
        return UUID.randomUUID().getLeastSignificantBits();
    }

    /**
     * Gets the storm.local.hostname value, or tries to figure out the local hostname if it is not set in the config.
     *
     * @return a string representation of the hostname.
     */
    public static String hostname() throws UnknownHostException {
        return _instance.hostnameImpl();
    }

    public static String localHostname() throws UnknownHostException {
        return _instance.localHostnameImpl();
    }

    public static void exitProcess(int val, String msg) {
        String combinedErrorMessage = "Halting process: " + msg;
        LOG.error(combinedErrorMessage, new RuntimeException(combinedErrorMessage));
        Runtime.getRuntime().exit(val);
    }

    public static String uuid() {
        return UUID.randomUUID().toString();
    }

    public static byte[] javaSerialize(Object obj) {
        try {
            ByteArrayOutputStream bos = new ByteArrayOutputStream();
            ObjectOutputStream oos = new ObjectOutputStream(bos);
            oos.writeObject(obj);
            oos.close();
            return bos.toByteArray();
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    public static <T> T javaDeserialize(byte[] serialized, Class<T> clazz) {
        if ("true".equalsIgnoreCase(System.getProperty("java.deserialization.disabled"))) {
            throw new AssertionError("java deserialization has been disabled and is only safe from within a worker process");
        }

        try {
            ByteArrayInputStream bis = new ByteArrayInputStream(serialized);
            ObjectInputStream ois = null;
            if (null == Utils.cl) {
                ois = new ObjectInputStream(bis);
            } else {
                // Use custom class loader set in testing environment
                ois = new ClassLoaderObjectInputStream(Utils.cl, bis);
            }
            Object ret = ois.readObject();
            ois.close();
            return (T) ret;
        } catch (IOException ioe) {
            throw new RuntimeException(ioe);
        } catch (ClassNotFoundException e) {
            throw new RuntimeException(e);
        }
    }

    public static <S, T> T get(Map<S, T> m, S key, T def) {
        T ret = m.get(key);
        if (ret == null) {
            ret = def;
        }
        return ret;
    }

    public static double zeroIfNaNOrInf(double x) {
        return (Double.isNaN(x) || Double.isInfinite(x)) ? 0.0 : x;
    }

    public static <T> String join(Iterable<T> coll, String sep) {
        Iterator<T> it = coll.iterator();
        StringBuilder ret = new StringBuilder();
        while (it.hasNext()) {
            ret.append(it.next());
            if (it.hasNext()) {
                ret.append(sep);
            }
        }
        return ret.toString();
    }

    public static Id parseZkId(String id, String configName) {
        String[] split = id.split(":", 2);
        if (split.length != 2) {
            throw new IllegalArgumentException(configName + " does not appear to be in the form scheme:acl, i.e. sasl:storm-user");
        }
        return new Id(split[0], split[1]);
    }

    /**
     * Get the ACL for nimbus/supervisor.  The Super User ACL. This assumes that security is enabled.
     *
     * @param conf the config to get the super User ACL from
     * @return the super user ACL.
     */
    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
    public static ACL getSuperUserAcl(Map<String, Object> conf) {
        String stormZKUser = (String) conf.get(Config.STORM_ZOOKEEPER_SUPERACL);
        if (stormZKUser == null) {
            throw new IllegalArgumentException("Authentication is enabled but " + Config.STORM_ZOOKEEPER_SUPERACL + " is not set");
        }
        return new ACL(ZooDefs.Perms.ALL, parseZkId(stormZKUser, Config.STORM_ZOOKEEPER_SUPERACL));
    }

    /**
     * Get the ZK ACLs that a worker should use when writing to ZK.
     *
     * @param conf the config for the topology.
     * @return the ACLs
     */
    @SuppressWarnings("checkstyle:AbbreviationAsWordInName")
    public static List<ACL> getWorkerACL(Map<String, Object> conf) {
        if (!isZkAuthenticationConfiguredTopology(conf)) {
            return null;
        }
        ArrayList<ACL> ret = new ArrayList<>(ZooDefs.Ids.CREATOR_ALL_ACL);
        ret.add(getSuperUserAcl(conf));
        return ret;
    }

    /**
     * Is the topology configured to have ZooKeeper authentication.
     *
     * @param conf the topology configuration
     * @return true if ZK is configured else false
     */
    public static boolean isZkAuthenticationConfiguredTopology(Map<String, Object> conf) {
        return (conf != null
                && conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME) != null
                && !((String) conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME)).isEmpty());
    }

    /**
     * Handles uncaught exceptions.
     *
     * @param worker true if this is for handling worker exceptions
     */
    public static void handleUncaughtException(Throwable t, Set<Class<?>> allowedExceptions, boolean worker) {
        if (t != null) {
            if (t instanceof OutOfMemoryError) {
                try {
                    System.err.println("Halting due to Out Of Memory Error..." + Thread.currentThread().getName());
                } catch (Throwable err) {
                    //Again we don't want to exit because of logging issues.
                }
                Runtime.getRuntime().halt(-1);
            }
        }

        if (allowedExceptions.contains(t.getClass())) {
            LOG.info("Swallowing {} {}", t.getClass(), t);
            return;
        }

        if (worker && isAllowedWorkerException(t)) {
            LOG.info("Swallowing {} {}", t.getClass(), t);
            return;
        }

        //Running in daemon mode, we would pass Error to calling thread.
        throw new Error(t);
    }

    public static void handleUncaughtException(Throwable t) {
        handleUncaughtException(t, defaultAllowedExceptions, false);
    }

    public static void handleWorkerUncaughtException(Throwable t) {
        handleUncaughtException(t, defaultAllowedExceptions, true);
    }

    // Hadoop UserGroupInformation can launch an autorenewal thread that can cause a NullPointerException
    // for workers.  See STORM-3606 for an explanation.
    private static boolean isAllowedWorkerException(Throwable t) {
        if (t instanceof NullPointerException) {
            StackTraceElement[] stackTrace = t.getStackTrace();
            for (StackTraceElement trace : stackTrace) {
                if (trace.getClassName().startsWith("org.apache.hadoop.security.UserGroupInformation")
                        && trace.getMethodName().equals("run")) {
                    return true;
                }
            }
        }
        return false;
    }

    public static byte[] thriftSerialize(TBase t) {
        try {
            TSerializer ser = threadSer.get();
            if (ser == null) {
                ser = new TSerializer();
                threadSer.set(ser);
            }
            return ser.serialize(t);
        } catch (TException e) {
            LOG.error("Failed to serialize to thrift: ", e);
            throw new RuntimeException(e);
        }
    }

    public static <T> T thriftDeserialize(Class<T> c, byte[] b) {
        try {
            return thriftDeserialize(c, b, 0, b.length);
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    public static <T> T thriftDeserialize(Class<T> c, byte[] b, int offset, int length) {
        try {
            T ret = c.newInstance();
            TDeserializer des = getDes();
            des.deserialize((TBase) ret, b, offset, length);
            return ret;
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    private static TDeserializer getDes() {
        try {
            TDeserializer des = threadDes.get();
            if (des == null) {
                des = new TDeserializer();
                threadDes.set(des);
            }
            return des;
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    public static void sleepNoSimulation(long millis) {
        try {
            Thread.sleep(millis);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new RuntimeException(e);
        }
    }
    
    public static void sleep(long millis) {
        try {
            Time.sleep(millis);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            throw new RuntimeException(e);
        }
    }

    public static UptimeComputer makeUptimeComputer() {
        return _instance.makeUptimeComputerImpl();
    }

    /**
     * <code>"{:a 1 :b 1 :c 2} -> {1 [:a :b] 2 :c}"</code>.
     *
     * <p>Example usage in java:
     * <code>Map&lt;Integer, String&gt; tasks; Map&lt;String, List&lt;Integer&gt;&gt; componentTasks = Utils.reverse_map(tasks);</code>
     *
     * <p>The order of he resulting list values depends on the ordering properties of the Map passed in. The caller is
     * responsible for passing an ordered map if they expect the result to be consistently ordered as well.
     *
     * @param map to reverse
     * @return a reversed map
     */
    public static <K, V> HashMap<V, List<K>> reverseMap(Map<K, V> map) {
        HashMap<V, List<K>> rtn = new HashMap<V, List<K>>();
        if (map == null) {
            return rtn;
        }
        for (Map.Entry<K, V> entry : map.entrySet()) {
            K key = entry.getKey();
            V val = entry.getValue();
            List<K> list = rtn.get(val);
            if (list == null) {
                list = new ArrayList<K>();
                rtn.put(entry.getValue(), list);
            }
            list.add(key);
        }
        return rtn;
    }

    /**
     * "[[:a 1] [:b 1] [:c 2]} -> {1 [:a :b] 2 :c}" Reverses an assoc-list style Map like reverseMap(Map...)
     *
     * @param listSeq to reverse
     * @return a reversed map
     */
    public static Map<Object, List<Object>> reverseMap(List<List<Object>> listSeq) {
        Map<Object, List<Object>> rtn = new HashMap<>();
        if (listSeq == null) {
            return rtn;
        }
        for (List<Object> listEntry : listSeq) {
            Object key = listEntry.get(0);
            Object val = listEntry.get(1);
            List<Object> list = rtn.get(val);
            if (list == null) {
                list = new ArrayList<>();
                rtn.put(val, list);
            }
            list.add(key);
        }
        return rtn;
    }

    public static boolean isOnWindows() {
        if (System.getenv("OS") != null) {
            return System.getenv("OS").equals("Windows_NT");
        }
        return false;
    }

    public static boolean checkFileExists(String path) {
        return Files.exists(new File(path).toPath());
    }

    /**
     * Deletes a file or directory and its contents if it exists. Does not complain if the input is null or does not exist.
     *
     * @param path the path to the file or directory
     */
    public static void forceDelete(String path) throws IOException {
        _instance.forceDeleteImpl(path);
    }

    public static byte[] serialize(Object obj) {
        return serializationDelegate.serialize(obj);
    }

    public static <T> T deserialize(byte[] serialized, Class<T> clazz) {
        return serializationDelegate.deserialize(serialized, clazz);
    }

    /**
     * Serialize an object using the configured serialization and then base64 encode it into a string.
     *
     * @param obj the object to encode
     * @return a string with the encoded object in it.
     */
    public static String serializeToString(Object obj) {
        return Base64.getEncoder().encodeToString(serializationDelegate.serialize(obj));
    }

    /**
     * Deserialize an object stored in a string. The String is assumed to be a base64 encoded string containing the bytes to actually
     * deserialize.
     *
     * @param str   the encoded string.
     * @param clazz the thrift class we are expecting.
     * @return the decoded object
     */
    public static <T> T deserializeFromString(String str, Class<T> clazz) {
        return deserialize(Base64.getDecoder().decode(str), clazz);
    }

    public static byte[] toByteArray(ByteBuffer buffer) {
        byte[] ret = new byte[buffer.remaining()];
        buffer.get(ret, 0, ret.length);
        return ret;
    }

    public static Runnable mkSuicideFn() {
        return new Runnable() {
            @Override
            public void run() {
                exitProcess(1, "Worker died");
            }
        };
    }

    public static void readAndLogStream(String prefix, InputStream in) {
        try {
            BufferedReader r = new BufferedReader(new InputStreamReader(in));
            String line = null;
            while ((line = r.readLine()) != null) {
                LOG.info("{}:{}", prefix, line);
            }
        } catch (IOException e) {
            LOG.warn("Error while trying to log stream", e);
        }
    }

    /**
     * Creates an instance of the pluggable SerializationDelegate or falls back to DefaultSerializationDelegate if something goes wrong.
     *
     * @param topoConf The config from which to pull the name of the pluggable class.
     * @return an instance of the class specified by storm.meta.serialization.delegate
     */
    private static SerializationDelegate getSerializationDelegate(Map<String, Object> topoConf) {
        String delegateClassName = (String) topoConf.get(Config.STORM_META_SERIALIZATION_DELEGATE);
        SerializationDelegate delegate;
        try {
            Class delegateClass = Class.forName(delegateClassName);
            delegate = (SerializationDelegate) delegateClass.newInstance();
        } catch (ClassNotFoundException | InstantiationException | IllegalAccessException e) {
            throw new RuntimeException("Failed to construct serialization delegate class " + delegateClassName, e);
        }
        delegate.prepare(topoConf);
        return delegate;
    }

    public static ComponentCommon getComponentCommon(StormTopology topology, String id) {
        if (topology.get_spouts().containsKey(id)) {
            return topology.get_spouts().get(id).get_common();
        }
        if (topology.get_bolts().containsKey(id)) {
            return topology.get_bolts().get(id).get_common();
        }
        if (topology.get_state_spouts().containsKey(id)) {
            return topology.get_state_spouts().get(id).get_common();
        }
        throw new IllegalArgumentException("Could not find component with id " + id);
    }

    public static List<Object> tuple(Object... values) {
        List<Object> ret = new ArrayList<Object>();
        for (Object v : values) {
            ret.add(v);
        }
        return ret;
    }

    public static byte[] gzip(byte[] data) {
        try {
            ByteArrayOutputStream bos = new ByteArrayOutputStream();
            GZIPOutputStream out = new GZIPOutputStream(bos);
            out.write(data);
            out.close();
            return bos.toByteArray();
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    public static byte[] gunzip(byte[] data) {
        try {
            ByteArrayOutputStream bos = new ByteArrayOutputStream();
            ByteArrayInputStream bis = new ByteArrayInputStream(data);
            GZIPInputStream in = new GZIPInputStream(bis);
            byte[] buffer = new byte[1024];
            int len = 0;
            while ((len = in.read(buffer)) >= 0) {
                bos.write(buffer, 0, len);
            }
            in.close();
            bos.close();
            return bos.toByteArray();
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    public static List<String> getRepeat(List<String> list) {
        List<String> rtn = new ArrayList<String>();
        Set<String> idSet = new HashSet<String>();

        for (String id : list) {
            if (idSet.contains(id)) {
                rtn.add(id);
            } else {
                idSet.add(id);
            }
        }

        return rtn;
    }

    public static GlobalStreamId getGlobalStreamId(String componentId, String streamId) {
        if (streamId == null) {
            return new GlobalStreamId(componentId, DEFAULT_STREAM_ID);
        }
        return new GlobalStreamId(componentId, streamId);
    }

    public static Object getSetComponentObject(ComponentObject obj) {
        if (obj.getSetField() == ComponentObject._Fields.SERIALIZED_JAVA) {
            return javaDeserialize(obj.get_serialized_java(), Serializable.class);
        } else if (obj.getSetField() == ComponentObject._Fields.JAVA_OBJECT) {
            return obj.get_java_object();
        } else {
            return obj.get_shell();
        }
    }

    /**
     * A cheap way to deterministically convert a number to a positive value. When the input is positive, the original value is returned.
     * When the input number is negative, the returned positive value is the original value bit AND against Integer.MAX_VALUE(0x7fffffff)
     * which is not its absolutely value.
     *
     * @param number a given number
     * @return a positive number.
     */
    public static int toPositive(int number) {
        return number & Integer.MAX_VALUE;
    }

    /**
     * Get process PID.
     * @return the pid of this JVM, because Java doesn't provide a real way to do this.
     */
    public static String processPid() {
        String name = ManagementFactory.getRuntimeMXBean().getName();
        String[] split = name.split("@");
        if (split.length != 2) {
            throw new RuntimeException("Got unexpected process name: " + name);
        }
        return split[0];
    }

    public static Map<String, Object> fromCompressedJsonConf(byte[] serialized) {
        try {
            ByteArrayInputStream bis = new ByteArrayInputStream(serialized);
            InputStreamReader in = new InputStreamReader(new GZIPInputStream(bis));
            Object ret = JSONValue.parseWithException(in);
            in.close();
            return (Map<String, Object>) ret;
        } catch (IOException | ParseException e) {
            throw new RuntimeException(e);
        }
    }

    /**
     * Creates a new map with a string value in the map replaced with an equivalently-lengthed string of '#'.  (If the object is not a
     * string to string will be called on it and replaced)
     *
     * @param m   The map that a value will be redacted from
     * @param key The key pointing to the value to be redacted
     * @return a new map with the value redacted. The original map will not be modified.
     */
    public static Map<String, Object> redactValue(Map<String, Object> m, String key) {
        if (m.containsKey(key)) {
            HashMap<String, Object> newMap = new HashMap<>(m);
            Object value = newMap.get(key);
            String v = value.toString();
            String redacted = new String(new char[v.length()]).replace("\0", "#");
            newMap.put(key, redacted);
            return newMap;
        }
        return m;
    }

    public static UncaughtExceptionHandler createDefaultUncaughtExceptionHandler() {
        return (thread, thrown) -> {
            try {
                handleUncaughtException(thrown);
            } catch (Error err) {
                LOG.error("Received error in thread {}.. terminating server...", thread.getName(), err);
                Runtime.getRuntime().exit(-2);
            }
        };
    }

    public static UncaughtExceptionHandler createWorkerUncaughtExceptionHandler() {
        return (thread, thrown) -> {
            try {
                handleWorkerUncaughtException(thrown);
            } catch (Error err) {
                LOG.error("Received error in thread {}.. terminating worker...", thread.getName(), err);
                Runtime.getRuntime().exit(-2);
            }
        };
    }

    public static void setupDefaultUncaughtExceptionHandler() {
        Thread.setDefaultUncaughtExceptionHandler(createDefaultUncaughtExceptionHandler());
    }

    public static void setupWorkerUncaughtExceptionHandler() {
        Thread.setDefaultUncaughtExceptionHandler(createWorkerUncaughtExceptionHandler());
    }

    /**
     * parses the arguments to extract jvm heap memory size in MB.
     *
     * @return the value of the JVM heap memory setting (in MB) in a java command.
     */
    public static Double parseJvmHeapMemByChildOpts(List<String> options, Double defaultValue) {
        if (options != null) {
            Pattern optsPattern = Pattern.compile("Xmx([0-9]+)([mkgMKG])");
            for (String option : options) {
                if (option == null) {
                    continue;
                }
                Matcher m = optsPattern.matcher(option);
                while (m.find()) {
                    long value = Long.parseLong(m.group(1));
                    char unitChar = m.group(2).toLowerCase().charAt(0);
                    int unit;
                    switch (unitChar) {
                        case 'k':
                            unit = 1024;
                            break;
                        case 'm':
                            unit = 1024 * 1024;
                            break;
                        case 'g':
                            unit = 1024 * 1024 * 1024;
                            break;
                        default:
                            unit = 1;
                    }
                    Double result = value * unit / 1024.0 / 1024.0;
                    return (result < 1.0) ? 1.0 : result;
                }
            }
            return defaultValue;
        } else {
            return defaultValue;
        }
    }

    public static ClientBlobStore getClientBlobStore(Map<String, Object> conf) {
        ClientBlobStore store = (ClientBlobStore) ReflectionUtils.newInstance((String) conf.get(Config.CLIENT_BLOBSTORE));
        store.prepare(conf);
        return store;
    }

    @SuppressWarnings("unchecked")
    private static Object normalizeConfValue(Object obj) {
        if (obj instanceof Map) {
            return normalizeConf((Map<String, Object>) obj);
        } else if (obj instanceof Collection) {
            List<Object> confList = new ArrayList<>((Collection<Object>) obj);
            for (int i = 0; i < confList.size(); i++) {
                Object val = confList.get(i);
                confList.set(i, normalizeConfValue(val));
            }
            return confList;
        } else if (obj instanceof Integer) {
            return ((Number) obj).longValue();
        } else if (obj instanceof Float) {
            return ((Float) obj).doubleValue();
        } else {
            return obj;
        }
    }

    private static Map<String, Object> normalizeConf(Map<String, Object> conf) {
        if (conf == null) {
            return new HashMap<>();
        }
        Map<String, Object> ret = new HashMap<>(conf);
        for (Map.Entry<String, Object> entry : ret.entrySet()) {
            ret.put(entry.getKey(), normalizeConfValue(entry.getValue()));
        }
        return ret;
    }

    @SuppressWarnings("unchecked")
    public static boolean isValidConf(Map<String, Object> topoConfIn) {
        Map<String, Object> origTopoConf = normalizeConf(topoConfIn);
        try {
            Map<String, Object> deserTopoConf = normalizeConf(
                (Map<String, Object>) JSONValue.parseWithException(JSONValue.toJSONString(topoConfIn)));
            return isValidConf(origTopoConf, deserTopoConf);
        } catch (ParseException e) {
            LOG.error("Json serialized config could not be deserialized", e);
        }
        return false;
    }

    @VisibleForTesting
    static boolean isValidConf(Map<String, Object> orig, Map<String, Object> deser) {
        MapDifference<String, Object> diff = Maps.difference(orig, deser);
        if (diff.areEqual()) {
            return true;
        }
        for (Map.Entry<String, Object> entryOnLeft : diff.entriesOnlyOnLeft().entrySet()) {
            LOG.warn("Config property ({}) is found in original config, but missing from the "
                     + "serialized-deserialized config. This is due to an internal error in "
                     + "serialization. Name: {} - Value: {}",
                     entryOnLeft.getKey(), entryOnLeft.getKey(), entryOnLeft.getValue());
        }
        for (Map.Entry<String, Object> entryOnRight : diff.entriesOnlyOnRight().entrySet()) {
            LOG.warn("Config property ({}) is not found in original config, but present in "
                     + "serialized-deserialized config. This is due to an internal error in "
                     + "serialization. Name: {} - Value: {}",
                     entryOnRight.getKey(), entryOnRight.getKey(), entryOnRight.getValue());
        }
        for (Map.Entry<String, MapDifference.ValueDifference<Object>> entryDiffers : diff.entriesDiffering().entrySet()) {
            Object leftValue = entryDiffers.getValue().leftValue();
            Object rightValue = entryDiffers.getValue().rightValue();
            LOG.warn("Config value differs after json serialization. Name: {} - Original Value: {} - DeSer. Value: {}",
                     entryDiffers.getKey(), leftValue, rightValue);
        }
        return false;
    }

    public static TopologyInfo getTopologyInfo(String name, String asUser, Map<String, Object> topoConf) {
        try (NimbusClient client = NimbusClient.getConfiguredClientAs(topoConf, asUser)) {
            return client.getClient().getTopologyInfoByName(name);
        } catch (NotAliveException notAliveException) {
            return null;
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
    }

    public static String getTopologyId(String name, Nimbus.Iface client) {
        try {
            TopologySummary topologySummary = client.getTopologySummaryByName(name);
            if (topologySummary != null) {
                return topologySummary.get_id();
            }
        } catch (NotAliveException notAliveException) {
            return null;
        } catch (Exception e) {
            throw new RuntimeException(e);
        }
        return null;
    }

    /**
     * Validate topology blobstore map.
     *
     * @param topoConf Topology configuration
     */
    public static void validateTopologyBlobStoreMap(Map<String, Object> topoConf) throws InvalidTopologyException, AuthorizationException {
        try (NimbusBlobStore client = new NimbusBlobStore()) {
            client.prepare(topoConf);
            validateTopologyBlobStoreMap(topoConf, client);
        }
    }

    /**
     * Validate topology blobstore map.
     *
     * @param topoConf Topology configuration
     * @param client   The NimbusBlobStore client. It must call prepare() before being used here.
     */
    public static void validateTopologyBlobStoreMap(Map<String, Object> topoConf, NimbusBlobStore client)
        throws InvalidTopologyException, AuthorizationException {
        Map<String, Map<String, Object>> blobStoreMap = (Map<String, Map<String, Object>>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);
        if (blobStoreMap != null) {
            for (String key : blobStoreMap.keySet()) {

                Map<String, Object> blobConf = blobStoreMap.get(key);
                try {
                    ObjectReader.getBoolean(blobConf.get("uncompress"), false);
                    ObjectReader.getBoolean(blobConf.get("workerRestart"), false);
                } catch (IllegalArgumentException e) {
                    throw new WrappedInvalidTopologyException("Invalid blob conf option: " + e.getMessage());
                }

                // try to get BlobMeta
                // This will check if the key exists and if the subject has authorization
                try {
                    client.getBlobMeta(key);
                } catch (KeyNotFoundException keyNotFound) {
                    // wrap KeyNotFoundException in an InvalidTopologyException
                    throw new WrappedInvalidTopologyException("Key not found: " + keyNotFound.get_msg());
                }
            }
        }
    }

    /**
     * Validate topology blobstore map.
     */
    public static void validateTopologyBlobStoreMap(Map<String, Object> topoConf, BlobStore blobStore)
        throws InvalidTopologyException, AuthorizationException {
        Map<String, Object> blobStoreMap = (Map<String, Object>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);
        if (blobStoreMap != null) {
            Subject subject = ReqContext.context().subject();
            for (String key : blobStoreMap.keySet()) {
                try {
                    blobStore.getBlobMeta(key, subject);
                } catch (KeyNotFoundException keyNotFound) {
                    // wrap KeyNotFoundException in an InvalidTopologyException
                    throw new WrappedInvalidTopologyException("Key not found: " + keyNotFound.get_msg());
                }
            }
        }
    }

    /**
     * Gets some information, including stack trace, for a running thread.
     *
     * @return A human-readable string of the dump.
     */
    public static String threadDump() {
        final StringBuilder dump = new StringBuilder();
        final java.lang.management.ThreadMXBean threadMxBean = ManagementFactory.getThreadMXBean();
        final java.lang.management.ThreadInfo[] threadInfos = threadMxBean.getThreadInfo(threadMxBean.getAllThreadIds(), 100);
        for (Entry<Thread, StackTraceElement[]> entry: Thread.getAllStackTraces().entrySet()) {
            Thread t = entry.getKey();
            ThreadInfo threadInfo = threadMxBean.getThreadInfo(t.getId());
            if (threadInfo == null) {
                //Thread died before we could get the info, skip
                continue;
            }
            dump.append('"');
            dump.append(threadInfo.getThreadName());
            dump.append("\" ");
            if (t.isDaemon()) {
                dump.append("(DAEMON)");
            }
            dump.append("\n   lock: ");
            dump.append(threadInfo.getLockName());
            dump.append(" owner: ");
            dump.append(threadInfo.getLockOwnerName());
            final Thread.State state = threadInfo.getThreadState();
            dump.append("\n   java.lang.Thread.State: ");
            dump.append(state);
            for (final StackTraceElement stackTraceElement : entry.getValue()) {
                dump.append("\n        at ");
                dump.append(stackTraceElement);
            }
            dump.append("\n\n");
        }
        return dump.toString();
    }

    public static boolean checkDirExists(String dir) {
        File file = new File(dir);
        return file.isDirectory();
    }

    /**
     * Return a new instance of a pluggable specified in the conf.
     *
     * @param conf      The conf to read from.
     * @param configKey The key pointing to the pluggable class
     * @return an instance of the class or null if it is not specified.
     */
    public static Object getConfiguredClass(Map<String, Object> conf, Object configKey) {
        if (conf.containsKey(configKey)) {
            return ReflectionUtils.newInstance((String) conf.get(configKey));
        }
        return null;
    }

    /**
     * Is the cluster configured to interact with ZooKeeper in a secure way? This only works when called from within Nimbus or a Supervisor
     * process.
     *
     * @param conf the storm configuration, not the topology configuration
     * @return true if it is configured else false.
     */
    public static boolean isZkAuthenticationConfiguredStormServer(Map<String, Object> conf) {
        return null != System.getProperty("java.security.auth.login.config")
               || (conf != null
                   && conf.get(Config.STORM_ZOOKEEPER_AUTH_SCHEME) != null
                   && !((String) conf.get(Config.STORM_ZOOKEEPER_AUTH_SCHEME)).isEmpty());
    }

    public static byte[] toCompressedJsonConf(Map<String, Object> topoConf) {
        try {
            ByteArrayOutputStream bos = new ByteArrayOutputStream();
            OutputStreamWriter out = new OutputStreamWriter(new GZIPOutputStream(bos));
            JSONValue.writeJSONString(topoConf, out);
            out.close();
            return bos.toByteArray();
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

    public static double nullToZero(Double v) {
        return (v != null ? v : 0);
    }

    /**
     * a or b the first one that is not null.
     *
     * @param a something
     * @param b something else
     * @return a or b the first one that is not null
     */
    @SuppressWarnings({"checkstyle:AbbreviationAsWordInName", "checkstyle:MethodName"})
    public static <V> V OR(V a, V b) {
        return a == null ? b : a;
    }

    public static TreeMap<Integer, Integer> integerDivided(int sum, int numPieces) {
        int base = sum / numPieces;
        int numInc = sum % numPieces;
        int numBases = numPieces - numInc;
        TreeMap<Integer, Integer> ret = new TreeMap<Integer, Integer>();
        ret.put(base, numBases);
        if (numInc != 0) {
            ret.put(base + 1, numInc);
        }
        return ret;
    }

    /**
     * Fills up chunks out of a collection (given a maximum amount of chunks).
     *
     * <p>i.e. partitionFixed(5, [1,2,3]) -> [[1,2,3]] partitionFixed(5, [1..9]) -> [[1,2], [3,4], [5,6], [7,8], [9]] partitionFixed(3,
     * [1..10]) -> [[1,2,3,4], [5,6,7], [8,9,10]]
     *
     * @param maxNumChunks the maximum number of chunks to return
     * @param coll         the collection to be chunked up
     * @return a list of the chunks, which are themselves lists.
     */
    public static <T> List<List<T>> partitionFixed(int maxNumChunks, Collection<T> coll) {
        List<List<T>> ret = new ArrayList<>();

        if (maxNumChunks == 0 || coll == null) {
            return ret;
        }

        Map<Integer, Integer> parts = integerDivided(coll.size(), maxNumChunks);

        // Keys sorted in descending order
        List<Integer> sortedKeys = new ArrayList<Integer>(parts.keySet());
        Collections.sort(sortedKeys, Collections.reverseOrder());


        Iterator<T> it = coll.iterator();
        for (Integer chunkSize : sortedKeys) {
            if (!it.hasNext()) {
                break;
            }
            Integer times = parts.get(chunkSize);
            for (int i = 0; i < times; i++) {
                if (!it.hasNext()) {
                    break;
                }
                List<T> chunkList = new ArrayList<>();
                for (int j = 0; j < chunkSize; j++) {
                    if (!it.hasNext()) {
                        break;
                    }
                    chunkList.add(it.next());
                }
                ret.add(chunkList);
            }
        }

        return ret;
    }

    public static Object readYamlFile(String yamlFile) {
        try (FileReader reader = new FileReader(yamlFile)) {
            return new Yaml(new SafeConstructor(new LoaderOptions())).load(reader);
        } catch (Exception ex) {
            LOG.error("Failed to read yaml file.", ex);
        }
        return null;
    }

    /**
     * Gets an available port. Consider if it is possible to pass port 0 to the server instead of using this method, since there is no
     * guarantee that the port returned by this method will remain free.
     *
     * @return The preferred port if available, or a random available port
     */
    public static int getAvailablePort(int preferredPort) {
        int localPort = -1;
        try (ServerSocket socket = new ServerSocket(preferredPort)) {
            localPort = socket.getLocalPort();
        } catch (IOException exp) {
            if (preferredPort > 0) {
                return getAvailablePort(0);
            }
        }
        return localPort;
    }

    /**
     * Shortcut to calling {@link #getAvailablePort(int) } with 0 as the preferred port.
     *
     * @return A random available port
     */
    public static int getAvailablePort() {
        return getAvailablePort(0);
    }

    /**
     * Find the first item of coll for which pred.test(...) returns true.
     *
     * @param pred The IPredicate to test for
     * @param coll The Collection of items to search through.
     * @return The first matching value in coll, or null if nothing matches.
     */
    public static <T> T findOne(IPredicate<T> pred, Collection<T> coll) {
        if (coll == null) {
            return null;
        }
        for (T elem : coll) {
            if (pred.test(elem)) {
                return elem;
            }
        }
        return null;
    }

    public static <T, U> T findOne(IPredicate<T> pred, Map<U, T> map) {
        if (map == null) {
            return null;
        }
        return findOne(pred, (Set<T>) map.entrySet());
    }

    public static Map<String, Object> parseJson(String json) {
        if (json == null) {
            return new HashMap<>();
        } else {
            try {
                return (Map<String, Object>) JSONValue.parseWithException(json);
            } catch (ParseException e) {
                throw new RuntimeException(e);
            }
        }
    }

    public static String memoizedLocalHostname() throws UnknownHostException {
        if (memoizedLocalHostnameString == null) {
            memoizedLocalHostnameString = localHostname();
        }
        return memoizedLocalHostnameString;
    }

    /**
     * Add version information to the given topology.
     *
     * @param topology the topology being submitted (MIGHT BE MODIFIED)
     * @return topology
     */
    public static StormTopology addVersions(StormTopology topology) {
        String stormVersion = VersionInfo.getVersion();
        if (stormVersion != null
            && !"Unknown".equalsIgnoreCase(stormVersion)
            && !topology.is_set_storm_version()) {
            topology.set_storm_version(stormVersion);
        }

        String jdkVersion = System.getProperty("java.version");
        if (jdkVersion != null && !topology.is_set_jdk_version()) {
            topology.set_jdk_version(jdkVersion);
        }
        return topology;
    }

    /**
     * Get a map of version to classpath from the conf Config.SUPERVISOR_WORKER_VERSION_CLASSPATH_MAP
     *
     * @param conf      what to read it out of
     * @param currentClassPath the current classpath for this version of storm (not included in the conf, but returned by this)
     * @return the map
     */
    public static NavigableMap<SimpleVersion, List<String>> getConfiguredClasspathVersions(Map<String, Object> conf,
                                                                                           List<String> currentClassPath) {
        TreeMap<SimpleVersion, List<String>> ret = new TreeMap<>();
        Map<String, String> fromConf =
            (Map<String, String>) conf.getOrDefault(Config.SUPERVISOR_WORKER_VERSION_CLASSPATH_MAP, Collections.emptyMap());
        for (Map.Entry<String, String> entry : fromConf.entrySet()) {
            ret.put(new SimpleVersion(entry.getKey()), Arrays.asList(entry.getValue().split(File.pathSeparator)));
        }
        ret.put(VersionInfo.OUR_VERSION, currentClassPath);
        return ret;
    }

    /**
     * Get a mapping of the configured supported versions of storm to their actual versions.
     * @param conf what to read the configuration out of.
     * @return the map.
     */
    public static NavigableMap<String, IVersionInfo> getAlternativeVersionsMap(Map<String, Object> conf) {
        TreeMap<String, IVersionInfo> ret = new TreeMap<>();
        Map<String, String> fromConf =
            (Map<String, String>) conf.getOrDefault(Config.SUPERVISOR_WORKER_VERSION_CLASSPATH_MAP, Collections.emptyMap());
        for (Map.Entry<String, String> entry : fromConf.entrySet()) {
            IVersionInfo version = VersionInfo.getFromClasspath(entry.getValue());
            if (version != null) {
                ret.put(entry.getKey(), version);
            } else {
                LOG.error("Could not find the real version of {} from CP {}", entry.getKey(), entry.getValue());
                ret.put(entry.getKey(), new IVersionInfo() {
                    @Override
                    public String getVersion() {
                        return "Unknown";
                    }

                    @Override
                    public String getRevision() {
                        return "Unknown";
                    }

                    @Override
                    public String getBranch() {
                        return "Unknown";
                    }

                    @Override
                    public String getDate() {
                        return "Unknown";
                    }

                    @Override
                    public String getUser() {
                        return "Unknown";
                    }

                    @Override
                    public String getUrl() {
                        return "Unknown";
                    }

                    @Override
                    public String getSrcChecksum() {
                        return "Unknown";
                    }

                    @Override
                    public String getBuildVersion() {
                        return "Unknown";
                    }
                });
            }
        }
        return ret;
    }

    /**
     * Get a map of version to worker main from the conf Config.SUPERVISOR_WORKER_VERSION_MAIN_MAP
     *
     * @param conf what to read it out of
     * @return the map
     */
    public static NavigableMap<SimpleVersion, String> getConfiguredWorkerMainVersions(Map<String, Object> conf) {
        TreeMap<SimpleVersion, String> ret = new TreeMap<>();
        Map<String, String> fromConf =
            (Map<String, String>) conf.getOrDefault(Config.SUPERVISOR_WORKER_VERSION_MAIN_MAP, Collections.emptyMap());
        for (Map.Entry<String, String> entry : fromConf.entrySet()) {
            ret.put(new SimpleVersion(entry.getKey()), entry.getValue());
        }

        ret.put(VersionInfo.OUR_VERSION, "org.apache.storm.daemon.worker.Worker");
        return ret;
    }

    /**
     * Get a map of version to worker log writer from the conf Config.SUPERVISOR_WORKER_VERSION_LOGWRITER_MAP
     *
     * @param conf what to read it out of
     * @return the map
     */
    public static NavigableMap<SimpleVersion, String> getConfiguredWorkerLogWriterVersions(Map<String, Object> conf) {
        TreeMap<SimpleVersion, String> ret = new TreeMap<>();
        Map<String, String> fromConf =
            (Map<String, String>) conf.getOrDefault(Config.SUPERVISOR_WORKER_VERSION_LOGWRITER_MAP, Collections.emptyMap());
        for (Map.Entry<String, String> entry : fromConf.entrySet()) {
            ret.put(new SimpleVersion(entry.getKey()), entry.getValue());
        }

        ret.put(VersionInfo.OUR_VERSION, "org.apache.storm.LogWriter");
        return ret;
    }

    public static <T> T getCompatibleVersion(NavigableMap<SimpleVersion, T> versionedMap, SimpleVersion desiredVersion, String what,
                                             T defaultValue) {
        Entry<SimpleVersion, T> ret = versionedMap.ceilingEntry(desiredVersion);
        if (ret == null || ret.getKey().getMajor() != desiredVersion.getMajor()) {
            //Could not find a "fully" compatible version.  Look to see if there is a possibly compatible version right below it
            ret = versionedMap.floorEntry(desiredVersion);
            if (ret == null || ret.getKey().getMajor() != desiredVersion.getMajor()) {
                if (defaultValue != null) {
                    LOG.warn("Could not find any compatible {} falling back to using {}", what, defaultValue);
                }
                return defaultValue;
            }
            LOG.warn("Could not find a higer compatible version for {} {}, using {} instead", what, desiredVersion, ret.getKey());
        }
        return ret.getValue();
    }

    @SuppressWarnings("unchecked")
    private static Map<String, Object> readConfIgnoreNotFound(Yaml yaml, File f) throws IOException {
        Map<String, Object> ret = null;
        if (f.exists()) {
            try (FileReader fr = new FileReader(f)) {
                ret = (Map<String, Object>) yaml.load(fr);
            }
        }
        return ret;
    }

    public static Map<String, Object> getConfigFromClasspath(List<String> cp, Map<String, Object> conf) throws IOException {
        if (cp == null || cp.isEmpty()) {
            return conf;
        }
        Yaml yaml = new Yaml(new SafeConstructor(new LoaderOptions()));
        Map<String, Object> defaultsConf = null;
        Map<String, Object> stormConf = null;

        // Based on how Java handles the classpath
        // https://docs.oracle.com/javase/8/docs/technotes/tools/unix/classpath.html
        for (String part : cp) {
            File f = new File(part);

            if (f.getName().equals("*")) {
                // wildcard is given in file
                // in java classpath, '*' is expanded to all jar/JAR files in the directory
                File dir = f.getParentFile();
                if (dir == null) {
                    // it happens when part is just '*' rather than denoting some directory
                    dir = new File(".");
                }

                File[] jarFiles = dir.listFiles((dir1, name) -> name.endsWith(".jar") || name.endsWith(".JAR"));

                // Quoting Javadoc in File.listFiles(FilenameFilter filter):
                // Returns {@code null} if this abstract pathname does not denote a directory, or if an I/O error occurs.
                // Both things are not expected and should not happen.
                if (jarFiles == null) {
                    throw new IOException("Fail to list jar files in directory: " + dir);
                }

                for (File jarFile : jarFiles) {
                    JarConfigReader jarConfigReader = new JarConfigReader(yaml, defaultsConf, stormConf, jarFile).readJar();
                    defaultsConf = jarConfigReader.getDefaultsConf();
                    stormConf = jarConfigReader.getStormConf();
                }
            } else if (f.isDirectory()) {
                // no wildcard, directory
                if (defaultsConf == null) {
                    defaultsConf = readConfIgnoreNotFound(yaml, new File(f, "defaults.yaml"));
                }

                if (stormConf == null) {
                    stormConf = readConfIgnoreNotFound(yaml, new File(f, "storm.yaml"));
                }
            } else if (f.isFile()) {
                // no wildcard, file
                String fileName = f.getName();
                if (fileName.endsWith(".zip") || fileName.endsWith(".ZIP")) {
                    JarConfigReader jarConfigReader = new JarConfigReader(yaml, defaultsConf, stormConf, f).readZip();
                    defaultsConf = jarConfigReader.getDefaultsConf();
                    stormConf = jarConfigReader.getStormConf();
                } else if (fileName.endsWith(".jar") || fileName.endsWith(".JAR")) {
                    JarConfigReader jarConfigReader = new JarConfigReader(yaml, defaultsConf, stormConf, f).readJar();
                    defaultsConf = jarConfigReader.getDefaultsConf();
                    stormConf = jarConfigReader.getStormConf();
                }
                // Class path entries that are neither directories nor archives (.zip or JAR files)
                // nor the asterisk (*) wildcard character are ignored.
            }
        }
        if (stormConf != null) {
            defaultsConf.putAll(stormConf);
        }
        return defaultsConf;
    }

    public static boolean isLocalhostAddress(String address) {
        return LOCALHOST_ADDRESSES.contains(address);
    }

    public static <K, V> Map<K, V> merge(Map<? extends K, ? extends V> first, Map<? extends K, ? extends V> other) {
        Map<K, V> ret = new HashMap<>(first);
        if (other != null) {
            ret.putAll(other);
        }
        return ret;
    }

    public static <V> ArrayList<V> convertToArray(Map<Integer, V> srcMap, int start) {
        Set<Integer> ids = srcMap.keySet();
        Integer largestId = ids.stream().max(Integer::compareTo).get();
        int end = largestId - start;
        ArrayList<V> result = new ArrayList<>(Collections.nCopies(end + 1, null)); // creates array[largestId+1] filled with nulls
        for (Map.Entry<Integer, V> entry : srcMap.entrySet()) {
            int id = entry.getKey();
            if (id < start) {
                LOG.debug("Entry {} will be skipped it is too small {} ...", id, start);
            } else {
                result.set(id - start, entry.getValue());
            }
        }
        return result;
    }

    // Non-static impl methods exist for mocking purposes.
    protected void forceDeleteImpl(String path) throws IOException {
        LOG.debug("Deleting path {}", path);
        if (checkFileExists(path)) {
            try {
                FileUtils.forceDelete(new File(path));
            } catch (FileNotFoundException ignored) {
                //ignore
            }
        }
    }

    // Non-static impl methods exist for mocking purposes.
    public UptimeComputer makeUptimeComputerImpl() {
        return new UptimeComputer();
    }

    // Non-static impl methods exist for mocking purposes.
    protected String localHostnameImpl() throws UnknownHostException {
        return InetAddress.getLocalHost().getCanonicalHostName();
    }

    // Non-static impl methods exist for mocking purposes.
    protected String hostnameImpl() throws UnknownHostException {
        if (localConf == null) {
            return memoizedLocalHostname();
        }
        Object hostnameString = localConf.get(Config.STORM_LOCAL_HOSTNAME);
        if (hostnameString == null || hostnameString.equals("")) {
            return memoizedLocalHostname();
        }
        return (String) hostnameString;
    }

    /**
     * Validates blob key.
     *
     * @param key Key for the blob.
     */
    public static boolean isValidKey(String key) {
        if (StringUtils.isEmpty(key) || "..".equals(key) || ".".equals(key) || !BLOB_KEY_PATTERN.matcher(key).matches()) {
            LOG.error("'{}' does not appear to be valid. It must match {}. And it can't be \".\", \"..\", null or empty string.", key,
                BLOB_KEY_PATTERN);
            return false;
        }
        return true;
    }

    /**
     * Validates topology name.
     * @param name the topology name
     * @throws IllegalArgumentException if the topology name is not valid
     */
    public static void validateTopologyName(String name) throws IllegalArgumentException {
        if (name == null || !TOPOLOGY_NAME_REGEX.matcher(name).matches()) {
            String message = "Topology name '" + name + "' is not valid. It can't be null and it must match " + TOPOLOGY_NAME_REGEX;
            throw new IllegalArgumentException(message);
        }
    }

    /**
     * A thread that can answer if it is sleeping in the case of simulated time. This class is not useful when simulated time is not being
     * used.
     */
    public static class SmartThread extends Thread {
        public SmartThread(Runnable r) {
            super(r);
        }

        public boolean isSleeping() {
            return Time.isThreadWaiting(this);
        }
    }

    public static class UptimeComputer {
        int startTime = 0;

        public UptimeComputer() {
            startTime = Time.currentTimeSecs();
        }

        public int upTime() {
            return Time.deltaSecs(startTime);
        }
    }

    private static class JarConfigReader {
        private Yaml yaml;
        private Map<String, Object> defaultsConf;
        private Map<String, Object> stormConf;
        private File file;

        JarConfigReader(Yaml yaml, Map<String, Object> defaultsConf, Map<String, Object> stormConf, File file) {
            this.yaml = yaml;
            this.defaultsConf = defaultsConf;
            this.stormConf = stormConf;
            this.file = file;
        }

        public Map<String, Object> getDefaultsConf() {
            return defaultsConf;
        }

        public Map<String, Object> getStormConf() {
            return stormConf;
        }

        public JarConfigReader readZip() throws IOException {
            try (ZipFile zipFile = new ZipFile(file)) {
                readArchive(zipFile);
            }
            return this;
        }

        public JarConfigReader readJar() throws IOException {
            try (JarFile jarFile = new JarFile(file)) {
                readArchive(jarFile);
            }
            return this;
        }

        private void readArchive(ZipFile zipFile) throws IOException {
            Enumeration<? extends ZipEntry> zipEnums = zipFile.entries();
            while (zipEnums.hasMoreElements()) {
                ZipEntry entry = zipEnums.nextElement();
                if (!entry.isDirectory()) {
                    if (defaultsConf == null && entry.getName().equals("defaults.yaml")) {
                        try (InputStreamReader isr = new InputStreamReader(zipFile.getInputStream(entry))) {
                            defaultsConf = (Map<String, Object>) yaml.load(isr);
                        }
                    }

                    if (stormConf == null && entry.getName().equals("storm.yaml")) {
                        try (InputStreamReader isr = new InputStreamReader(zipFile.getInputStream(entry))) {
                            stormConf = (Map<String, Object>) yaml.load(isr);
                        }
                    }
                }
            }
        }
    }

    /**
     * Create a map of forward edges for bolts in a topology. Note that spouts can be source but not a target in
     * the edge. The mapping contains ids of spouts and bolts.
     *
     * @param topology StormTopology to examine.
     * @return a map with entry for each SpoutId/BoltId to a set of outbound edges of BoltIds.
     */
    private static Map<String, Set<String>> getStormTopologyForwardGraph(StormTopology topology) {
        Map<String, Set<String>> edgesOut = new HashMap<>();

        if (topology.get_bolts() != null) {
            topology.get_bolts().entrySet().forEach(entry -> {
                if (!Utils.isSystemId(entry.getKey())) {
                    entry.getValue().get_common().get_inputs().forEach((k, v) -> {
                        edgesOut.computeIfAbsent(k.get_componentId(), x -> new HashSet<>()).add(entry.getKey());
                    });
                }
            });
        }
        return edgesOut;
    }

    /**
     * Use recursive descent to detect cycles. This is a Depth First recursion. Component Cycle is recorded when encountered.
     * In addition, the last link in the cycle is removed to avoid re-detecting same cycle/subcycle.
     *
     * @param stack used for recursion.
     * @param edgesOut outbound edge connections, modified when cycle is detected.
     * @param seen keeps track of component ids that have already been seen.
     * @param cycles list of cycles seen so far.
     */
    private static void findComponentCyclesRecursion(
            Stack<String> stack, Map<String, Set<String>> edgesOut, Set<String> seen, List<List<String>> cycles) {
        if (stack.isEmpty()) {
            return;
        }
        String compId1 = stack.peek();
        if (!edgesOut.containsKey(compId1) || edgesOut.get(compId1).isEmpty()) {
            stack.pop();
            return;
        }
        Set<String> children = new HashSet<>(edgesOut.get(compId1));
        for (String compId2: children) {
            if (seen.contains(compId2)) {
                // cycle/diamond detected
                List<String> possibleCycle = new ArrayList<>();
                if (compId1.equals(compId2)) {
                    possibleCycle.add(compId2);
                } else if (edgesOut.get(compId2) != null && edgesOut.get(compId2).contains(compId1)) {
                    possibleCycle.addAll(Arrays.asList(compId1, compId2));
                } else {
                    List<String> tmp = Collections.list(stack.elements());
                    int prevIdx = tmp.indexOf(compId2);
                    if (prevIdx >= 0) {
                        // cycle (as opposed to diamond)
                        tmp = tmp.subList(prevIdx, tmp.size());
                        tmp.add(compId2);
                        possibleCycle.addAll(tmp);
                    }
                }
                if (!possibleCycle.isEmpty()) {
                    cycles.add(possibleCycle);
                    edgesOut.get(compId1).remove(compId2); // disconnect this cycle
                    continue;
                }
            }
            seen.add(compId2);
            stack.push(compId2);
            findComponentCyclesRecursion(stack, edgesOut, seen, cycles);
        }
        stack.pop();
    }

    /**
     * Find and return components cycles in the topology graph when starting from spout.
     * Return a list of cycles. Each cycle may consist of one or more components.
     * Components that cannot be reached from any of the spouts are ignored.
     *
     * @return a List of cycles. Each cycle has a list of component names.
     *
     */
    @VisibleForTesting
    public static List<List<String>> findComponentCycles(StormTopology topology, String topoId) {
        List<List<String>> ret = new ArrayList<>();
        Map<String, Set<String>> edgesOut = getStormTopologyForwardGraph(topology);
        Set<String> allComponentIds = new HashSet<>();
        edgesOut.forEach((k, v) -> {
            allComponentIds.add(k) ;
            allComponentIds.addAll(v);
        });

        if (topology.get_spouts_size() == 0) {
            LOG.error("Topology {} does not contain any spouts, cannot traverse graph to determine cycles", topoId);
            return ret;
        }

        Set<String> unreachable = new HashSet<>(edgesOut.keySet());
        topology.get_spouts().forEach((spoutId, spout)  -> {
            Stack<String> dfsStack = new Stack<>();
            dfsStack.push(spoutId);
            Set<String> seen = new HashSet<>();
            seen.add(spoutId);
            findComponentCyclesRecursion(dfsStack, edgesOut, seen, ret);
            unreachable.removeAll(seen);
        });

        // warning about unreachable components
        if (!unreachable.isEmpty()) {
            LOG.warn("Topology {} contains unreachable components \"{}\"", topoId, String.join(",", unreachable));
        }
        return ret;
    }

    /**
     * Validate that the topology is cycle free. If not, then throw an InvalidTopologyException describing the cycle(s).
     *
     * @param topology StormTopology instance to examine.
     * @param name Name of the topology, used in exception error message.
     * @throws InvalidTopologyException if there are cycles, with message describing the cycles encountered.
     */
    public static void validateCycleFree(StormTopology topology, String name) throws InvalidTopologyException {
        List<List<String>> cycles = Utils.findComponentCycles(topology, name);
        if (!cycles.isEmpty()) {
            String err = String.format("Topology %s contains cycles in components \"%s\"", name,
                    cycles.stream().map(x -> String.join(",", x)).collect(Collectors.joining(" ; ")));
            throw new WrappedInvalidTopologyException(err);
        }
    }
}

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file introduce a new method `getConfiguredClientAs` and modify existing constructors to include an additional parameter `asUser`. These changes directly impact the authentication mechanism by allowing the client to be configured with a specific user. This could have significant security implications, such as ensuring that the user-specific configurations are correctly handled and do not introduce vulnerabilities. Given the critical nature of authentication mechanisms, it is essential to re-run the security tests to verify that these changes do not introduce any security issues.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 18
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
new file mode 100644
index 000000000..4d9f5da7b
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
@@ -0,0 +1,289 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.task.OutputCollector;
+import backtype.storm.task.TopologyContext;
+import backtype.storm.tuple.Tuple;
+import backtype.storm.topology.base.BaseRichBolt;
+import backtype.storm.topology.OutputFieldsDeclarer;
+import org.apache.storm.hive.common.HiveWriter;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import org.apache.hive.hcatalog.streaming.*;
+import org.apache.storm.hive.common.HiveOptions;
+import org.apache.storm.hive.common.HiveUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.Map.Entry;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.io.IOException;
+
+public class HiveBolt extends  BaseRichBolt {
+    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
+    private OutputCollector collector;
+    private HiveOptions options;
+    private Integer currentBatchSize;
+    private ExecutorService callTimeoutPool;
+    private transient Timer heartBeatTimer;
+    private Boolean kerberosEnabled = false;
+    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
+    private UserGroupInformation ugi = null;
+    HashMap<HiveEndPoint, HiveWriter> allWriters;
+
+    public HiveBolt(HiveOptions options) {
+        this.options = options;
+        this.currentBatchSize = 0;
+    }
+
+    @Override
+    public void prepare(Map conf, TopologyContext topologyContext, OutputCollector collector)  {
+        try {
+            if(options.getKerberosPrincipal() == null && options.getKerberosKeytab() == null) {
+                kerberosEnabled = false;
+            } else if(options.getKerberosPrincipal() != null && options.getKerberosKeytab() != null) {
+                kerberosEnabled = true;
+            } else {
+                throw new IllegalArgumentException("To enable Kerberos, need to set both KerberosPrincipal " +
+                                                   " & KerberosKeytab");
+            }
+
+            if (kerberosEnabled) {
+                try {
+                    ugi = HiveUtils.authenticate(options.getKerberosKeytab(), options.getKerberosPrincipal());
+                } catch(HiveUtils.AuthenticationFailed ex) {
+                    LOG.error("Hive Kerberos authentication failed " + ex.getMessage(), ex);
+                    throw new IllegalArgumentException(ex);
+                }
+            }
+            this.collector = collector;
+            allWriters = new HashMap<HiveEndPoint,HiveWriter>();
+            String timeoutName = "hive-bolt-%d";
+            this.callTimeoutPool = Executors.newFixedThreadPool(1,
+                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
+            heartBeatTimer = new Timer();
+            setupHeartBeatTimer();
+        } catch(Exception e) {
+            LOG.warn("unable to make connection to hive ",e);
+        }
+    }
+
+    @Override
+    public void execute(Tuple tuple) {
+        try {
+            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
+            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
+            HiveWriter writer = getOrCreateWriter(endPoint);
+            if(timeToSendHeartBeat.compareAndSet(true, false)) {
+                enableHeartBeatOnAllWriters();
+            }
+            writer.write(options.getMapper().mapRecord(tuple));
+            currentBatchSize++;
+            if(currentBatchSize >= options.getBatchSize()) {
+                flushAllWriters();
+                currentBatchSize = 0;
+            }
+            collector.ack(tuple);
+        } catch(Exception e) {
+            this.collector.reportError(e);
+            collector.fail(tuple);
+            flushAndCloseWriters();
+        }
+    }
+
+    @Override
+    public void declareOutputFields(OutputFieldsDeclarer declarer) {
+
+    }
+
+    @Override
+    public void cleanup() {
+        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
+            try {
+                HiveWriter w = entry.getValue();
+                LOG.info("Flushing writer to {}", w);
+                w.flush(false);
+                LOG.info("Closing writer to {}", w);
+                w.close();
+            } catch (Exception ex) {
+                LOG.warn("Error while closing writer to " + entry.getKey() +
+                         ". Exception follows.", ex);
+                if (ex instanceof InterruptedException) {
+                    Thread.currentThread().interrupt();
+                }
+            }
+        }
+
+        ExecutorService toShutdown[] = {callTimeoutPool};
+        for (ExecutorService execService : toShutdown) {
+            execService.shutdown();
+            try {
+                while (!execService.isTerminated()) {
+                    execService.awaitTermination(
+                                 options.getCallTimeOut(), TimeUnit.MILLISECONDS);
+                }
+            } catch (InterruptedException ex) {
+                LOG.warn("shutdown interrupted on " + execService, ex);
+            }
+        }
+        callTimeoutPool = null;
+        super.cleanup();
+        LOG.info("Hive Bolt stopped");
+    }
+
+
+    private void setupHeartBeatTimer() {
+        if(options.getHeartBeatInterval()>0) {
+            heartBeatTimer.schedule(new TimerTask() {
+                    @Override
+                    public void run() {
+                        timeToSendHeartBeat.set(true);
+                        setupHeartBeatTimer();
+                    }
+                }, options.getHeartBeatInterval() * 1000);
+        }
+    }
+
+    private void flushAllWriters()
+        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
+        for(HiveWriter writer: allWriters.values()) {
+            writer.flush(true);
+        }
+    }
+
+    /**
+     * Closes all writers and remove them from cache
+     * @return number of writers retired
+     */
+    private void closeAllWriters() {
+        try {
+            //1) Retire writers
+            for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+                entry.getValue().close();
+            }
+            //2) Clear cache
+            allWriters.clear();
+        } catch(Exception e) {
+            LOG.warn("unable to close writers. ", e);
+        }
+    }
+
+    private void flushAndCloseWriters() {
+        try {
+            flushAllWriters();
+        } catch(Exception e) {
+            LOG.warn("unable to flush hive writers. ", e);
+        } finally {
+            closeAllWriters();
+        }
+    }
+
+    private void enableHeartBeatOnAllWriters() {
+        for (HiveWriter writer : allWriters.values()) {
+            writer.setHeartBeatNeeded();
+        }
+    }
+
+    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
+        throws HiveWriter.ConnectFailure, InterruptedException {
+        try {
+            HiveWriter writer = allWriters.get( endPoint );
+            if( writer == null ) {
+                LOG.debug("Creating Writer to Hive end point : " + endPoint);
+                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options);
+                if(allWriters.size() > options.getMaxOpenConnections()){
+                    int retired = retireIdleWriters();
+                    if(retired==0) {
+                        retireEldestWriter();
+                    }
+                }
+                allWriters.put(endPoint, writer);
+            }
+            return writer;
+        } catch (HiveWriter.ConnectFailure e) {
+            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
+            throw e;
+        }
+    }
+
+    /**
+     * Locate writer that has not been used for longest time and retire it
+     */
+    private void retireEldestWriter() {
+        long oldestTimeStamp = System.currentTimeMillis();
+        HiveEndPoint eldest = null;
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(entry.getValue().getLastUsed() < oldestTimeStamp) {
+                eldest = entry.getKey();
+                oldestTimeStamp = entry.getValue().getLastUsed();
+            }
+        }
+        try {
+            LOG.info("Closing least used Writer to Hive end point : " + eldest);
+            allWriters.remove(eldest).close();
+        } catch (IOException e) {
+            LOG.warn("Failed to close writer for end point: " + eldest, e);
+        } catch (InterruptedException e) {
+            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
+            Thread.currentThread().interrupt();
+        }
+    }
+
+    /**
+     * Locate all writers past idle timeout and retire them
+     * @return number of writers retired
+     */
+    private int retireIdleWriters() {
+        int count = 0;
+        long now = System.currentTimeMillis();
+        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();
+
+        //1) Find retirement candidates
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
+                ++count;
+                retirees.add(entry.getKey());
+            }
+        }
+        //2) Retire them
+        for(HiveEndPoint ep : retirees) {
+            try {
+                LOG.info("Closing idle Writer to Hive end point : {}", ep);
+                allWriters.remove(ep).close();
+            } catch (IOException e) {
+                LOG.warn("Failed to close writer for end point: {}. Error: "+ ep, e);
+            } catch (InterruptedException e) {
+                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
+                Thread.currentThread().interrupt();
+            }
+        }
+        return count;
+    }
+
+}

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The diff file introduces a new class `HiveBolt` which includes several critical security components such as Kerberos authentication (`UserGroupInformation ugi`), error logging (`LOG.error`), and access control mechanisms. The presence of Kerberos authentication indicates that the changes directly affect the authentication mechanism, which is a critical security component. Additionally, the logging of errors and exceptions can have implications for security event logging. Given these factors, it is necessary to re-run the security tests to ensure that these changes do not introduce any vulnerabilities or issues.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 19
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/DelimitedRecordHiveMapper.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/DelimitedRecordHiveMapper.java
new file mode 100644
index 000000000..d5167952a
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/DelimitedRecordHiveMapper.java
@@ -0,0 +1,143 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.bolt.mapper;
+
+
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Tuple;
+import storm.trident.tuple.TridentTuple;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.apache.hive.hcatalog.streaming.DelimitedInputWriter;
+import org.apache.hive.hcatalog.streaming.HiveEndPoint;
+import org.apache.hive.hcatalog.streaming.RecordWriter;
+import org.apache.hive.hcatalog.streaming.StreamingException;
+import org.apache.hive.hcatalog.streaming.TransactionBatch;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Date;
+import java.text.SimpleDateFormat;
+import java.io.IOException;
+
+public class DelimitedRecordHiveMapper implements HiveMapper {
+    private static final Logger LOG = LoggerFactory.getLogger(DelimitedRecordHiveMapper.class);
+    private static final String DEFAULT_FIELD_DELIMITER = ",";
+    private Fields columnFields;
+    private Fields partitionFields;
+    private String[] columnNames;
+    private String timeFormat;
+    private String fieldDelimiter = DEFAULT_FIELD_DELIMITER;
+    private SimpleDateFormat parseDate;
+
+    public DelimitedRecordHiveMapper() {
+    }
+
+    public DelimitedRecordHiveMapper withColumnFields(Fields columnFields) {
+        this.columnFields = columnFields;
+        List<String> tempColumnNamesList = this.columnFields.toList();
+        columnNames = new String[tempColumnNamesList.size()];
+        tempColumnNamesList.toArray(columnNames);
+        return this;
+    }
+
+    public DelimitedRecordHiveMapper withPartitionFields(Fields partitionFields) {
+        this.partitionFields = partitionFields;
+        return this;
+    }
+
+    public DelimitedRecordHiveMapper withFieldDelimiter(String delimiter){
+        this.fieldDelimiter = delimiter;
+        return this;
+    }
+
+    public DelimitedRecordHiveMapper withTimeAsPartitionField(String timeFormat) {
+        this.timeFormat = timeFormat;
+        parseDate = new SimpleDateFormat(timeFormat);
+        return this;
+    }
+
+    @Override
+    public RecordWriter createRecordWriter(HiveEndPoint endPoint)
+        throws StreamingException, IOException, ClassNotFoundException {
+        return new DelimitedInputWriter(columnNames, fieldDelimiter,endPoint);
+    }
+
+    @Override
+    public void write(TransactionBatch txnBatch, Tuple tuple)
+        throws StreamingException, IOException, InterruptedException {
+        txnBatch.write(mapRecord(tuple));
+    }
+
+    @Override
+    public List<String> mapPartitions(Tuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(Tuple tuple) {
+        StringBuilder builder = new StringBuilder();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                builder.append(tuple.getValueByField(field));
+                builder.append(fieldDelimiter);
+            }
+        }
+        return builder.toString().getBytes();
+    }
+
+    @Override
+    public List<String> mapPartitions(TridentTuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(TridentTuple tuple) {
+        StringBuilder builder = new StringBuilder();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                builder.append(tuple.getValueByField(field));
+                builder.append(fieldDelimiter);
+            }
+        }
+        return builder.toString().getBytes();
+    }
+
+    private String getPartitionsByTimeFormat() {
+        Date d = new Date();
+        return parseDate.format(d.getTime());
+    }
+}

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The changes in the provided diff file involve the addition of a new class `DelimitedRecordHiveMapper` which primarily deals with mapping records and partitions for Hive. The modifications do not directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, re-running the security tests is not necessary.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 21
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java
new file mode 100644
index 000000000..ce3e4756b
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java
@@ -0,0 +1,132 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.bolt.mapper;
+
+
+import backtype.storm.tuple.Fields;
+import storm.trident.tuple.TridentTuple;
+import backtype.storm.tuple.Tuple;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hive.hcatalog.streaming.HiveEndPoint;
+import org.apache.hive.hcatalog.streaming.RecordWriter;
+import org.apache.hive.hcatalog.streaming.StreamingException;
+import org.apache.hive.hcatalog.streaming.StrictJsonWriter;
+import org.apache.hive.hcatalog.streaming.TransactionBatch;
+import org.json.simple.JSONObject;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Date;
+import java.text.SimpleDateFormat;
+import java.io.IOException;
+
+public class JsonRecordHiveMapper implements HiveMapper {
+    private static final Logger LOG = LoggerFactory.getLogger(DelimitedRecordHiveMapper.class);
+    private Fields columnFields;
+    private Fields partitionFields;
+    private String timeFormat;
+    private SimpleDateFormat parseDate;
+
+    public JsonRecordHiveMapper() {
+    }
+
+    public JsonRecordHiveMapper withColumnFields(Fields columnFields) {
+        this.columnFields = columnFields;
+        return this;
+    }
+
+    public JsonRecordHiveMapper withPartitionFields(Fields partitionFields) {
+        this.partitionFields = partitionFields;
+        return this;
+    }
+
+    public JsonRecordHiveMapper withTimeAsPartitionField(String timeFormat) {
+        this.timeFormat = timeFormat;
+        parseDate = new SimpleDateFormat(timeFormat);
+        return this;
+    }
+
+    @Override
+    public RecordWriter createRecordWriter(HiveEndPoint endPoint)
+        throws StreamingException, IOException, ClassNotFoundException {
+        return new StrictJsonWriter(endPoint);
+    }
+
+    @Override
+    public void write(TransactionBatch txnBatch, Tuple tuple)
+        throws StreamingException, IOException, InterruptedException {
+        txnBatch.write(mapRecord(tuple));
+    }
+
+    @Override
+    public List<String> mapPartitions(Tuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(Tuple tuple) {
+        JSONObject obj = new JSONObject();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                obj.put(field,tuple.getValueByField(field));
+            }
+        }
+        return obj.toJSONString().getBytes();
+    }
+
+    @Override
+    public List<String> mapPartitions(TridentTuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(TridentTuple tuple) {
+        JSONObject obj = new JSONObject();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                obj.put(field,tuple.getValueByField(field));
+            }
+        }
+        return obj.toJSONString().getBytes();
+    }
+
+    private String getPartitionsByTimeFormat() {
+        Date d = new Date();
+        return parseDate.format(d.getTime());
+    }
+}

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The changes in the diff file involve the addition of a new file, JsonRecordHiveMapper.java, which primarily deals with mapping records and partitions for Hive. The modifications do not directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, re-running the security tests is not necessary.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 22
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java
new file mode 100644
index 000000000..d3162949a
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java
@@ -0,0 +1,146 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.common;
+
+import java.io.Serializable;
+
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.storm.hive.bolt.mapper.HiveMapper;
+import org.apache.hive.hcatalog.streaming.*;
+
+
+public class HiveOptions implements Serializable {
+    protected HiveMapper mapper;
+    protected String databaseName;
+    protected String tableName;
+    protected String metaStoreURI;
+    protected Integer txnsPerBatch = 100;
+    protected Integer maxOpenConnections = 500;
+    protected Integer batchSize = 15000;
+    protected Integer idleTimeout = 0;
+    protected Integer callTimeout = 10000;
+    protected Integer heartBeatInterval = 240;
+    protected Boolean autoCreatePartitions = true;
+    protected String kerberosPrincipal;
+    protected String kerberosKeytab;
+
+    public HiveOptions(String metaStoreURI,String databaseName,String tableName,HiveMapper mapper) {
+        this.metaStoreURI = metaStoreURI;
+        this.databaseName = databaseName;
+        this.tableName = tableName;
+        this.mapper = mapper;
+    }
+
+    public HiveOptions withTxnsPerBatch(Integer txnsPerBatch) {
+        this.txnsPerBatch = txnsPerBatch;
+        return this;
+    }
+
+    public HiveOptions withMaxOpenConnections(Integer maxOpenConnections) {
+        this.maxOpenConnections = maxOpenConnections;
+        return this;
+    }
+
+    public HiveOptions withBatchSize(Integer batchSize) {
+        this.batchSize = batchSize;
+        return this;
+    }
+
+    public HiveOptions withIdleTimeout(Integer idleTimeout) {
+        this.idleTimeout = idleTimeout;
+        return this;
+    }
+
+    public HiveOptions withCallTimeout(Integer callTimeout) {
+        this.callTimeout = callTimeout;
+        return this;
+    }
+
+    public HiveOptions withHeartBeatInterval(Integer heartBeatInterval) {
+        this.heartBeatInterval = heartBeatInterval;
+        return this;
+    }
+
+    public HiveOptions withAutoCreatePartitions(Boolean autoCreatePartitions) {
+        this.autoCreatePartitions = autoCreatePartitions;
+        return this;
+    }
+
+    public HiveOptions withKerberosKeytab(String kerberosKeytab) {
+        this.kerberosKeytab = kerberosKeytab;
+        return this;
+    }
+
+    public HiveOptions withKerberosPrincipal(String kerberosPrincipal) {
+        this.kerberosPrincipal = kerberosPrincipal;
+        return this;
+    }
+
+    public String getMetaStoreURI() {
+        return metaStoreURI;
+    }
+
+    public String getDatabaseName() {
+        return databaseName;
+    }
+
+    public String getTableName() {
+        return tableName;
+    }
+
+    public HiveMapper getMapper() {
+        return mapper;
+    }
+
+    public Integer getBatchSize() {
+        return batchSize;
+    }
+
+    public Integer getCallTimeOut() {
+        return callTimeout;
+    }
+
+    public Integer getHeartBeatInterval() {
+        return heartBeatInterval;
+    }
+
+    public Integer getMaxOpenConnections() {
+        return maxOpenConnections;
+    }
+
+    public Integer getIdleTimeout() {
+        return idleTimeout;
+    }
+
+    public Integer getTxnsPerBatch() {
+        return txnsPerBatch;
+    }
+
+    public Boolean getAutoCreatePartitions() {
+        return autoCreatePartitions;
+    }
+
+    public String getKerberosPrincipal() {
+        return kerberosPrincipal;
+    }
+
+    public String getKerberosKeytab() {
+        return kerberosKeytab;
+    }
+}

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The newly added file `HiveOptions.java` includes methods and properties related to Kerberos authentication (`kerberosPrincipal` and `kerberosKeytab`). Kerberos is a critical security component used for authentication. Any changes or additions involving authentication mechanisms warrant a re-run of security tests to ensure there are no vulnerabilities or misconfigurations introduced.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 23
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveUtils.java b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveUtils.java
new file mode 100644
index 000000000..5483b077c
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveUtils.java
@@ -0,0 +1,76 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.common;
+
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.storm.hive.bolt.mapper.HiveMapper;
+import org.apache.hive.hcatalog.streaming.*;
+
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.io.File;
+import java.io.IOException;
+
+public class HiveUtils {
+
+    public static HiveEndPoint makeEndPoint(List<String> partitionVals, HiveOptions options) throws ConnectionError {
+        if(partitionVals==null) {
+            return new HiveEndPoint(options.getMetaStoreURI(), options.getDatabaseName(), options.getTableName(), null);
+        }
+        return new HiveEndPoint(options.getMetaStoreURI(), options.getDatabaseName(), options.getTableName(), partitionVals);
+    }
+
+    public static HiveWriter makeHiveWriter(HiveEndPoint endPoint, ExecutorService callTimeoutPool, UserGroupInformation ugi, HiveOptions options)
+        throws HiveWriter.ConnectFailure, InterruptedException {
+        return new HiveWriter(endPoint, options.getTxnsPerBatch(), options.getAutoCreatePartitions(),
+                              options.getCallTimeOut(), callTimeoutPool, options.getMapper(), ugi);
+    }
+
+    public static synchronized UserGroupInformation authenticate(String keytab, String principal)
+    throws AuthenticationFailed {
+        File kfile = new File(keytab);
+        if (!(kfile.isFile() && kfile.canRead())) {
+            throw new IllegalArgumentException("The keyTab file: "
+                                               + keytab + " is nonexistent or can't read. "
+                                               + "Please specify a readable keytab file for Kerberos auth.");
+        }
+        try {
+            principal = SecurityUtil.getServerPrincipal(principal, "");
+        } catch (Exception e) {
+            throw new AuthenticationFailed("Host lookup error when resolving principal " + principal, e);
+        }
+        try {
+            UserGroupInformation.loginUserFromKeytab(principal, keytab);
+            return UserGroupInformation.getLoginUser();
+        } catch (IOException e) {
+            throw new AuthenticationFailed("Login failed for principal " + principal, e);
+        }
+    }
+
+     public static class AuthenticationFailed extends Exception {
+         public AuthenticationFailed(String reason, Exception cause) {
+             super("Kerberos Authentication Failed. " + reason, cause);
+         }
+     }
+
+
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.bolt;

import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.SerializationError;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.Config;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.BatchHelper;
import org.apache.storm.utils.TupleUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveBolt extends BaseRichBolt {
    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
    @VisibleForTesting
    Map<HiveEndPoint, HiveWriter> allWriters;
    private OutputCollector collector;
    private HiveOptions options;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean sendHeartBeat = new AtomicBoolean(false);
    private UserGroupInformation ugi = null;
    private BatchHelper batchHelper;
    private boolean tokenAuthEnabled;

    public HiveBolt(HiveOptions options) {
        this.options = options;
    }

    @Override
    public void prepare(Map<String, Object> conf, TopologyContext topologyContext, OutputCollector collector) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            this.collector = collector;
            this.batchHelper = new BatchHelper(options.getBatchSize(), collector);
            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());

            sendHeartBeat.set(true);
            heartBeatTimer = new Timer(topologyContext.getThisTaskId() + "-hb-timer", true);
            setupHeartBeatTimer();

        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    @Override
    public void execute(Tuple tuple) {
        try {
            if (batchHelper.shouldHandle(tuple)) {
                List<String> partitionVals = options.getMapper().mapPartitions(tuple);
                HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
                HiveWriter writer = getOrCreateWriter(endPoint);
                writer.write(options.getMapper().mapRecord(tuple));
                batchHelper.addBatch(tuple);
            }

            if (batchHelper.shouldFlush()) {
                flushAllWriters(true);
                LOG.info("acknowledging tuples after writers flushed ");
                batchHelper.ack();
            }
            if (TupleUtils.isTick(tuple)) {
                retireIdleWriters();
            }
        } catch (SerializationError se) {
            LOG.info("Serialization exception occurred, tuple is acknowledged but not written to Hive.", tuple);
            this.collector.reportError(se);
            collector.ack(tuple);
        } catch (Exception e) {
            batchHelper.fail(e);
            abortAndCloseWriters();
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {

    }

    @Override
    public void cleanup() {
        sendHeartBeat.set(false);
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                HiveWriter w = entry.getValue();
                w.flushAndClose();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }

        callTimeoutPool = null;
        if (heartBeatTimer != null) {
            heartBeatTimer.cancel();
        }
        super.cleanup();
        LOG.info("Hive Bolt stopped");
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        Map<String, Object> conf = super.getComponentConfiguration();
        if (conf == null) {
            conf = new Config();
        }

        if (options.getTickTupleInterval() > 0) {
            conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, options.getTickTupleInterval());
        }

        return conf;
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat.get()) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    void flushAllWriters(boolean rollToNext)
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(rollToNext);
        }
    }

    void abortAndCloseWriters() {
        try {
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().abort();
            } catch (Exception e) {
                LOG.error("Failed to abort hive transaction batch, HiveEndPoint " + entry.getValue() + " due to exception ", e);
            }
        }
    }

    /**
     * Closes all writers and remove them from cache.
     */
    private void closeAllWriters() {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().close();
            } catch (Exception e) {
                LOG.warn("unable to close writers. ", e);
            }
        }
        //2) Clear cache
        allWriters.clear();
    }

    @VisibleForTesting
    HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.debug("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    LOG.info("cached HiveEndPoint size {} exceeded maxOpenConnections {} ", allWriters.size(),
                             options.getMaxOpenConnections());
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
                HiveUtils.logAllHiveEndPoints(allWriters);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }
    }

    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        LOG.info("Attempting close eldest writers");
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        LOG.info("Attempting close idle writers");
        int count = 0;
        long now = System.currentTimeMillis();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retire(entry.getKey());
            }
        }
        return count;
    }

    private void retire(HiveEndPoint ep) {
        try {
            HiveWriter writer = allWriters.remove(ep);
            if (writer != null) {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                writer.flushAndClose();
            }
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
        }
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.bolt;

import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.SerializationError;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.Config;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.BatchHelper;
import org.apache.storm.utils.TupleUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveBolt extends BaseRichBolt {
    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
    @VisibleForTesting
    Map<HiveEndPoint, HiveWriter> allWriters;
    private OutputCollector collector;
    private HiveOptions options;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean sendHeartBeat = new AtomicBoolean(false);
    private UserGroupInformation ugi = null;
    private BatchHelper batchHelper;
    private boolean tokenAuthEnabled;

    public HiveBolt(HiveOptions options) {
        this.options = options;
    }

    @Override
    public void prepare(Map<String, Object> conf, TopologyContext topologyContext, OutputCollector collector) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            this.collector = collector;
            this.batchHelper = new BatchHelper(options.getBatchSize(), collector);
            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());

            sendHeartBeat.set(true);
            heartBeatTimer = new Timer(topologyContext.getThisTaskId() + "-hb-timer", true);
            setupHeartBeatTimer();

        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    @Override
    public void execute(Tuple tuple) {
        try {
            if (batchHelper.shouldHandle(tuple)) {
                List<String> partitionVals = options.getMapper().mapPartitions(tuple);
                HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
                HiveWriter writer = getOrCreateWriter(endPoint);
                writer.write(options.getMapper().mapRecord(tuple));
                batchHelper.addBatch(tuple);
            }

            if (batchHelper.shouldFlush()) {
                flushAllWriters(true);
                LOG.info("acknowledging tuples after writers flushed ");
                batchHelper.ack();
            }
            if (TupleUtils.isTick(tuple)) {
                retireIdleWriters();
            }
        } catch (SerializationError se) {
            LOG.info("Serialization exception occurred, tuple is acknowledged but not written to Hive.", tuple);
            this.collector.reportError(se);
            collector.ack(tuple);
        } catch (Exception e) {
            batchHelper.fail(e);
            abortAndCloseWriters();
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {

    }

    @Override
    public void cleanup() {
        sendHeartBeat.set(false);
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                HiveWriter w = entry.getValue();
                w.flushAndClose();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }

        callTimeoutPool = null;
        if (heartBeatTimer != null) {
            heartBeatTimer.cancel();
        }
        super.cleanup();
        LOG.info("Hive Bolt stopped");
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        Map<String, Object> conf = super.getComponentConfiguration();
        if (conf == null) {
            conf = new Config();
        }

        if (options.getTickTupleInterval() > 0) {
            conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, options.getTickTupleInterval());
        }

        return conf;
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat.get()) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    void flushAllWriters(boolean rollToNext)
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(rollToNext);
        }
    }

    void abortAndCloseWriters() {
        try {
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().abort();
            } catch (Exception e) {
                LOG.error("Failed to abort hive transaction batch, HiveEndPoint " + entry.getValue() + " due to exception ", e);
            }
        }
    }

    /**
     * Closes all writers and remove them from cache.
     */
    private void closeAllWriters() {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().close();
            } catch (Exception e) {
                LOG.warn("unable to close writers. ", e);
            }
        }
        //2) Clear cache
        allWriters.clear();
    }

    @VisibleForTesting
    HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.debug("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    LOG.info("cached HiveEndPoint size {} exceeded maxOpenConnections {} ", allWriters.size(),
                             options.getMaxOpenConnections());
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
                HiveUtils.logAllHiveEndPoints(allWriters);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }
    }

    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        LOG.info("Attempting close eldest writers");
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        LOG.info("Attempting close idle writers");
        int count = 0;
        long now = System.currentTimeMillis();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retire(entry.getKey());
            }
        }
        return count;
    }

    private void retire(HiveEndPoint ep) {
        try {
            HiveWriter writer = allWriters.remove(ep);
            if (writer != null) {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                writer.flushAndClose();
            }
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
        }
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.trident;

import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.IMetricsContext;
import org.apache.storm.topology.FailedException;
import org.apache.storm.trident.operation.TridentCollector;
import org.apache.storm.trident.state.State;
import org.apache.storm.trident.tuple.TridentTuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveState implements State {
    private static final Logger LOG = LoggerFactory.getLogger(HiveState.class);
    private HiveOptions options;
    private Integer currentBatchSize;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
    private Boolean sendHeartBeat = true;
    private UserGroupInformation ugi = null;
    private Boolean kerberosEnabled = false;
    private Map<HiveEndPoint, HiveWriter> allWriters;
    private boolean tokenAuthEnabled;

    public HiveState(HiveOptions options) {
        this.options = options;
        this.currentBatchSize = 0;
    }


    @Override
    public void beginCommit(Long txId) {
    }

    @Override
    public void commit(Long txId) {
        try {
            flushAllWriters();
            currentBatchSize = 0;
        } catch (HiveWriter.TxnFailure | InterruptedException | HiveWriter.CommitFailure | HiveWriter.TxnBatchFailure ex) {
            LOG.warn("Commit failed. Failing the batch.", ex);
            throw new FailedException(ex);
        }
    }

    public void prepare(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
            heartBeatTimer = new Timer("hive-hb-timer", true);
            setupHeartBeatTimer();
        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
        try {
            writeTuples(tuples);
        } catch (Exception e) {
            abortAndCloseWriters();
            LOG.warn("hive streaming failed.", e);
            throw new FailedException(e);
        }
    }

    private void writeTuples(List<TridentTuple> tuples)
        throws Exception {
        for (TridentTuple tuple : tuples) {
            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
            HiveWriter writer = getOrCreateWriter(endPoint);
            writer.write(options.getMapper().mapRecord(tuple));
            currentBatchSize++;
            if (currentBatchSize >= options.getBatchSize()) {
                flushAllWriters();
                currentBatchSize = 0;
            }
        }
    }

    private void abortAndCloseWriters() {
        try {
            sendHeartBeat = false;
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().abort();
        }
    }


    /**
     * Closes all writers and remove them from cache.
     * @return number of writers retired
     */
    private void closeAllWriters() throws InterruptedException, IOException {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().close();
        }
        //2) Clear cache
        allWriters.clear();
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void flushAllWriters()
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(true);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.info("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }

    }


    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        int count = 0;
        long now = System.currentTimeMillis();
        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retirees.add(entry.getKey());
            }
        }
        //2) Retire them
        for (HiveEndPoint ep : retirees) {
            try {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                allWriters.remove(ep).flushAndClose();
            } catch (IOException e) {
                LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
            } catch (InterruptedException e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
                Thread.currentThread().interrupt();
            } catch (Exception e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            }
        }
        return count;
    }

    public void cleanup() {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                sendHeartBeat = false;
                HiveWriter w = entry.getValue();
                LOG.info("Flushing writer to {}", w);
                w.flush(false);
                LOG.info("Closing writer to {}", w);
                w.close();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }
        heartBeatTimer.cancel();
        callTimeoutPool = null;
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.trident;

import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.IMetricsContext;
import org.apache.storm.topology.FailedException;
import org.apache.storm.trident.operation.TridentCollector;
import org.apache.storm.trident.state.State;
import org.apache.storm.trident.tuple.TridentTuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveState implements State {
    private static final Logger LOG = LoggerFactory.getLogger(HiveState.class);
    private HiveOptions options;
    private Integer currentBatchSize;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
    private Boolean sendHeartBeat = true;
    private UserGroupInformation ugi = null;
    private Boolean kerberosEnabled = false;
    private Map<HiveEndPoint, HiveWriter> allWriters;
    private boolean tokenAuthEnabled;

    public HiveState(HiveOptions options) {
        this.options = options;
        this.currentBatchSize = 0;
    }


    @Override
    public void beginCommit(Long txId) {
    }

    @Override
    public void commit(Long txId) {
        try {
            flushAllWriters();
            currentBatchSize = 0;
        } catch (HiveWriter.TxnFailure | InterruptedException | HiveWriter.CommitFailure | HiveWriter.TxnBatchFailure ex) {
            LOG.warn("Commit failed. Failing the batch.", ex);
            throw new FailedException(ex);
        }
    }

    public void prepare(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
            heartBeatTimer = new Timer("hive-hb-timer", true);
            setupHeartBeatTimer();
        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
        try {
            writeTuples(tuples);
        } catch (Exception e) {
            abortAndCloseWriters();
            LOG.warn("hive streaming failed.", e);
            throw new FailedException(e);
        }
    }

    private void writeTuples(List<TridentTuple> tuples)
        throws Exception {
        for (TridentTuple tuple : tuples) {
            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
            HiveWriter writer = getOrCreateWriter(endPoint);
            writer.write(options.getMapper().mapRecord(tuple));
            currentBatchSize++;
            if (currentBatchSize >= options.getBatchSize()) {
                flushAllWriters();
                currentBatchSize = 0;
            }
        }
    }

    private void abortAndCloseWriters() {
        try {
            sendHeartBeat = false;
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().abort();
        }
    }


    /**
     * Closes all writers and remove them from cache.
     * @return number of writers retired
     */
    private void closeAllWriters() throws InterruptedException, IOException {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().close();
        }
        //2) Clear cache
        allWriters.clear();
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void flushAllWriters()
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(true);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.info("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }

    }


    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        int count = 0;
        long now = System.currentTimeMillis();
        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retirees.add(entry.getKey());
            }
        }
        //2) Retire them
        for (HiveEndPoint ep : retirees) {
            try {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                allWriters.remove(ep).flushAndClose();
            } catch (IOException e) {
                LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
            } catch (InterruptedException e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
                Thread.currentThread().interrupt();
            } catch (Exception e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            }
        }
        return count;
    }

    public void cleanup() {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                sendHeartBeat = false;
                HiveWriter w = entry.getValue();
                LOG.info("Flushing writer to {}", w);
                w.flush(false);
                LOG.info("Closing writer to {}", w);
                w.close();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }
        heartBeatTimer.cancel();
        callTimeoutPool = null;
    }

}

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The diff file introduces a new file `HiveUtils.java` which contains critical security-related code, specifically the `authenticate` method that handles Kerberos authentication. This method is crucial for ensuring secure access control and authentication. Given that this method is called in the `HiveBolt` and `HiveState` classes, it directly impacts the system's authentication mechanism. Any changes or additions to authentication mechanisms warrant a re-run of security tests to ensure there are no vulnerabilities or issues introduced.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 25
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java
new file mode 100644
index 000000000..6050aa886
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java
@@ -0,0 +1,306 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.trident;
+
+import storm.trident.operation.TridentCollector;
+import storm.trident.state.State;
+import storm.trident.tuple.TridentTuple;
+import backtype.storm.task.IMetricsContext;
+import backtype.storm.topology.FailedException;
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.hive.hcatalog.streaming.*;
+import org.apache.storm.hive.common.HiveOptions;
+import org.apache.storm.hive.common.HiveUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.Map.Entry;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class HiveState implements State {
+    private static final Logger LOG = LoggerFactory.getLogger(HiveState.class);
+    private HiveOptions options;
+    private Integer currentBatchSize;
+    private ExecutorService callTimeoutPool;
+    private transient Timer heartBeatTimer;
+    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
+    private UserGroupInformation ugi = null;
+    private Boolean kerberosEnabled = false;
+    HashMap<HiveEndPoint, HiveWriter> allWriters;
+
+    public HiveState(HiveOptions options) {
+        this.options = options;
+        this.currentBatchSize = 0;
+    }
+
+
+    @Override
+    public void beginCommit(Long txId) {
+    }
+
+    @Override
+    public void commit(Long txId) {
+    }
+
+    public void prepare(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions)  {
+        try {
+            if(options.getKerberosPrincipal() == null && options.getKerberosKeytab() == null) {
+                kerberosEnabled = false;
+            } else if(options.getKerberosPrincipal() != null && options.getKerberosKeytab() != null) {
+                kerberosEnabled = true;
+            } else {
+                throw new IllegalArgumentException("To enable Kerberos, need to set both KerberosPrincipal " +
+                                                   " & KerberosKeytab");
+            }
+
+            if (kerberosEnabled) {
+                try {
+                    ugi = HiveUtils.authenticate(options.getKerberosKeytab(), options.getKerberosPrincipal());
+                } catch(HiveUtils.AuthenticationFailed ex) {
+                    LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
+                    throw new IllegalArgumentException(ex);
+                }
+            }
+
+            allWriters = new HashMap<HiveEndPoint,HiveWriter>();
+            String timeoutName = "hive-bolt-%d";
+            this.callTimeoutPool = Executors.newFixedThreadPool(1,
+                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
+            heartBeatTimer= new Timer();
+            setupHeartBeatTimer();
+        } catch(Exception e) {
+            LOG.warn("unable to make connection to hive ",e);
+        }
+    }
+
+    public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
+        try {
+            writeTuples(tuples);
+        } catch (Exception e) {
+            abortAndCloseWriters();
+            LOG.warn("hive streaming failed.",e);
+            throw new FailedException(e);
+        }
+    }
+
+    private void writeTuples(List<TridentTuple> tuples)
+        throws Exception {
+        if(timeToSendHeartBeat.compareAndSet(true, false)) {
+            enableHeartBeatOnAllWriters();
+        }
+        for (TridentTuple tuple : tuples) {
+            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
+            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
+            HiveWriter writer = getOrCreateWriter(endPoint);
+            writer.write(options.getMapper().mapRecord(tuple));
+            currentBatchSize++;
+            if(currentBatchSize >= options.getBatchSize()) {
+                flushAllWriters();
+                currentBatchSize = 0;
+            }
+        }
+    }
+
+    private void abortAndCloseWriters() {
+        try {
+            abortAllWriters();
+            closeAllWriters();
+        } catch(InterruptedException e) {
+            LOG.warn("unable to close hive connections. ", e);
+        } catch(IOException ie) {
+            LOG.warn("unable to close hive connections. ", ie);
+        }
+    }
+
+    /**
+     * Abort current Txn on all writers
+     * @return number of writers retired
+     */
+    private void abortAllWriters() throws InterruptedException {
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            entry.getValue().abort();
+        }
+    }
+
+
+    /**
+     * Closes all writers and remove them from cache
+     * @return number of writers retired
+     */
+    private void closeAllWriters() throws InterruptedException, IOException {
+        //1) Retire writers
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            entry.getValue().close();
+        }
+        //2) Clear cache
+        allWriters.clear();
+    }
+
+    private void setupHeartBeatTimer() {
+        if(options.getHeartBeatInterval()>0) {
+            heartBeatTimer.schedule(new TimerTask() {
+                    @Override
+                    public void run() {
+                        timeToSendHeartBeat.set(true);
+                        setupHeartBeatTimer();
+                    }
+                }, options.getHeartBeatInterval() * 1000);
+        }
+    }
+
+    private void flushAllWriters()
+        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
+        for(HiveWriter writer: allWriters.values()) {
+            writer.flush(true);
+        }
+    }
+
+    private void enableHeartBeatOnAllWriters() {
+        for (HiveWriter writer : allWriters.values()) {
+            writer.setHeartBeatNeeded();
+        }
+    }
+
+    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
+        throws HiveWriter.ConnectFailure, InterruptedException {
+        try {
+            HiveWriter writer = allWriters.get( endPoint );
+            if( writer == null ) {
+                LOG.info("Creating Writer to Hive end point : " + endPoint);
+                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options);
+                if(allWriters.size() > options.getMaxOpenConnections()){
+                    int retired = retireIdleWriters();
+                    if(retired==0) {
+                        retireEldestWriter();
+                    }
+                }
+                allWriters.put(endPoint, writer);
+            }
+            return writer;
+        } catch (HiveWriter.ConnectFailure e) {
+            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
+            throw e;
+        }
+
+    }
+
+
+
+    /**
+     * Locate writer that has not been used for longest time and retire it
+     */
+    private void retireEldestWriter() {
+        long oldestTimeStamp = System.currentTimeMillis();
+        HiveEndPoint eldest = null;
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(entry.getValue().getLastUsed() < oldestTimeStamp) {
+                eldest = entry.getKey();
+                oldestTimeStamp = entry.getValue().getLastUsed();
+            }
+        }
+        try {
+            LOG.info("Closing least used Writer to Hive end point : " + eldest);
+            allWriters.remove(eldest).close();
+        } catch (IOException e) {
+            LOG.warn("Failed to close writer for end point: " + eldest, e);
+        } catch (InterruptedException e) {
+            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
+            Thread.currentThread().interrupt();
+        }
+    }
+
+    /**
+     * Locate all writers past idle timeout and retire them
+     * @return number of writers retired
+     */
+    private int retireIdleWriters() {
+        int count = 0;
+        long now = System.currentTimeMillis();
+        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();
+
+        //1) Find retirement candidates
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
+                ++count;
+                retirees.add(entry.getKey());
+            }
+        }
+        //2) Retire them
+        for(HiveEndPoint ep : retirees) {
+            try {
+                LOG.info("Closing idle Writer to Hive end point : {}", ep);
+                allWriters.remove(ep).close();
+            } catch (IOException e) {
+                LOG.warn("Failed to close writer for end point: {}. Error: "+ ep, e);
+            } catch (InterruptedException e) {
+                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
+                Thread.currentThread().interrupt();
+            }
+        }
+        return count;
+    }
+
+    public void cleanup() {
+        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
+            try {
+                HiveWriter w = entry.getValue();
+                LOG.info("Flushing writer to {}", w);
+                w.flush(false);
+                LOG.info("Closing writer to {}", w);
+                w.close();
+            } catch (Exception ex) {
+                LOG.warn("Error while closing writer to " + entry.getKey() +
+                         ". Exception follows.", ex);
+                if (ex instanceof InterruptedException) {
+                    Thread.currentThread().interrupt();
+                }
+            }
+        }
+
+        ExecutorService toShutdown[] = {callTimeoutPool};
+        for (ExecutorService execService : toShutdown) {
+            execService.shutdown();
+            try {
+                while (!execService.isTerminated()) {
+                    execService.awaitTermination(
+                                                 options.getCallTimeOut(), TimeUnit.MILLISECONDS);
+                }
+            } catch (InterruptedException ex) {
+                LOG.warn("shutdown interrupted on " + execService, ex);
+            }
+        }
+        callTimeoutPool = null;
+    }
+
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.trident;

import java.util.Map;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.task.IMetricsContext;
import org.apache.storm.trident.state.State;
import org.apache.storm.trident.state.StateFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class HiveStateFactory implements StateFactory {
    private static final Logger LOG = LoggerFactory.getLogger(HiveStateFactory.class);
    private HiveOptions options;

    public HiveStateFactory() {}

    /**
     * The options for connecting to Hive.
     */
    public HiveStateFactory withOptions(HiveOptions options) {
        if (options.getTickTupleInterval() != HiveOptions.DEFAULT_TICK_TUPLE_INTERVAL_SECS) {
            LOG.error("Tick tuple interval will be ignored for trident."
                    + " The Hive writers are flushed after each batch.");
        }
        this.options = options;
        return this;
    }

    @Override
    public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
        LOG.info("makeState(partitonIndex={}, numpartitions={}", partitionIndex, numPartitions);
        HiveState state = new HiveState(this.options);
        state.prepare(conf, metrics, partitionIndex, numPartitions);
        return state;
    }
}

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The diff file introduces a new class `HiveState` which includes significant changes related to authentication mechanisms, specifically the handling of Kerberos authentication. The `prepare` method checks for Kerberos credentials and attempts to authenticate using them. This directly impacts the security of the system as it involves authentication mechanisms. Additionally, the class handles connections to Hive, which could involve sensitive data, and includes logging of potential security events. Given these changes, it is crucial to re-run the security tests to ensure that the new authentication and connection handling mechanisms do not introduce vulnerabilities.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 26
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java
new file mode 100644
index 000000000..8f3b9e94d
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java
@@ -0,0 +1,31 @@
+package org.apache.storm.hive.trident;
+
+import backtype.storm.task.IMetricsContext;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import storm.trident.state.State;
+import storm.trident.state.StateFactory;
+import org.apache.storm.hive.common.HiveOptions;
+
+import java.util.Map;
+
+
+public class HiveStateFactory implements StateFactory {
+    private static final Logger LOG = LoggerFactory.getLogger(HiveStateFactory.class);
+    private HiveOptions options;
+
+    public HiveStateFactory(){}
+
+    public HiveStateFactory withOptions(HiveOptions options){
+        this.options = options;
+        return this;
+    }
+
+    @Override
+    public State makeState(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        LOG.info("makeState(partitonIndex={}, numpartitions={}", partitionIndex, numPartitions);
+        HiveState state = new HiveState(this.options);
+        state.prepare(conf, metrics, partitionIndex, numPartitions);
+        return state;
+    }
+}

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The changes in the diff file involve the addition of a new class `HiveStateFactory` which implements the `StateFactory` interface. The modifications do not directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The new class primarily deals with state management and does not introduce any new security risks that would necessitate re-running security tests.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 27
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java
new file mode 100644
index 000000000..b0b32f10f
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java
@@ -0,0 +1,14 @@
+package org.apache.storm.hive.trident;
+
+import storm.trident.operation.TridentCollector;
+import storm.trident.state.BaseStateUpdater;
+import storm.trident.tuple.TridentTuple;
+
+import java.util.List;
+
+public class HiveUpdater extends BaseStateUpdater<HiveState>{
+    @Override
+    public void updateState(HiveState state, List<TridentTuple> tuples, TridentCollector collector) {
+        state.updateState(tuples, collector);
+    }
+}

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The provided diff file shows the addition of a new class `HiveUpdater` which extends `BaseStateUpdater` and overrides the `updateState` method. This change does not directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The new class appears to be related to state updating in a Trident topology, which is not inherently a security-sensitive operation.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 28
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveSetupUtil.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveSetupUtil.java
new file mode 100644
index 000000000..d49281993
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveSetupUtil.java
@@ -0,0 +1,220 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.RawLocalFileSystem;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.hadoop.hive.metastore.IMetaStoreClient;
+import org.apache.hadoop.hive.metastore.TableType;
+import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
+import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;
+import org.apache.hadoop.hive.ql.io.orc.OrcSerde;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.thrift.TException;
+
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+public class HiveSetupUtil {
+    public static class RawFileSystem extends RawLocalFileSystem {
+        private static final URI NAME;
+        static {
+            try {
+                NAME = new URI("raw:///");
+            } catch (URISyntaxException se) {
+                throw new IllegalArgumentException("bad uri", se);
+            }
+        }
+
+        @Override
+        public URI getUri() {
+            return NAME;
+        }
+
+        @Override
+        public FileStatus getFileStatus(Path path) throws IOException {
+            File file = pathToFile(path);
+            if (!file.exists()) {
+                throw new FileNotFoundException("Can't find " + path);
+            }
+            // get close enough
+            short mod = 0;
+            if (file.canRead()) {
+                mod |= 0444;
+            }
+            if (file.canWrite()) {
+                mod |= 0200;
+            }
+            if (file.canExecute()) {
+                mod |= 0111;
+            }
+            ShimLoader.getHadoopShims();
+            return new FileStatus(file.length(), file.isDirectory(), 1, 1024,
+                                  file.lastModified(), file.lastModified(),
+                                  FsPermission.createImmutable(mod), "owen", "users", path);
+        }
+    }
+
+    private final static String txnMgr = "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager";
+
+    public static HiveConf getHiveConf() {
+        HiveConf conf = new HiveConf();
+        // String metastoreDBLocation = "jdbc:derby:databaseName=/tmp/metastore_db;create=true";
+        // conf.set("javax.jdo.option.ConnectionDriverName","org.apache.derby.jdbc.EmbeddedDriver");
+        // conf.set("javax.jdo.option.ConnectionURL",metastoreDBLocation);
+        conf.set("fs.raw.impl", RawFileSystem.class.getName());
+        conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, txnMgr);
+        conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
+        return conf;
+    }
+
+    public static void createDbAndTable(HiveConf conf, String databaseName,
+                                        String tableName, List<String> partVals,
+                                        String[] colNames, String[] colTypes,
+                                        String[] partNames, String dbLocation)
+        throws Exception {
+        IMetaStoreClient client = new HiveMetaStoreClient(conf);
+        try {
+            Database db = new Database();
+            db.setName(databaseName);
+            db.setLocationUri(dbLocation);
+            client.createDatabase(db);
+
+            Table tbl = new Table();
+            tbl.setDbName(databaseName);
+            tbl.setTableName(tableName);
+            tbl.setTableType(TableType.MANAGED_TABLE.toString());
+            StorageDescriptor sd = new StorageDescriptor();
+            sd.setCols(getTableColumns(colNames, colTypes));
+            sd.setNumBuckets(1);
+            sd.setLocation(dbLocation + Path.SEPARATOR + tableName);
+            if(partNames!=null && partNames.length!=0) {
+                tbl.setPartitionKeys(getPartitionKeys(partNames));
+            }
+
+            tbl.setSd(sd);
+
+            sd.setBucketCols(new ArrayList<String>(2));
+            sd.setSerdeInfo(new SerDeInfo());
+            sd.getSerdeInfo().setName(tbl.getTableName());
+            sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+            sd.getSerdeInfo().getParameters().put(serdeConstants.SERIALIZATION_FORMAT, "1");
+
+            sd.getSerdeInfo().setSerializationLib(OrcSerde.class.getName());
+            sd.setInputFormat(OrcInputFormat.class.getName());
+            sd.setOutputFormat(OrcOutputFormat.class.getName());
+
+            Map<String, String> tableParams = new HashMap<String, String>();
+            tbl.setParameters(tableParams);
+            client.createTable(tbl);
+            try {
+                if(partVals!=null && partVals.size() > 0) {
+                    addPartition(client, tbl, partVals);
+                }
+            } catch(AlreadyExistsException e) {
+            }
+        } finally {
+            client.close();
+        }
+    }
+
+    // delete db and all tables in it
+    public static void dropDB(HiveConf conf, String databaseName) throws HiveException, MetaException {
+        IMetaStoreClient client = new HiveMetaStoreClient(conf);
+        try {
+            for (String table : client.listTableNamesByFilter(databaseName, "", (short) -1)) {
+                client.dropTable(databaseName, table, true, true);
+            }
+            client.dropDatabase(databaseName);
+        } catch (TException e) {
+            client.close();
+        }
+    }
+
+    private static void addPartition(IMetaStoreClient client, Table tbl
+                                     , List<String> partValues)
+        throws IOException, TException {
+        Partition part = new Partition();
+        part.setDbName(tbl.getDbName());
+        part.setTableName(tbl.getTableName());
+        StorageDescriptor sd = new StorageDescriptor(tbl.getSd());
+        sd.setLocation(sd.getLocation() + Path.SEPARATOR + makePartPath(tbl.getPartitionKeys(), partValues));
+        part.setSd(sd);
+        part.setValues(partValues);
+        client.add_partition(part);
+    }
+
+    private static String makePartPath(List<FieldSchema> partKeys, List<String> partVals) {
+        if(partKeys.size()!=partVals.size()) {
+            throw new IllegalArgumentException("Partition values:" + partVals +
+                                               ", does not match the partition Keys in table :" + partKeys );
+        }
+        StringBuffer buff = new StringBuffer(partKeys.size()*20);
+        int i=0;
+        for(FieldSchema schema : partKeys) {
+            buff.append(schema.getName());
+            buff.append("=");
+            buff.append(partVals.get(i));
+            if(i!=partKeys.size()-1) {
+                buff.append(Path.SEPARATOR);
+            }
+            ++i;
+        }
+        return buff.toString();
+    }
+
+    private static List<FieldSchema> getTableColumns(String[] colNames, String[] colTypes) {
+        List<FieldSchema> fields = new ArrayList<FieldSchema>();
+        for (int i=0; i<colNames.length; ++i) {
+            fields.add(new FieldSchema(colNames[i], colTypes[i], ""));
+        }
+        return fields;
+    }
+
+    private static List<FieldSchema> getPartitionKeys(String[] partNames) {
+        List<FieldSchema> fields = new ArrayList<FieldSchema>();
+        for (int i=0; i < partNames.length; ++i) {
+           fields.add(new FieldSchema(partNames[i], serdeConstants.STRING_TYPE_NAME, ""));
+        }
+        return fields;
+    }
+
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.bolt;

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.google.common.collect.Lists;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.txn.TxnDbUtil;
import org.apache.hadoop.hive.serde.serdeConstants;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.storm.Config;
import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
import org.apache.storm.hive.bolt.mapper.JsonRecordHiveMapper;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.GeneralTopologyContext;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.TupleImpl;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.MockTupleHelpers;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.Mock;
import org.mockito.Mockito;
import org.mockito.MockitoAnnotations;
import org.mockito.invocation.InvocationOnMock;
import org.mockito.stubbing.Answer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.any;
import static org.mockito.Mockito.never;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.verifyNoInteractions;

public class TestHiveBolt {
    final static String dbName = "testdb";
    final static String tblName = "test_table";
    final static String dbName1 = "testdb1";
    final static String tblName1 = "test_table1";
    final static String PART1_NAME = "city";
    final static String PART2_NAME = "state";
    final static String[] partNames = { PART1_NAME, PART2_NAME };
    private static final String COL1 = "id";
    private static final String COL2 = "msg";
    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
    final String partitionVals = "sunnyvale,ca";
    final String[] colNames = { COL1, COL2 };
    final String[] colNames1 = { COL2, COL1 };
    final String metaStoreURI;
    private final HiveConf conf;
    private String[] colTypes = { serdeConstants.INT_TYPE_NAME, serdeConstants.STRING_TYPE_NAME };
    private Config config = new Config();
    private TestingHiveBolt bolt;
    ;
    private ObjectMapper objectMapper = new ObjectMapper();
    @Mock
    private OutputCollector collector;

    public TestHiveBolt() throws Exception {
        //metaStoreURI = "jdbc:derby:;databaseName="+System.getProperty("java.io.tmpdir") +"metastore_db;create=true";
        metaStoreURI = null;
        conf = HiveSetupUtil.getHiveConf();
        TxnDbUtil.setConfValues(conf);
        if (metaStoreURI != null) {
            conf.setVar(HiveConf.ConfVars.METASTOREURIS, metaStoreURI);
        }
    }

    @BeforeEach
    public void setup() throws Exception {
        MockitoAnnotations.initMocks(this);
    }

    @Test
    public void testWithByteArrayIdandMessage()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 100;
        String msg = "test-123";
        String city = "sunnyvale";
        String state = "ca";

        Set<Tuple> tupleSet = new HashSet<Tuple>();
        for (int i = 0; i < 4; i++) {
            Tuple tuple = generateTestTuple(id, msg, city, state);
            bolt.execute(tuple);
            tupleSet.add(tuple);
        }

        List<String> partVals = Lists.newArrayList(city, state);

        for (Tuple t : tupleSet) {
            verify(collector).ack(t);
        }

        assertEquals(4, bolt.getRecordWritten(partVals).size());

        bolt.cleanup();
    }

    @Test
    public void testWithoutPartitions()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName1, tblName1, mapper)
            .withTxnsPerBatch(2).withBatchSize(2).withAutoCreatePartitions(false);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 100;
        String msg = "test-123";
        String city = "sunnyvale";
        String state = "ca";

        Set<Tuple> tupleSet = new HashSet<Tuple>();
        for (int i = 0; i < 4; i++) {
            Tuple tuple = generateTestTuple(id, msg, city, state);
            bolt.execute(tuple);
            tupleSet.add(tuple);
        }

        List<String> partVals = Collections.emptyList();

        for (Tuple t : tupleSet) {
            verify(collector).ack(t);
        }

        List<byte[]> recordWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordWritten);
        assertEquals(4, recordWritten.size());

        bolt.cleanup();
    }

    @Test
    public void testWithTimeformat()
        throws Exception {
        String timeFormat = "yyyy/MM/dd";
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField(timeFormat);
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName1, tblName1, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(1)
            .withMaxOpenConnections(1);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 100;
        String msg = "test-123";
        Date d = new Date();
        SimpleDateFormat parseDate = new SimpleDateFormat(timeFormat);
        String today = parseDate.format(d.getTime());

        List<Tuple> tuples = new ArrayList<>();
        for (int i = 0; i < 2; i++) {
            Tuple tuple = generateTestTuple(id, msg, null, null);
            tuples.add(tuple);
            bolt.execute(tuple);
        }

        for (Tuple t : tuples) {
            verify(collector).ack(t);
        }

        List<String> partVals = Lists.newArrayList(today);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(2, recordsWritten.size());

        byte[] mapped = generateDelimiteredRecord(Lists.newArrayList(id, msg), mapper.getFieldDelimiter());

        for (byte[] record : recordsWritten) {
            assertArrayEquals(mapped, record);
        }

        bolt.cleanup();
    }

    @Test
    public void testData()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(1);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Integer id = 1;
        String msg = "SJC";
        String city = "Sunnyvale";
        String state = "CA";

        Tuple tuple1 = generateTestTuple(id, msg, city, state);

        bolt.execute(tuple1);
        verify(collector).ack(tuple1);

        List<String> partVals = Lists.newArrayList(city, state);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(1, recordsWritten.size());

        byte[] mapped = generateDelimiteredRecord(Lists.newArrayList(id, msg), mapper.getFieldDelimiter());
        assertArrayEquals(mapped, recordsWritten.get(0));

        bolt.cleanup();
    }

    @Test
    public void testJsonWriter()
        throws Exception {
        // json record doesn't need columns to be in the same order
        // as table in hive.
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(1);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 1;
        String msg = "SJC";
        String city = "Sunnyvale";
        String state = "CA";

        Tuple tuple1 = generateTestTuple(id, msg, city, state);

        bolt.execute(tuple1);
        verify(collector).ack(tuple1);

        List<String> partVals = Lists.newArrayList(city, state);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(1, recordsWritten.size());

        byte[] written = recordsWritten.get(0);

        Map<String, ?> writtenMap = objectMapper.readValue(new String(written), new TypeReference<Map<String, ?>>() {
        });

        Map<String, Object> expected = new HashMap<>();
        expected.put(COL1, id);
        expected.put(COL2, msg);

        assertEquals(expected, writtenMap);

        bolt.cleanup();
    }

    @Test
    public void testNoAcksUntilFlushed() {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Tuple tuple1 = generateTestTuple(1, "SJC", "Sunnyvale", "CA");
        Tuple tuple2 = generateTestTuple(2, "SFO", "San Jose", "CA");

        bolt.execute(tuple1);
        verifyNoInteractions(collector);

        bolt.execute(tuple2);
        verify(collector).ack(tuple1);
        verify(collector).ack(tuple2);
        bolt.cleanup();
    }

    @Test
    public void testNoAcksIfFlushFails() throws Exception {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        HiveBolt failingBolt = new FlushFailureHiveBolt(hiveOptions);

        failingBolt.prepare(config, null, new OutputCollector(collector));

        Tuple tuple1 = generateTestTuple(1, "SJC", "Sunnyvale", "CA");
        Tuple tuple2 = generateTestTuple(2, "SFO", "San Jose", "CA");

        failingBolt.execute(tuple1);
        failingBolt.execute(tuple2);

        verify(collector, never()).ack(tuple1);
        verify(collector, never()).ack(tuple2);

        failingBolt.cleanup();
    }

    @Test
    public void testTickTuple() {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Tuple tuple1 = generateTestTuple(1, "SJC", "Sunnyvale", "CA");
        Tuple tuple2 = generateTestTuple(2, "SFO", "San Jose", "CA");

        bolt.execute(tuple1);

        //The tick should cause tuple1 to be ack'd
        Tuple mockTick = MockTupleHelpers.mockTickTuple();
        bolt.execute(mockTick);
        verify(collector).ack(tuple1);

        //The second tuple should NOT be ack'd because the batch should be cleared and this will be
        //the first transaction in the new batch
        bolt.execute(tuple2);
        verify(collector, never()).ack(tuple2);

        bolt.cleanup();
    }

    @Test
    public void testNoTickEmptyBatches() throws Exception {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        //The tick should NOT cause any acks since the batch was empty except for acking itself
        Tuple mockTick = MockTupleHelpers.mockTickTuple();
        bolt.execute(mockTick);
        verifyNoInteractions(collector);

        bolt.cleanup();
    }

    @Test
    public void testMultiPartitionTuples()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(10)
            .withBatchSize(10);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Integer id = 1;
        String msg = "test";
        String city = "San Jose";
        String state = "CA";

        List<Tuple> tuples = new ArrayList<>();
        for (int i = 0; i < 100; i++) {
            Tuple tuple = generateTestTuple(id, msg, city, state);
            tuples.add(tuple);
            bolt.execute(tuple);
        }

        for (Tuple t : tuples) {
            verify(collector).ack(t);
        }

        List<String> partVals = Lists.newArrayList(city, state);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(100, recordsWritten.size());


        byte[] mapped = generateDelimiteredRecord(Lists.newArrayList(id, msg), mapper.getFieldDelimiter());

        for (byte[] record : recordsWritten) {
            assertArrayEquals(mapped, record);
        }

        bolt.cleanup();
    }

    private Tuple generateTestTuple(Object id, Object msg, Object city, Object state) {
        TopologyBuilder builder = new TopologyBuilder();
        GeneralTopologyContext topologyContext = new GeneralTopologyContext(builder.createTopology(),
                                                                            new Config(), new HashMap(), new HashMap(), new HashMap(), "") {
            @Override
            public Fields getComponentOutputFields(String componentId, String streamId) {
                return new Fields("id", "msg", "city", "state");
            }
        };
        return new TupleImpl(topologyContext, new Values(id, msg, city, state), "", 1, "");
    }

    private byte[] generateDelimiteredRecord(List<?> values, String fieldDelimiter) {
        StringBuilder builder = new StringBuilder();
        for (Object value : values) {
            builder.append(value);
            builder.append(fieldDelimiter);
        }
        return builder.toString().getBytes();
    }

    private static class TestingHiveBolt extends HiveBolt {

        protected Map<List<String>, List<byte[]>> partitionValuesToWrittenRecords = new HashMap<>();

        public TestingHiveBolt(HiveOptions options) {
            super(options);
        }

        @Override
        HiveWriter getOrCreateWriter(final HiveEndPoint endPoint)
            throws HiveWriter.ConnectFailure, InterruptedException {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                // always provide mocked HiveWriter
                writer = Mockito.mock(HiveWriter.class);
                try {
                    Mockito.doAnswer(new Answer<Void>() {
                        @Override
                        public Void answer(InvocationOnMock invocation) throws Throwable {
                            Object[] arguments = invocation.getArguments();
                            List<String> partitionVals = endPoint.partitionVals;
                            List<byte[]> writtenRecords = partitionValuesToWrittenRecords.get(partitionVals);
                            if (writtenRecords == null) {
                                writtenRecords = new ArrayList<>();
                                partitionValuesToWrittenRecords.put(partitionVals, writtenRecords);
                            }
                            writtenRecords.add((byte[]) arguments[0]);
                            return null;
                        }
                    }).when(writer).write(any(byte[].class));
                } catch (Exception exc) {
                    throw new RuntimeException(exc);
                }
            }
            return writer;
        }

        public Map<List<String>, List<byte[]>> getPartitionValuesToWrittenRecords() {
            return partitionValuesToWrittenRecords;
        }

        public List<byte[]> getRecordWritten(List<String> partitionValues) {
            return partitionValuesToWrittenRecords.get(partitionValues);
        }
    }

    private static class FlushFailureHiveBolt extends TestingHiveBolt {

        public FlushFailureHiveBolt(HiveOptions options) {
            super(options);
        }

        @Override
        void flushAllWriters(boolean rollToNext) throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure,
            InterruptedException {
            if (rollToNext) {
                throw new InterruptedException();
            } else {
                super.flushAllWriters(false);
            }
        }
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.common;

import com.google.common.util.concurrent.ThreadFactoryBuilder;

import java.io.File;
import java.io.IOException;
import java.util.Arrays;
import java.util.HashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.txn.TxnDbUtil;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.RecordWriter;
import org.apache.hive.hcatalog.streaming.SerializationError;
import org.apache.hive.hcatalog.streaming.StreamingConnection;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.hive.hcatalog.streaming.TransactionBatch;
import org.apache.storm.Config;
import org.apache.storm.hive.bolt.HiveSetupUtil;
import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
import org.apache.storm.hive.bolt.mapper.HiveMapper;
import org.apache.storm.task.GeneralTopologyContext;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.TupleImpl;
import org.apache.storm.tuple.Values;
import org.junit.jupiter.api.Test;
import org.mockito.Mockito;

import static org.junit.jupiter.api.Assertions.assertEquals;

public class TestHiveWriter {
    public static final String PART1_NAME = "city";
    public static final String PART2_NAME = "state";
    public static final String[] partNames = { PART1_NAME, PART2_NAME };
    final static String dbName = "testdb";
    final static String tblName = "test_table2";
    final String[] partitionVals = { "sunnyvale", "ca" };
    final String[] colNames = { "id", "msg" };
    private final int port;
    private final String metaStoreURI;
    private final HiveConf conf;
    int timeout = 10000; // msec
    UserGroupInformation ugi = null;
    private ExecutorService callTimeoutPool;

    public TestHiveWriter() throws Exception {
        port = 9083;
        metaStoreURI = null;
        int callTimeoutPoolSize = 1;
        callTimeoutPool = Executors.newFixedThreadPool(callTimeoutPoolSize,
                                                       new ThreadFactoryBuilder().setNameFormat("hiveWriterTest").build());

        // 1) Start metastore
        conf = HiveSetupUtil.getHiveConf();
        TxnDbUtil.setConfValues(conf);
        if (metaStoreURI != null) {
            conf.setVar(HiveConf.ConfVars.METASTOREURIS, metaStoreURI);
        }
    }

    @Test
    public void testInstantiate() throws Exception {
        DelimitedRecordHiveMapper mapper = new MockedDelemiteredRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
        TestingHiveWriter writer = new TestingHiveWriter(endPoint, 10, true, timeout
            , callTimeoutPool, mapper, ugi, false);
        writer.close();
    }

    @Test
    public void testWriteBasic() throws Exception {
        DelimitedRecordHiveMapper mapper = new MockedDelemiteredRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
        TestingHiveWriter writer = new TestingHiveWriter(endPoint, 10, true, timeout
            , callTimeoutPool, mapper, ugi, false);
        writeTuples(writer, mapper, 3);
        writer.flush(false);
        writer.close();
        Mockito.verify(writer.getMockedTxBatch(), Mockito.times(3)).write(Mockito.any(byte[].class));
    }

    @Test
    public void testWriteMultiFlush() throws Exception {
        DelimitedRecordHiveMapper mapper = new MockedDelemiteredRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));

        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
        TestingHiveWriter writer = new TestingHiveWriter(endPoint, 10, true, timeout
            , callTimeoutPool, mapper, ugi, false);
        Tuple tuple = generateTestTuple("1", "abc");
        writer.write(mapper.mapRecord(tuple));
        tuple = generateTestTuple("2", "def");
        writer.write(mapper.mapRecord(tuple));
        assertEquals(writer.getTotalRecords(), 2);
        Mockito.verify(writer.getMockedTxBatch(), Mockito.times(2)).write(Mockito.any(byte[].class));
        Mockito.verify(writer.getMockedTxBatch(), Mockito.never()).commit();
        writer.flush(true);
        assertEquals(writer.getTotalRecords(), 0);
        Mockito.verify(writer.getMockedTxBatch(), Mockito.atLeastOnce()).commit();

        tuple = generateTestTuple("3", "ghi");
        writer.write(mapper.mapRecord(tuple));
        writer.flush(true);

        tuple = generateTestTuple("4", "klm");
        writer.write(mapper.mapRecord(tuple));
        writer.flush(true);
        writer.close();
        Mockito.verify(writer.getMockedTxBatch(), Mockito.times(4)).write(Mockito.any(byte[].class));
    }

    private Tuple generateTestTuple(Object id, Object msg) {
        TopologyBuilder builder = new TopologyBuilder();
        GeneralTopologyContext topologyContext = new GeneralTopologyContext(builder.createTopology(),
                                                                            new Config(), new HashMap(), new HashMap(), new HashMap(), "") {
            @Override
            public Fields getComponentOutputFields(String componentId, String streamId) {
                return new Fields("id", "msg");
            }
        };
        return new TupleImpl(topologyContext, new Values(id, msg), "", 1, "");
    }

    private void writeTuples(HiveWriter writer, HiveMapper mapper, int count)
        throws HiveWriter.WriteFailure, InterruptedException, SerializationError {
        Integer id = 100;
        String msg = "test-123";
        for (int i = 1; i <= count; i++) {
            Tuple tuple = generateTestTuple(id, msg);
            writer.write(mapper.mapRecord(tuple));
        }
    }

    private static class TestingHiveWriter extends HiveWriter {

        private StreamingConnection mockedStreamingConn;
        private TransactionBatch mockedTxBatch;

        public TestingHiveWriter(HiveEndPoint endPoint, int txnsPerBatch, boolean autoCreatePartitions, long callTimeout,
                                 ExecutorService callTimeoutPool, HiveMapper mapper, UserGroupInformation ugi,
                                 boolean tokenAuthEnabled) throws InterruptedException, ConnectFailure {
            super(endPoint, txnsPerBatch, autoCreatePartitions, callTimeout, callTimeoutPool, mapper, ugi, tokenAuthEnabled);
        }

        @Override
        synchronized StreamingConnection newConnection(UserGroupInformation ugi, boolean tokenAuthEnabled) throws InterruptedException,
            ConnectFailure {
            if (mockedStreamingConn == null) {
                mockedStreamingConn = Mockito.mock(StreamingConnection.class);
                mockedTxBatch = Mockito.mock(TransactionBatch.class);

                try {
                    Mockito.when(mockedStreamingConn.fetchTransactionBatch(Mockito.anyInt(), Mockito.any(RecordWriter.class)))
                           .thenReturn(mockedTxBatch);
                } catch (StreamingException e) {
                    throw new RuntimeException(e);
                }
            }

            return mockedStreamingConn;
        }

        public TransactionBatch getMockedTxBatch() {
            return mockedTxBatch;
        }
    }

    private static class MockedDelemiteredRecordHiveMapper extends DelimitedRecordHiveMapper {
        private final RecordWriter mockedRecordWriter;

        public MockedDelemiteredRecordHiveMapper() {
            this.mockedRecordWriter = Mockito.mock(RecordWriter.class);
        }

        @Override
        public RecordWriter createRecordWriter(HiveEndPoint endPoint) throws StreamingException, IOException, ClassNotFoundException {
            return mockedRecordWriter;
        }

        public RecordWriter getMockedRecordWriter() {
            return mockedRecordWriter;
        }
    }

}
```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The changes in the provided diff file are related to the addition of a new utility class for setting up Hive in a test environment. The modifications do not directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The changes are confined to test utilities and do not impact the main application code or its security posture.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 29
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopology.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopology.java
new file mode 100644
index 000000000..e9ecbd06f
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopology.java
@@ -0,0 +1,150 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.Config;
+import backtype.storm.LocalCluster;
+import backtype.storm.StormSubmitter;
+import backtype.storm.spout.SpoutOutputCollector;
+import backtype.storm.task.TopologyContext;
+import backtype.storm.topology.OutputFieldsDeclarer;
+import backtype.storm.topology.TopologyBuilder;
+import backtype.storm.topology.base.BaseRichSpout;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Values;
+
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.common.HiveOptions;
+
+import java.util.Map;
+import java.util.UUID;
+import java.util.concurrent.ConcurrentHashMap;
+
+
+public class HiveTopology {
+    static final String USER_SPOUT_ID = "user-spout";
+    static final String BOLT_ID = "my-hive-bolt";
+    static final String TOPOLOGY_NAME = "hive-test-topology1";
+
+    public static void main(String[] args) throws Exception {
+        String metaStoreURI = args[0];
+        String dbName = args[1];
+        String tblName = args[2];
+        String[] colNames = {"id","name","phone","street","city","state"};
+        Config config = new Config();
+        config.setNumWorkers(1);
+        UserDataSpout spout = new UserDataSpout();
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames));
+        HiveOptions hiveOptions;
+        if (args.length == 6) {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(100)
+                .withIdleTimeout(10)
+                .withKerberosKeytab(args[4])
+                .withKerberosPrincipal(args[5]);
+        } else {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(100)
+                .withIdleTimeout(10);
+        }
+
+        HiveBolt hiveBolt = new HiveBolt(hiveOptions);
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.setSpout(USER_SPOUT_ID, spout, 1);
+        // SentenceSpout --> MyBolt
+        builder.setBolt(BOLT_ID, hiveBolt, 1)
+                .shuffleGrouping(USER_SPOUT_ID);
+        if (args.length == 3) {
+            LocalCluster cluster = new LocalCluster();
+            cluster.submitTopology(TOPOLOGY_NAME, config, builder.createTopology());
+            waitForSeconds(20);
+            cluster.killTopology(TOPOLOGY_NAME);
+            System.out.println("cluster begin to shutdown");
+            cluster.shutdown();
+            System.out.println("cluster shutdown");
+            System.exit(0);
+        } else if(args.length >= 4) {
+            StormSubmitter.submitTopology(args[3], config, builder.createTopology());
+        } else {
+            System.out.println("Usage: HiveTopology metastoreURI dbName tableName [topologyNamey] [keytab file] [principal name]");
+        }
+    }
+
+    public static void waitForSeconds(int seconds) {
+        try {
+            Thread.sleep(seconds * 1000);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    public static class UserDataSpout extends BaseRichSpout {
+        private ConcurrentHashMap<UUID, Values> pending;
+        private SpoutOutputCollector collector;
+        private String[] sentences = {
+                "1,user1,123456,street1,sunnyvale,ca",
+                "2,user2,123456,street2,sunnyvale,ca",
+                "3,user3,123456,street3,san jose,ca",
+                "4,user4,123456,street4,san jose,ca",
+        };
+        private int index = 0;
+        private int count = 0;
+        private long total = 0L;
+
+        public void declareOutputFields(OutputFieldsDeclarer declarer) {
+            declarer.declare(new Fields("id","name","phone","street","city","state"));
+        }
+
+        public void open(Map config, TopologyContext context,
+                         SpoutOutputCollector collector) {
+            this.collector = collector;
+            this.pending = new ConcurrentHashMap<UUID, Values>();
+        }
+
+        public void nextTuple() {
+            String[] user = sentences[index].split(",");
+            Values values = new Values(Integer.parseInt(user[0]),user[1],user[2],user[3],user[4],user[5]);
+            UUID msgId = UUID.randomUUID();
+            this.pending.put(msgId, values);
+            this.collector.emit(values, msgId);
+            index++;
+            if (index >= sentences.length) {
+                index = 0;
+            }
+            count++;
+            total++;
+            if(count > 1000){
+                count = 0;
+                System.out.println("Pending count: " + this.pending.size() + ", total: " + this.total);
+            }
+            Thread.yield();
+        }
+
+        public void ack(Object msgId) {
+            this.pending.remove(msgId);
+        }
+
+        public void fail(Object msgId) {
+            System.out.println("**** RESENDING FAILED TUPLE");
+            this.collector.emit(this.pending.get(msgId), msgId);
+        }
+    }
+}

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The provided diff file introduces a new test file for a Hive topology in the Apache Storm project. The changes are confined to adding a new test class and do not modify any existing authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Since the changes are isolated to a new test file and do not impact critical security components, re-running the security tests is not necessary.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 30
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopologyPartitioned.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopologyPartitioned.java
new file mode 100644
index 000000000..c3197c22f
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopologyPartitioned.java
@@ -0,0 +1,153 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.Config;
+import backtype.storm.LocalCluster;
+import backtype.storm.StormSubmitter;
+import backtype.storm.spout.SpoutOutputCollector;
+import backtype.storm.task.TopologyContext;
+import backtype.storm.topology.OutputFieldsDeclarer;
+import backtype.storm.topology.TopologyBuilder;
+import backtype.storm.topology.base.BaseRichSpout;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Values;
+import backtype.storm.utils.Utils;
+
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.common.HiveOptions;
+
+import java.util.Map;
+import java.util.UUID;
+import java.util.concurrent.ConcurrentHashMap;
+
+
+public class HiveTopologyPartitioned {
+    static final String USER_SPOUT_ID = "hive-user-spout-partitioned";
+    static final String BOLT_ID = "my-hive-bolt-partitioned";
+    static final String TOPOLOGY_NAME = "hive-test-topology-partitioned";
+
+    public static void main(String[] args) throws Exception {
+        String metaStoreURI = args[0];
+        String dbName = args[1];
+        String tblName = args[2];
+        String[] partNames = {"city","state"};
+        String[] colNames = {"id","name","phone","street"};
+        Config config = new Config();
+        config.setNumWorkers(1);
+        UserDataSpout spout = new UserDataSpout();
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions;
+        if (args.length == 6) {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(1000)
+                .withIdleTimeout(10)
+                .withKerberosKeytab(args[4])
+                .withKerberosPrincipal(args[5]);
+        } else {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(1000)
+                .withIdleTimeout(10);
+        }
+
+        HiveBolt hiveBolt = new HiveBolt(hiveOptions);
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.setSpout(USER_SPOUT_ID, spout, 1);
+        // SentenceSpout --> MyBolt
+        builder.setBolt(BOLT_ID, hiveBolt, 1)
+                .shuffleGrouping(USER_SPOUT_ID);
+        if (args.length == 3) {
+            LocalCluster cluster = new LocalCluster();
+            cluster.submitTopology(TOPOLOGY_NAME, config, builder.createTopology());
+            waitForSeconds(20);
+            cluster.killTopology(TOPOLOGY_NAME);
+            System.out.println("cluster begin to shutdown");
+            cluster.shutdown();
+            System.out.println("cluster shutdown");
+            System.exit(0);
+        } else if(args.length >= 4) {
+            StormSubmitter.submitTopology(args[3], config, builder.createTopology());
+        } else {
+            System.out.println("Usage: HiveTopologyPartitioned metastoreURI dbName tableName [topologyNamey] [keytab file] [principal name]");
+        }
+    }
+
+    public static void waitForSeconds(int seconds) {
+        try {
+            Thread.sleep(seconds * 1000);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    public static class UserDataSpout extends BaseRichSpout {
+        private ConcurrentHashMap<UUID, Values> pending;
+        private SpoutOutputCollector collector;
+        private String[] sentences = {
+                "1,user1,123456,street1,sunnyvale,ca",
+                "2,user2,123456,street2,sunnyvale,ca",
+                "3,user3,123456,street3,san jose,ca",
+                "4,user4,123456,street4,san jose,ca",
+        };
+        private int index = 0;
+        private int count = 0;
+        private long total = 0L;
+
+        public void declareOutputFields(OutputFieldsDeclarer declarer) {
+            declarer.declare(new Fields("id","name","phone","street","city","state"));
+        }
+
+        public void open(Map config, TopologyContext context,
+                         SpoutOutputCollector collector) {
+            this.collector = collector;
+            this.pending = new ConcurrentHashMap<UUID, Values>();
+        }
+
+        public void nextTuple() {
+            String[] user = sentences[index].split(",");
+            Values values = new Values(Integer.parseInt(user[0]),user[1],user[2],user[3],user[4],user[5]);
+            UUID msgId = UUID.randomUUID();
+            this.pending.put(msgId, values);
+            this.collector.emit(values, msgId);
+            index++;
+            if (index >= sentences.length) {
+                index = 0;
+            }
+            count++;
+            total++;
+            if(count > 1000){
+		Utils.sleep(1000);
+                count = 0;
+                System.out.println("Pending count: " + this.pending.size() + ", total: " + this.total);
+            }
+        }
+
+        public void ack(Object msgId) {
+            this.pending.remove(msgId);
+        }
+
+        public void fail(Object msgId) {
+            System.out.println("**** RESENDING FAILED TUPLE");
+            this.collector.emit(this.pending.get(msgId), msgId);
+        }
+    }
+}

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The provided diff file introduces a new test file for a Hive topology in Apache Storm. The changes are related to adding a new test class and do not modify any existing critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Since the changes are confined to a new test file and do not impact the core security functionalities, re-running the security tests is not necessary.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 33
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java b/external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java
new file mode 100644
index 000000000..bc607f3f9
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java
@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.trident;
+
+
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.common.HiveOptions;
+
+import backtype.storm.Config;
+import backtype.storm.LocalCluster;
+import backtype.storm.StormSubmitter;
+import backtype.storm.generated.StormTopology;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Values;
+import backtype.storm.task.TopologyContext;
+import storm.trident.operation.TridentCollector;
+import storm.trident.spout.IBatchSpout;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import storm.trident.Stream;
+import storm.trident.TridentState;
+import storm.trident.TridentTopology;
+import storm.trident.state.StateFactory;
+
+
+public class TridentHiveTopology {
+    public static StormTopology buildTopology(String metaStoreURI, String dbName, String tblName, Object keytab, Object principal) {
+        int batchSize = 100;
+        FixedBatchSpout spout = new FixedBatchSpout(batchSize);
+        spout.setCycle(true);
+        TridentTopology topology = new TridentTopology();
+        Stream stream = topology.newStream("hiveTridentspout1",spout);
+        String[] partNames = {"city","state"};
+        String[] colNames = {"id","name","phone","street"};
+        Fields hiveFields = new Fields("id","name","phone","street","city","state");
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions;
+        if (keytab != null && principal != null) {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(batchSize)
+                .withIdleTimeout(10)
+                .withCallTimeout(30000)
+                .withKerberosKeytab((String)keytab)
+                .withKerberosPrincipal((String)principal);
+        } else  {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(batchSize)
+                .withCallTimeout(30000)
+                .withIdleTimeout(10);
+        }
+        StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
+        TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());
+        return topology.build();
+    }
+
+    public static void waitForSeconds(int seconds) {
+        try {
+            Thread.sleep(seconds * 1000);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    public static void main(String[] args) {
+        String metaStoreURI = args[0];
+        String dbName = args[1];
+        String tblName = args[2];
+        Config conf = new Config();
+        conf.setMaxSpoutPending(5);
+        if(args.length == 3) {
+            LocalCluster cluster = new LocalCluster();
+            cluster.submitTopology("tridentHiveTopology", conf, buildTopology(metaStoreURI, dbName, tblName,null,null));
+            System.out.println("waiting for 60 seconds");
+            waitForSeconds(60);
+            System.out.println("killing topology");
+            cluster.killTopology("tridenHiveTopology");
+            System.out.println("cluster shutdown");
+            cluster.shutdown();
+            System.out.println("cluster shutdown");
+            System.exit(0);
+        } else if(args.length == 4) {
+            try {
+                StormSubmitter.submitTopology(args[3], conf, buildTopology(metaStoreURI, dbName, tblName,null,null));
+            } catch(Exception e) {
+                System.out.println("Failed to submit topology "+e);
+            }
+        } else if (args.length == 6) {
+            try {
+                StormSubmitter.submitTopology(args[3], conf, buildTopology(metaStoreURI, dbName, tblName,args[4],args[5]));
+            } catch(Exception e) {
+                System.out.println("Failed to submit topology "+e);
+            }
+        } else {
+            System.out.println("Usage: TridentHiveTopology metastoreURI dbName tableName [topologyNamey]");
+        }
+    }
+
+    public static class FixedBatchSpout implements IBatchSpout {
+        int maxBatchSize;
+        HashMap<Long, List<List<Object>>> batches = new HashMap<Long, List<List<Object>>>();
+        private Values[] outputs = {
+            new Values("1","user1","123456","street1","sunnyvale","ca"),
+            new Values("2","user2","123456","street2","sunnyvale","ca"),
+            new Values("3","user3","123456","street3","san jose","ca"),
+            new Values("4","user4","123456","street4","san jose","ca"),
+        };
+        private int index = 0;
+        boolean cycle = false;
+
+        public FixedBatchSpout(int maxBatchSize) {
+            this.maxBatchSize = maxBatchSize;
+        }
+
+        public void setCycle(boolean cycle) {
+            this.cycle = cycle;
+        }
+
+        @Override
+        public Fields getOutputFields() {
+            return new Fields("id","name","phone","street","city","state");
+        }
+
+        @Override
+        public void open(Map conf, TopologyContext context) {
+            index = 0;
+        }
+
+        @Override
+        public void emitBatch(long batchId, TridentCollector collector) {
+            List<List<Object>> batch = this.batches.get(batchId);
+            if(batch == null){
+                batch = new ArrayList<List<Object>>();
+                if(index>=outputs.length && cycle) {
+                    index = 0;
+                }
+                for(int i=0; i < maxBatchSize; index++, i++) {
+                    if(index == outputs.length){
+                        index=0;
+                    }
+                    batch.add(outputs[index]);
+                }
+                this.batches.put(batchId, batch);
+            }
+            for(List<Object> list : batch){
+                collector.emit(list);
+            }
+        }
+
+        @Override
+        public void ack(long batchId) {
+            this.batches.remove(batchId);
+        }
+
+        @Override
+        public void close() {
+        }
+
+        @Override
+        public Map getComponentConfiguration() {
+            Config conf = new Config();
+            conf.setMaxTaskParallelism(1);
+            return conf;
+        }
+
+    }
+
+}

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The provided diff file introduces a new test file for the TridentHiveTopology class. The changes are related to setting up a topology for testing purposes and do not modify any existing authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Since the changes are confined to a new test file and do not impact critical security components, re-running the security tests is not necessary.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 34
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java b/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java
index cc723c332..7bac4c8d7 100644
--- a/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java
+++ b/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java
@@ -24,10 +24,11 @@ import java.sql.Timestamp;
 import java.sql.Types;
 
 public class Util {
-    public static String getSqlTypeName(int sqlType) {
+    private static String getSqlTypeName(int sqlType) {
         try {
+            Integer val = new Integer(sqlType);
             for (Field field : Types.class.getFields()) {
-                if (sqlType == field.get(null)) {
+                if (val.equals(field.get(null))) {
                     return field.getName();
                 }
             }

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The changes in the diff file are related to the visibility of the method `getSqlTypeName` and a minor modification in the comparison logic. The method was changed from public to private, which reduces its accessibility and can be seen as a minor security improvement. The comparison logic was updated to use `Integer.equals` instead of a direct comparison, which is a minor code quality improvement. These changes do not directly impact critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 36
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
index b6cccadd2..ecb0daf6a 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
@@ -141,7 +141,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
                     new PrivilegedExceptionAction<Void>() {
                 public Void run() {
                     try {
-                        LOG.info("do as:"+ principal);
+                        LOG.debug("do as:"+ principal);
                         sasalTransport.open();
                     }
                     catch (Exception e) {

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The only change in the diff file is the logging level from INFO to DEBUG for a specific log message. This change does not directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, re-running the security tests is not necessary.",
  "confidence": 10
}
``` -> security_relevancy: not

Diff number: 37
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/Config.java b/storm-core/src/jvm/backtype/storm/Config.java
index 1237c28f2..1655cf6ca 100644
--- a/storm-core/src/jvm/backtype/storm/Config.java
+++ b/storm-core/src/jvm/backtype/storm/Config.java
@@ -441,6 +441,14 @@ public class Config extends HashMap<String, Object> {
     public static final String NIMBUS_AUTHORIZER = "nimbus.authorizer";
     public static final Object NIMBUS_AUTHORIZER_SCHEMA = String.class;
 
+
+    /**
+     * Impersonation user ACL config entries.
+     */
+    public static final String NIMBUS_IMPERSONATION_AUTHORIZER = "nimbus.impersonation.authorizer";
+    public static final Object NIMBUS_IMPERSONATION_AUTHORIZER_SCHEMA = String.class;
+
+
     /**
      * Impersonation user ACL config entries.
      */

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes introduce a new configuration parameter 'NIMBUS_IMPERSONATION_AUTHORIZER' and its schema. This parameter is related to authorization, which is a critical security component. Any changes or additions to authorization mechanisms can have significant security implications, and thus warrant re-running the security tests to ensure that the new configuration does not introduce vulnerabilities or misconfigurations.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 38
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
index 1e947ae67..d6431be56 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
@@ -13,6 +13,7 @@ import java.util.*;
 
 public class ImpersonationAuthorizer implements IAuthorizer {
     private static final Logger LOG = LoggerFactory.getLogger(ImpersonationAuthorizer.class);
+    protected static final String WILD_CARD = "*";
 
     protected Map<String, ImpersonationACL> userImpersonationACL;
     protected IPrincipalToLocal _ptol;
@@ -97,12 +98,17 @@ public class ImpersonationAuthorizer implements IAuthorizer {
     }
 
     private boolean isAllowedToImpersonateFromHost(Set<String> authorizedHosts, InetAddress remoteAddress) {
-        return authorizedHosts.contains(remoteAddress.getCanonicalHostName()) ||
+        return authorizedHosts.contains(WILD_CARD) ||
+                authorizedHosts.contains(remoteAddress.getCanonicalHostName()) ||
                 authorizedHosts.contains(remoteAddress.getHostName()) ||
                 authorizedHosts.contains(remoteAddress.getHostAddress());
     }
 
     private boolean isAllowedToImpersonateUser(Set<String> authorizedGroups, String userBeingImpersonated) {
+        if(authorizedGroups.contains(WILD_CARD)) {
+            return true;
+        }
+
         Set<String> groups = null;
         try {
             groups = _groupMappingProvider.getGroups(userBeingImpersonated);

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file directly affect the access control procedures within the ImpersonationAuthorizer class. Specifically, the introduction of the WILD_CARD constant and its usage in the methods isAllowedToImpersonateFromHost and isAllowedToImpersonateUser could potentially allow broader access than intended. This modification impacts the security of the system by altering the conditions under which impersonation is authorized, which is a critical security component.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 39
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
index 25114b959..888eefb4f 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
@@ -50,7 +50,6 @@ public class SimpleACLAuthorizer implements IAuthorizer {
     protected Set<String> _supervisors;
     protected IPrincipalToLocal _ptol;
     protected IGroupMappingServiceProvider _groupMappingProvider;
-    protected ImpersonationAuthorizer _impersonationAuthorizer;
     /**
      * Invoked once immediately after construction
      * @param conf Storm configuration
@@ -69,8 +68,6 @@ public class SimpleACLAuthorizer implements IAuthorizer {
 
         _ptol = AuthUtils.GetPrincipalToLocalPlugin(conf);
         _groupMappingProvider = AuthUtils.GetGroupMappingServiceProviderPlugin(conf);
-        _impersonationAuthorizer = new ImpersonationAuthorizer();
-        _impersonationAuthorizer.prepare(conf);
     }
 
     /**
@@ -91,10 +88,6 @@ public class SimpleACLAuthorizer implements IAuthorizer {
         String principal = context.principal().getName();
         String user = _ptol.toLocal(context.principal());
 
-        if(!_impersonationAuthorizer.permit(context, operation, topology_conf)) {
-            return false;
-        }
-
         if (_admins.contains(principal) || _admins.contains(user)) {
             return true;
         }

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The diff file shows the removal of the ImpersonationAuthorizer component, which is a critical part of the authorization mechanism. This component was responsible for checking if an impersonation operation is permitted. Removing this check could potentially allow unauthorized impersonation, which is a significant security risk. Therefore, it is necessary to re-run the security tests to ensure that the system's security is not compromised by these changes.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 40
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
index 0ff3d35bd..1788dab08 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
@@ -112,7 +112,7 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private void handleAuthorizeCallback(AuthorizeCallback ac) {
         String authenticationID = ac.getAuthenticationID();
-        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID + " authorizationID= " + ac.getAuthorizationID());
+        LOG.info("Successfully authenticated client: authenticationID = " + authenticationID + " authorizationID = " + ac.getAuthorizationID());
 
         //if authorizationId is not set, set it to authenticationId.
         if(ac.getAuthorizationID() == null) {
@@ -122,6 +122,7 @@ public class ServerCallbackHandler implements CallbackHandler {
         //When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We
         //add the authNid as the real user in reqContext's subject which will be used during authorization.
         if(!authenticationID.equals(ac.getAuthorizationID())) {
+            LOG.info("Impersonation attempt  authenticationID = " + ac.getAuthenticationID() + " authorizationID = " + ac.getAuthorizationID());
             ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
         }
 

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The changes in the diff file are primarily related to logging messages. The modifications include adding spaces in log messages and adding a new log statement for impersonation attempts. These changes do not directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, re-running the security tests is not necessary.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 41
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
index 86455580a..59c5b065d 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
@@ -72,16 +72,24 @@ public class DefaultHttpCredentialsPlugin implements IHttpCredentialsPlugin {
     public ReqContext populateContext(ReqContext context,
             HttpServletRequest req) {
         String userName = getUserName(req);
-        Principal p = null;
-        if (userName != null) {
-            p = new SingleUserPrincipal(userName);
+
+        String doAsUser = req.getHeader("doAsUser");
+        if(doAsUser == null) {
+            doAsUser = req.getParameter("doAsUser");
         }
-        Set<Principal> principals = new HashSet<Principal>(1);
-        if (p != null) {
-            principals.add(p);
+
+        if(doAsUser != null) {
+            context.setRealPrincipal(new SingleUserPrincipal(userName));
+            userName = doAsUser;
         }
-        Subject s = new Subject(true, principals, new HashSet(), new HashSet());
-        context.setSubject(s);
+
+        if(userName != null) {
+            Subject s = new Subject();
+            Principal p = new SingleUserPrincipal(userName);
+            s.getPrincipals().add(p);
+            context.setSubject(s);
+        }
+
         return context;
     }
 }

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file directly affect the authentication mechanism by modifying how user principals are set and how the 'doAsUser' parameter is handled. These changes could have significant security implications, such as altering the way user identities are verified and potentially introducing vulnerabilities related to user impersonation. Given the critical nature of these modifications, it is essential to re-run the security tests to ensure that the new implementation does not introduce any security flaws.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 42
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/StormSubmitter.java b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
index 63cc5a146..a4ccf5f87 100644
--- a/storm-core/src/jvm/backtype/storm/StormSubmitter.java
+++ b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
@@ -268,7 +268,7 @@ public class StormSubmitter {
     @SuppressWarnings("unchecked")
     public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
              ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
-
+        submitTopologyAs(name, stormConf, topology, opts, progressListener, null);
     }
 
     /**

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "no",
  "reason": "The change in the diff file involves a modification to the method call within the submitTopology function, specifically adding a call to submitTopologyAs with an additional null parameter. This change does not directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, re-running the security tests is not necessary.",
  "confidence": 9
}
``` -> security_relevancy: not

Diff number: 43
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java b/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
index b6c3de487..cf169dc40 100644
--- a/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
+++ b/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
@@ -101,11 +101,10 @@ public class KafkaBolt<K, V> extends BaseRichBolt {
             } else {
                 LOG.warn("skipping key = " + key + ", topic selector returned null.");
             }
-        } catch (Exception ex) {
-            LOG.error("Could not send message with key = " + key
-                    + " and value = " + message + " to topic = " + topic, ex);
-        } finally {
             collector.ack(input);
+        } catch (Exception ex) {
+            collector.reportError(ex);
+            collector.fail(input);
         }
     }
 

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file modify the error handling mechanism in the KafkaBolt class. Specifically, the code now calls `collector.reportError(ex)` and `collector.fail(input)` instead of just logging the error. This change affects how errors are reported and handled, which can have implications for system security, particularly in terms of logging and error management. Ensuring that errors are properly reported and handled is crucial for maintaining the integrity and security of the system.",
  "confidence": 9
}
``` -> security_relevancy: security

Diff number: 45
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus is on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
The provided diff file below was previously run for such security testing.
Based on the changes in this diff file, concentrating solely on the lines that start with '+' or '-', is it worth re-running the security testing on this same file?
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"worth_to_re_run": {"description": "A string whose value is one of the following: ['yes' (if re-running the security tests on the given diff file is necessary), 'no' (if re-running the security tests on the given diff file is not worth it)].", "title": "Worth To Re Run", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your answer. If re-running is not worth it, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["worth_to_re_run", "reason", "confidence"]}
```
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this extra information for your evaluation.
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
index 59c5b065d..e2469e5d9 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
@@ -83,12 +83,13 @@ public class DefaultHttpCredentialsPlugin implements IHttpCredentialsPlugin {
             userName = doAsUser;
         }
 
+        Set<Principal> principals = new HashSet<Principal>();
         if(userName != null) {
-            Subject s = new Subject();
             Principal p = new SingleUserPrincipal(userName);
-            s.getPrincipals().add(p);
-            context.setSubject(s);
+            principals.add(p);
         }
+        Subject s = new Subject(true, principals, new HashSet(), new HashSet());
+        context.setSubject(s);
 
         return context;
     }

```
Context:
```

```
Important that testing is a costly operation.
Determine the answer considering the immediate implications of the changes on system security, especially for modifications to critical components.

Output: ```json
{
  "worth_to_re_run": "yes",
  "reason": "The changes in the diff file directly affect the authentication mechanism by modifying how the Subject is created and set in the security context. Specifically, the creation of the Subject now includes a set of principals, which could impact the authentication process and the security context. Given that authentication is a critical security component, it is essential to re-run the security tests to ensure that these changes do not introduce any vulnerabilities or issues.",
  "confidence": 9
}
``` -> security_relevancy: security

Number of diff files: 45
Number of processed files: 37
Statistics: [{'security': 22, 'mean': 9}, {'not': 15, 'mean': 9}]
Error number: 8
Accuracy: 0.8648648648648649
Precision: 0.8636363636
Recall: 0.9047619048
F1 Score: 0.8837209302