File: storm, pull request #448
Model: gpt-4-0125-preview

Diff number: 1
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/Config.java b/storm-core/src/jvm/backtype/storm/Config.java
index 73a1976ff..1237c28f2 100644
--- a/storm-core/src/jvm/backtype/storm/Config.java
+++ b/storm-core/src/jvm/backtype/storm/Config.java
@@ -441,6 +441,12 @@ public class Config extends HashMap<String, Object> {
     public static final String NIMBUS_AUTHORIZER = "nimbus.authorizer";
     public static final Object NIMBUS_AUTHORIZER_SCHEMA = String.class;
 
+    /**
+     * Impersonation user ACL config entries.
+     */
+    public static final String NIMBUS_IMPERSONATION_ACL = "nimbus.impersonation.acl";
+    public static final Object NIMBUS_IMPERSONATION_ACL_SCHEMA = ConfigValidation.MapOfStringToMapValidator;
+
     /**
      * How often nimbus should wake up to renew credentials if needed.
      */

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The addition of the 'NIMBUS_IMPERSONATION_ACL' configuration parameter directly impacts the security functionalities of the system. Impersonation Access Control Lists (ACLs) are crucial for defining and enforcing who can impersonate other users within a system. This change introduces or modifies the mechanism by which users are authorized to perform actions on behalf of others, which is a critical aspect of secure authentication and authorization processes. By controlling impersonation, the system can prevent unauthorized access and ensure that only legitimate users can perform actions on behalf of others, thereby maintaining the integrity and confidentiality of user data and actions. The introduction of a validator ('ConfigValidation.MapOfStringToMapValidator') for this ACL indicates that there is an enforcement mechanism in place to validate the configurations, which is essential for maintaining the security posture of the system. This change could potentially enhance the system's security by providing a more granular control over impersonation rights, assuming it is properly implemented and used in conjunction with secure authentication and authorization practices.",
  "confidence": 9
}
```

Diff number: 2
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/ConfigValidation.java b/storm-core/src/jvm/backtype/storm/ConfigValidation.java
index 14394a08b..24991d783 100644
--- a/storm-core/src/jvm/backtype/storm/ConfigValidation.java
+++ b/storm-core/src/jvm/backtype/storm/ConfigValidation.java
@@ -168,12 +168,18 @@ public class ConfigValidation {
      * Validates a list of Strings.
      */
     public static Object StringsValidator = listFv(String.class, true);
-    
+
     /**
      * Validates a map of Strings to Numbers.
      */
     public static Object MapOfStringToNumberValidator = mapFv(String.class, Number.class, true);
 
+    /**
+     * Validates a map of Strings to a map of Strings to a list.
+     * {str -> {str -> [str,str]}
+     */
+    public static Object MapOfStringToMapValidator = mapFv(fv(String.class, false), mapFv(fv(String.class, false), listFv(String.class, false), false), true);
+
     /**
      * Validates is a list of Maps.
      */

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file shows modifications in the ConfigValidation.java file of the storm-core project. The changes involve the addition of a new validator (`MapOfStringToMapValidator`) that validates a map of strings to a map of strings to a list. This addition is primarily related to data structure validation and does not directly impact critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The modifications are focused on enhancing the validation capabilities of the system's configuration parameters, which, while important for overall system integrity and reliability, do not inherently alter the security posture of the system. Therefore, these changes are classified as not directly relevant to security functionalities.",
  "confidence": 9
}
```

Diff number: 3
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/StormSubmitter.java b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
index 54cc70100..63cc5a146 100644
--- a/storm-core/src/jvm/backtype/storm/StormSubmitter.java
+++ b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
@@ -175,22 +175,19 @@ public class StormSubmitter {
     }
 
     /**
-     * Submits a topology to run on the cluster. A topology runs forever or until
-     * explicitly killed.
      *
-     *
-     * @param name the name of the storm.
-     * @param stormConf the topology-specific configuration. See {@link Config}.
-     * @param topology the processing to execute.
-     * @param opts to manipulate the starting of the topology
-     * @param progressListener to track the progress of the jar upload process
-     * @throws AlreadyAliveException if a topology with this name is already running
-     * @throws InvalidTopologyException if an invalid topology was submitted
-     * @throws AuthorizationException if authorization is failed
+     * @param name
+     * @param stormConf
+     * @param topology
+     * @param opts
+     * @param progressListener
+     * @param asUser The user as which this topology should be submitted.
+     * @throws AlreadyAliveException
+     * @throws InvalidTopologyException
+     * @throws AuthorizationException
      */
-    @SuppressWarnings("unchecked")
-    public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
-             ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
+    public static void submitTopologyAs(String name, Map stormConf, StormTopology topology, SubmitOptions opts, ProgressListener progressListener, String asUser)
+            throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
         if(!Utils.isValidConf(stormConf)) {
             throw new IllegalArgumentException("Storm conf is not valid. Must be json-serializable");
         }
@@ -218,25 +215,25 @@ public class StormSubmitter {
             if(localNimbus!=null) {
                 LOG.info("Submitting topology " + name + " in local mode");
                 if(opts!=null) {
-                    localNimbus.submitTopologyWithOpts(name, stormConf, topology, opts);                    
+                    localNimbus.submitTopologyWithOpts(name, stormConf, topology, opts);
                 } else {
                     // this is for backwards compatibility
-                    localNimbus.submitTopology(name, stormConf, topology);                                            
+                    localNimbus.submitTopology(name, stormConf, topology);
                 }
             } else {
                 String serConf = JSONValue.toJSONString(stormConf);
-                NimbusClient client = NimbusClient.getConfiguredClient(conf);
-                if(topologyNameExists(conf, name)) {
+                NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser);
+                if(topologyNameExists(conf, name, asUser)) {
                     throw new RuntimeException("Topology with name `" + name + "` already exists on cluster");
                 }
-                String jar = submitJar(conf, progressListener);
+                String jar = submitJarAs(conf, System.getProperty("storm.jar"), progressListener, asUser);
                 try {
                     LOG.info("Submitting topology " +  name + " in distributed mode with conf " + serConf);
                     if(opts!=null) {
-                        client.getClient().submitTopologyWithOpts(name, jar, serConf, topology, opts);                    
+                        client.getClient().submitTopologyWithOpts(name, jar, serConf, topology, opts);
                     } else {
                         // this is for backwards compatibility
-                        client.getClient().submitTopology(name, jar, serConf, topology);                                            
+                        client.getClient().submitTopology(name, jar, serConf, topology);
                     }
                 } catch(InvalidTopologyException e) {
                     LOG.warn("Topology submission exception: "+e.get_msg());
@@ -254,6 +251,26 @@ public class StormSubmitter {
         }
     }
 
+    /**
+     * Submits a topology to run on the cluster. A topology runs forever or until
+     * explicitly killed.
+     *
+     *
+     * @param name the name of the storm.
+     * @param stormConf the topology-specific configuration. See {@link Config}.
+     * @param topology the processing to execute.
+     * @param opts to manipulate the starting of the topology
+     * @param progressListener to track the progress of the jar upload process
+     * @throws AlreadyAliveException if a topology with this name is already running
+     * @throws InvalidTopologyException if an invalid topology was submitted
+     * @throws AuthorizationException if authorization is failed
+     */
+    @SuppressWarnings("unchecked")
+    public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
+             ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
+
+    }
+
     /**
      * Submits a topology to run on the cluster with a progress bar. A topology runs forever or until
      * explicitly killed.
@@ -310,8 +327,8 @@ public class StormSubmitter {
         });
     }
 
-    private static boolean topologyNameExists(Map conf, String name) {
-        NimbusClient client = NimbusClient.getConfiguredClient(conf);
+    private static boolean topologyNameExists(Map conf, String name, String asUser) {
+        NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser);
         try {
             ClusterSummary summary = client.getClient().getClusterInfo();
             for(TopologySummary s : summary.get_topologies()) {
@@ -342,19 +359,13 @@ public class StormSubmitter {
         return submitJar(conf, localJar, null);
     }
 
-    /**
-     * Submit jar file
-     * @param conf the topology-specific configuration. See {@link Config}.
-     * @param localJar file path of the jar file to submit
-     * @param listener progress listener to track the jar file upload
-     * @return the remote location of the submitted jar
-     */
-    public static String submitJar(Map conf, String localJar, ProgressListener listener) {
+
+    public static String submitJarAs(Map conf, String localJar, ProgressListener listener, String asUser) {
         if (localJar == null) {
             throw new RuntimeException("Must submit topologies using the 'storm' client script so that StormSubmitter knows which jar to upload.");
         }
 
-        NimbusClient client = NimbusClient.getConfiguredClient(conf);
+        NimbusClient client = NimbusClient.getConfiguredClientAs(conf, asUser);
         try {
             String uploadLocation = client.getClient().beginFileUpload();
             LOG.info("Uploading topology jar " + localJar + " to assigned location: " + uploadLocation);
@@ -385,12 +396,23 @@ public class StormSubmitter {
             LOG.info("Successfully uploaded topology jar to assigned location: " + uploadLocation);
             return uploadLocation;
         } catch(Exception e) {
-            throw new RuntimeException(e);            
+            throw new RuntimeException(e);
         } finally {
             client.close();
         }
     }
 
+    /**
+     * Submit jar file
+     * @param conf the topology-specific configuration. See {@link Config}.
+     * @param localJar file path of the jar file to submit
+     * @param listener progress listener to track the jar file upload
+     * @return the remote location of the submitted jar
+     */
+    public static String submitJar(Map conf, String localJar, ProgressListener listener) {
+        return submitJarAs(conf,localJar, listener, null);
+    }
+
     /**
      * Interface use to track progress of file upload
      */

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.command;

import java.util.Arrays;
import java.util.Map;

import org.apache.commons.lang.ArrayUtils;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.NimbusSummary;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.ServerUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class ShellSubmission {
    private static final Logger LOG = LoggerFactory.getLogger(ShellSubmission.class);

    public static void main(String[] args) throws Exception {
        if (args.length <= 1) {
            LOG.error("Arguments should be of the form: <path_to_jar> [argument...]");
            System.exit(-1);
        }
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        try (NimbusClient client = NimbusClient.getConfiguredClient(conf)) {
            NimbusSummary ns = client.getClient().getLeader();
            String host = ns.get_host();
            int port = ns.get_port();
            String jarPath = StormSubmitter.submitJar(conf, args[0]);
            String[] newArgs = (String[]) ArrayUtils.addAll(Arrays.copyOfRange(args, 1, args.length),
                    new String[]{host, String.valueOf(port), jarPath});
            ServerUtils.execCommand(newArgs);
        }
    }
}/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.storm;

import java.util.Map;
import org.apache.storm.generated.InvalidTopologyException;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.generated.SubmitOptions;
import org.apache.storm.generated.TopologyInitialStatus;
import org.apache.storm.testing.TestWordCounter;
import org.apache.storm.topology.TopologyBuilder;
import org.junit.jupiter.api.Test;

import static org.junit.jupiter.api.Assertions.fail;

public class TestStormSubmitter {

    @Test
    public void invalidTopologyWithoutSpout() {
        String expectedExceptionMsgFragment = "does not have any spout";
        TopologyBuilder tb = new TopologyBuilder();
        tb.setBolt("bolt1", new TestWordCounter(), 10).shuffleGrouping("spout1");
        tb.setBolt("bolt11", new TestWordCounter(), 10).shuffleGrouping("bolt1");
        tb.setBolt("bolt12", new TestWordCounter(), 10).shuffleGrouping("bolt1");
        StormTopology topology = tb.createTopology();
        Map<String, Object> topoConf = null;
        SubmitOptions opts = new SubmitOptions(TopologyInitialStatus.INACTIVE);

        try {
            StormSubmitter.submitTopologyAs("test-topo-without-spout", topoConf, topology, opts, null, "none");
            fail("Topology without spout should fail in submission");
        } catch (InvalidTopologyException ex) {
            if (!ex.getMessage().contains(expectedExceptionMsgFragment)) {
                String err = String.format("Topology submit failure should contain string \"%s\", but is \"%s\"",
                        expectedExceptionMsgFragment, ex.getMessage());
                fail(err);
            }
        } catch (Throwable ex) {
            ex.printStackTrace();
            fail("Unexpected exception submitting topology without spout: " + ex);
        }
    }
}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.sql;

import java.io.BufferedOutputStream;
import java.io.ByteArrayOutputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.List;
import java.util.Map;
import java.util.jar.Attributes;
import java.util.jar.JarOutputStream;
import java.util.jar.Manifest;
import java.util.zip.ZipEntry;
import org.apache.calcite.sql.SqlNode;
import org.apache.storm.StormSubmitter;
import org.apache.storm.generated.StormTopology;
import org.apache.storm.generated.SubmitOptions;
import org.apache.storm.sql.javac.CompilingClassLoader;
import org.apache.storm.sql.parser.SqlCreateFunction;
import org.apache.storm.sql.parser.SqlCreateTable;
import org.apache.storm.sql.parser.StormParser;

class StormSqlImpl extends StormSql {
    private final StormSqlContext sqlContext;

    StormSqlImpl() {
        sqlContext = new StormSqlContext();
    }

    @Override
    public void submit(
        String name, Iterable<String> statements, Map<String, Object> topoConf, SubmitOptions opts,
        StormSubmitter.ProgressListener progressListener, String asUser)
        throws Exception {
        for (String sql : statements) {
            StormParser parser = new StormParser(sql);
            SqlNode node = parser.impl().parseSqlStmtEof();
            if (node instanceof SqlCreateTable) {
                sqlContext.interpretCreateTable((SqlCreateTable) node);
            } else if (node instanceof SqlCreateFunction) {
                sqlContext.interpretCreateFunction((SqlCreateFunction) node);
            } else {
                AbstractStreamsProcessor processor = sqlContext.compileSql(sql);
                StormTopology topo = processor.build();

                Path jarPath = null;
                try {
                    // QueryPlanner on Streams mode configures the topology with compiled classes,
                    // so we need to add new classes into topology jar
                    // Topology will be serialized and sent to Nimbus, and deserialized and executed in workers.

                    jarPath = Files.createTempFile("storm-sql", ".jar");
                    System.setProperty("storm.jar", jarPath.toString());
                    packageTopology(jarPath, processor);
                    StormSubmitter.submitTopologyAs(name, topoConf, topo, opts, progressListener, asUser);
                } finally {
                    if (jarPath != null) {
                        Files.delete(jarPath);
                    }
                }
            }
        }
    }

    @Override
    public void explain(Iterable<String> statements) throws Exception {
        for (String sql : statements) {
            System.out.println("===========================================================");
            System.out.println("query>");
            System.out.println(sql);
            System.out.println("-----------------------------------------------------------");

            StormParser parser = new StormParser(sql);
            SqlNode node = parser.impl().parseSqlStmtEof();
            if (node instanceof SqlCreateTable) {
                sqlContext.interpretCreateTable((SqlCreateTable) node);
                System.out.println("No plan presented on DDL");
            } else if (node instanceof SqlCreateFunction) {
                sqlContext.interpretCreateFunction((SqlCreateFunction) node);
                System.out.println("No plan presented on DDL");
            } else {
                String plan = sqlContext.explain(sql);
                System.out.println("plan>");
                System.out.println(plan);
            }

            System.out.println("===========================================================");
        }
    }

    private void packageTopology(Path jar, AbstractStreamsProcessor processor) throws IOException {
        Manifest manifest = new Manifest();
        Attributes attr = manifest.getMainAttributes();
        attr.put(Attributes.Name.MANIFEST_VERSION, "1.0");
        attr.put(Attributes.Name.MAIN_CLASS, processor.getClass().getCanonicalName());
        try (JarOutputStream out = new JarOutputStream(
            new BufferedOutputStream(new FileOutputStream(jar.toFile())), manifest)) {
            List<CompilingClassLoader> classLoaders = processor.getClassLoaders();
            if (classLoaders != null && !classLoaders.isEmpty()) {
                for (CompilingClassLoader classLoader : classLoaders) {
                    for (Map.Entry<String, ByteArrayOutputStream> e : classLoader.getClasses().entrySet()) {
                        out.putNextEntry(new ZipEntry(e.getKey().replace(".", "/") + ".class"));
                        out.write(e.getValue().toByteArray());
                        out.closeEntry();
                    }
                }
            }
        }
    }
}

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes introduced in the diff file are directly related to security functionalities, specifically in the context of authentication and authorization mechanisms. The addition of the 'asUser' parameter in methods such as 'submitTopologyAs', 'submitJarAs', and 'topologyNameExists' indicates a shift towards executing actions on behalf of different users, rather than executing all actions under a single system or user context. This change can have significant security implications, as it introduces the possibility of performing actions with potentially elevated or different privileges than the current user. If not properly validated and controlled, this could lead to privilege escalation attacks where a less privileged user could execute actions as a more privileged one. Furthermore, the modification in how the NimbusClient is configured ('NimbusClient.getConfiguredClientAs') to use this 'asUser' parameter suggests changes in how authentication and session management are handled, potentially affecting the security posture by introducing new paths for authentication and authorization that must be securely managed. Additionally, the changes in how jars are submitted ('submitJarAs') with the 'asUser' parameter could impact the integrity and confidentiality of the code being executed, as it might allow users to submit and execute code on behalf of others without proper authorization checks. Overall, these changes necessitate a thorough security review to ensure that proper authentication, authorization, and session management controls are in place and that the principle of least privilege is adhered to.",
  "confidence": 9
}
```

Diff number: 5
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java b/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java
index 4033f1870..a252f85b8 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/ReqContext.java
@@ -22,6 +22,9 @@ import java.util.Set;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.net.InetAddress;
 import com.google.common.annotations.VisibleForTesting;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
 import java.security.AccessControlContext;
 import java.security.AccessController;
 import java.security.Principal;
@@ -39,6 +42,10 @@ public class ReqContext {
     private InetAddress _remoteAddr;
     private Integer _reqID;
     private Map _storm_conf;
+    private Principal realPrincipal;
+
+    private static final Logger LOG = LoggerFactory.getLogger(ReqContext.class);
+
 
     /**
      * Get a request context associated with current thread
@@ -87,7 +94,7 @@ public class ReqContext {
      * Set remote subject explicitly
      */
     public void setSubject(Subject subject) {
-        _subject = subject;	
+        _subject = subject;
     }
 
     /**
@@ -106,6 +113,24 @@ public class ReqContext {
         if (princs.size()==0) return null;
         return (Principal) (princs.toArray()[0]);
     }
+
+    public void setRealPrincipal(Principal realPrincipal) {
+        this.realPrincipal = realPrincipal;
+    }
+    /**
+     * The real principal associated with the subject.
+     */
+    public Principal realPrincipal() {
+        return this.realPrincipal;
+    }
+
+    /**
+     * Returns true if this request is an impersonation request.
+     * @return
+     */
+    public boolean isImpersonating() {
+        return this.realPrincipal != null;
+    }
     
     /**
      * request ID of this request
@@ -113,4 +138,5 @@ public class ReqContext {
     public Integer requestID() {
         return _reqID;
     }
+
 }

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}
```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes introduced in the diff file are directly related to security functionalities, specifically in the context of authentication and logging. The addition of a 'realPrincipal' field and methods related to it ('setRealPrincipal', 'realPrincipal', and 'isImpersonating') directly impact the authentication mechanism by introducing the concept of impersonation. This allows for distinguishing between the actual principal (the entity that has been authenticated) and a 'real' principal (the entity on whose behalf actions are being performed), which is crucial for systems that support impersonation or delegated authentication. The introduction of impersonation capabilities can have significant security implications, as it affects how authentication and authorization decisions are made. It is essential to ensure that the impersonation feature is implemented securely, with adequate checks and balances to prevent unauthorized impersonation. Additionally, the introduction of logging (through the 'LOG' variable) could impact the system's security posture by providing a mechanism to log security-relevant events. However, without seeing the specific usage of the logger in this context, it's challenging to assess the impact fully. Overall, these changes are directly related to security functionalities and could potentially impact the system's security, depending on how they are utilized within the broader application context.",
  "confidence": 9
}
```

Diff number: 6
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
index 64bec193b..532ebd726 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/SaslTransportPlugin.java
@@ -116,7 +116,6 @@ public abstract class SaslTransportPlugin implements ITransportPlugin {
             TTransport trans = inProt.getTransport();
             //Sasl transport
             TSaslServerTransport saslTrans = (TSaslServerTransport)trans;
-
             //remote address
             TSocket tsocket = (TSocket)saslTrans.getUnderlyingTransport();
             Socket socket = tsocket.getSocket();
@@ -128,7 +127,7 @@ public abstract class SaslTransportPlugin implements ITransportPlugin {
             Subject remoteUser = new Subject();
             remoteUser.getPrincipals().add(new User(authId));
             req_context.setSubject(remoteUser);
-            
+
             //invoke service handler
             return wrapped.process(inProt, outProt);
         }

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The changes in the diff file consist of a removal of a blank line and a replacement of a blank line with another that has a different whitespace character (likely a space instead of a tab or vice versa). These modifications do not alter any logic or functionality related to authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, they do not directly impact the system's security posture or its security functionalities. The changes are purely stylistic and do not affect the execution or behavior of the code in any way that would be relevant to security.",
  "confidence": 10
}
```

Diff number: 7
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java
index 0e1a3652a..ab06635fb 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/SimpleTransportPlugin.java
@@ -90,8 +90,11 @@ public class SimpleTransportPlugin implements ITransportPlugin {
     /**
      * Connect to the specified server via framed transport 
      * @param transport The underlying Thrift transport.
+     * @param serverHost unused.
+     * @param asUser unused.
      */
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException {
+    @Override
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException {
         //create a framed transport
         TTransport conn = new TFramedTransport(transport);
 

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff shows a modification in the SimpleTransportPlugin.java file, specifically in the method signature of the 'connect' method. The changes include adding two new parameters ('serverHost' and 'asUser') to the method signature. However, these new parameters are marked as unused in the method's documentation. Since there is no implementation logic provided or modified that utilizes these parameters for authentication, authorization, encryption, or any other security-related functionality, the changes appear to be preparatory or structural rather than directly impacting the system's security posture. Without further context or implementation details that show how these parameters are intended to be used for security purposes, the changes can be classified as not directly relevant to security functionalities.",
  "confidence": 8
}
```

Diff number: 8
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java b/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
index eaed61ff4..f547868f8 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/TBackoffConnect.java
@@ -40,12 +40,12 @@ public class TBackoffConnect {
                                                               retryTimes);
     }
 
-    public TTransport doConnectWithRetry(ITransportPlugin transportPlugin, TTransport underlyingTransport, String host) throws IOException {
+    public TTransport doConnectWithRetry(ITransportPlugin transportPlugin, TTransport underlyingTransport, String host, String asUser) throws IOException {
         boolean connected = false;
         TTransport transportResult = null;
         while(!connected) {
             try {
-                transportResult = transportPlugin.connect(underlyingTransport, host);
+                transportResult = transportPlugin.connect(underlyingTransport, host, asUser);
                 connected = true;
             } catch (TTransportException ex) {
                 retryNext(ex);

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.util.Map;
import javax.security.auth.login.Configuration;
import org.apache.storm.Config;
import org.apache.storm.thrift.protocol.TBinaryProtocol;
import org.apache.storm.thrift.protocol.TProtocol;
import org.apache.storm.thrift.transport.TSocket;
import org.apache.storm.thrift.transport.TTransport;
import org.apache.storm.utils.ObjectReader;

public class ThriftClient implements AutoCloseable {
    protected TProtocol protocol;
    protected boolean retryForever = false;
    private TTransport transport;
    private String host;
    private Integer port;
    private Integer timeout;
    private Map conf;
    private ThriftConnectionType type;
    private String asUser;

    public ThriftClient(Map<String, Object> topoConf, ThriftConnectionType type, String host) {
        this(topoConf, type, host, null, null, null);
    }

    public ThriftClient(Map<String, Object> topoConf, ThriftConnectionType type, String host, Integer port, Integer timeout) {
        this(topoConf, type, host, port, timeout, null);
    }

    public ThriftClient(Map<String, Object> topoConf, ThriftConnectionType type, String host, Integer port, Integer timeout,
                        String asUser) {
        //create a socket with server
        if (host == null) {
            throw new IllegalArgumentException("host is not set");
        }

        if (port == null) {
            port = type.getPort(topoConf);
        }

        if (timeout == null) {
            timeout = type.getSocketTimeOut(topoConf);
        }

        if (port <= 0 && !type.isFake()) {
            throw new IllegalArgumentException("invalid port: " + port);
        }

        this.host = host;
        this.port = port;
        this.timeout = timeout;
        conf = topoConf;
        this.type = type;
        this.asUser = asUser;
        if (!type.isFake()) {
            reconnect();
        }
    }

    public synchronized TTransport transport() {
        return transport;
    }

    public synchronized void reconnect() {
        close();
        TSocket socket = null;
        try {
            socket = new TSocket(host, port);
            if (timeout != null) {
                socket.setTimeout(timeout);
            }

            //construct a transport plugin
            ITransportPlugin transportPlugin = ClientAuthUtils.getTransportPlugin(type, conf);

            //TODO get this from type instead of hardcoding to Nimbus.
            //establish client-server transport via plugin
            //do retries if the connect fails
            TBackoffConnect connectionRetry
                = new TBackoffConnect(
                ObjectReader.getInt(conf.get(Config.STORM_NIMBUS_RETRY_TIMES)),
                ObjectReader.getInt(conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL)),
                ObjectReader.getInt(conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)),
                    retryForever);
            transport = connectionRetry.doConnectWithRetry(transportPlugin, socket, host, asUser);
        } catch (Exception ex) {
            // close the socket, which releases connection if it has created any.
            if (socket != null) {
                try {
                    socket.close();
                } catch (Exception e) {
                    //ignore
                }
            }
            throw new RuntimeException(ex);
        }
        protocol = null;
        if (transport != null) {
            protocol = new TBinaryProtocol(transport);
        }
    }

    @Override
    public synchronized void close() {
        if (transport != null) {
            transport.close();
            transport = null;
            protocol = null;
        }
    }
}

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The code changes in the diff file directly impact the authentication mechanism of the system. Specifically, the modification adds an 'asUser' parameter to the 'doConnectWithRetry' method and subsequently to the 'connect' method of the 'ITransportPlugin' interface. This change suggests that the system's authentication process now includes the capability to specify the user context under which the connection should be established. This is a significant alteration to how authentication and potentially authorization are handled, as it introduces the possibility for connections to be made on behalf of different users. Depending on how this 'asUser' parameter is utilized and validated within the 'connect' method of the implementing 'ITransportPlugin', there could be implications for security such as privilege escalation, if not properly secured. The addition of user impersonation or delegation features requires careful consideration of security controls to prevent abuse. Therefore, this change is directly related to security functionalities, specifically affecting the authentication mechanism and potentially the authorization model of the system.",
  "confidence": 9
}
```

Diff number: 9
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java b/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java
index ec337d990..9f77ab92d 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/ThriftClient.java
@@ -39,12 +39,17 @@ public class ThriftClient {
     private Integer _timeout;
     private Map _conf;
     private ThriftConnectionType _type;
+    private String _asUser;
 
     public ThriftClient(Map storm_conf, ThriftConnectionType type, String host) {
-        this(storm_conf, type, host, null, null);
+        this(storm_conf, type, host, null, null, null);
     }
 
-    public ThriftClient(Map storm_conf, ThriftConnectionType type, String host, Integer port, Integer timeout) {
+    public ThriftClient(Map storm_conf, ThriftConnectionType type, String host, Integer port, Integer timeout){
+        this(storm_conf, type, host, port, timeout, null);
+    }
+
+    public ThriftClient(Map storm_conf, ThriftConnectionType type, String host, Integer port, Integer timeout, String asUser) {
         //create a socket with server
         if (host==null) {
             throw new IllegalArgumentException("host is not set");
@@ -63,6 +68,7 @@ public class ThriftClient {
         _timeout = timeout;
         _conf = storm_conf;
         _type = type;
+        _asUser = asUser;
         reconnect();
     }
 
@@ -94,7 +100,7 @@ public class ThriftClient {
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_TIMES)),
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL)),
                                       Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)));
-            _transport = connectionRetry.doConnectWithRetry(transportPlugin, underlyingTransport, _host);
+            _transport = connectionRetry.doConnectWithRetry(transportPlugin, underlyingTransport, _host, _asUser);
         } catch (IOException ex) {
             throw new RuntimeException(ex);
         }

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes introduced in the diff file are directly related to the security functionalities of the system, specifically within the authentication mechanism of the ThriftClient class. The addition of a new private field '_asUser' and its incorporation into the ThriftClient constructors suggest a modification in how authentication or user impersonation is handled within the system. The '_asUser' field is likely intended to specify the user context under which the Thrift client operations are performed, which is a significant aspect of security, especially in a multi-user environment where access control and user privileges are critical. Furthermore, the modification in the 'doConnectWithRetry' method to include '_asUser' as a parameter indicates that the connection or session establishment process now considers this user context, potentially altering the authentication flow or the access control logic. This change could impact how users are authenticated and how their permissions are verified when establishing a connection, which is essential for ensuring that only authorized users can perform certain actions or access specific information. Therefore, these modifications could affect the system's security posture by introducing changes to the authentication mechanisms, potentially impacting access control procedures and the enforcement of user privileges.",
  "confidence": 8
}
```

Diff number: 10
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
new file mode 100644
index 000000000..1e947ae67
--- /dev/null
+++ b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
@@ -0,0 +1,148 @@
+package backtype.storm.security.auth.authorizer;
+
+import backtype.storm.Config;
+import backtype.storm.security.auth.*;
+import com.google.common.collect.ImmutableSet;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.*;
+
+
+public class ImpersonationAuthorizer implements IAuthorizer {
+    private static final Logger LOG = LoggerFactory.getLogger(ImpersonationAuthorizer.class);
+
+    protected Map<String, ImpersonationACL> userImpersonationACL;
+    protected IPrincipalToLocal _ptol;
+    protected IGroupMappingServiceProvider _groupMappingProvider;
+
+    @Override
+    public void prepare(Map conf) {
+        userImpersonationACL = new HashMap<String, ImpersonationACL>();
+
+        Map<String, Map<String, List<String>>> userToHostAndGroup = (Map<String, Map<String, List<String>>>) conf.get(Config.NIMBUS_IMPERSONATION_ACL);
+
+        if (userToHostAndGroup != null) {
+            for (String user : userToHostAndGroup.keySet()) {
+                Set<String> groups = ImmutableSet.copyOf(userToHostAndGroup.get(user).get("groups"));
+                Set<String> hosts = ImmutableSet.copyOf(userToHostAndGroup.get(user).get("hosts"));
+                userImpersonationACL.put(user, new ImpersonationACL(user, groups, hosts));
+            }
+        }
+
+        _ptol = AuthUtils.GetPrincipalToLocalPlugin(conf);
+        _groupMappingProvider = AuthUtils.GetGroupMappingServiceProviderPlugin(conf);
+    }
+
+    @Override
+    public boolean permit(ReqContext context, String operation, Map topology_conf) {
+        if (!context.isImpersonating()) {
+            LOG.debug("Not an impersonation attempt.");
+            return true;
+        }
+
+        String impersonatingPrincipal = context.realPrincipal().getName();
+        String impersonatingUser = _ptol.toLocal(context.realPrincipal());
+        String userBeingImpersonated = _ptol.toLocal(context.principal());
+        InetAddress remoteAddress = context.remoteAddress();
+
+        LOG.info("user = {}, principal = {} is attmepting to impersonate user = {} for operation = {} from host = {}",
+                impersonatingUser, impersonatingPrincipal, userBeingImpersonated, operation, remoteAddress);
+
+        /**
+         * no config is present for impersonating principal or user, do not permit impersonation.
+         */
+        if (!userImpersonationACL.containsKey(impersonatingPrincipal) && !userImpersonationACL.containsKey(impersonatingUser)) {
+            LOG.info("user = {}, principal = {} is trying to impersonate user {}, but config {} does not have entry for impersonating user or principal." +
+                    "Please see SECURITY.MD to learn how to configure users for impersonation."
+                    , impersonatingUser, impersonatingPrincipal, userBeingImpersonated, Config.NIMBUS_IMPERSONATION_ACL);
+            return false;
+        }
+
+        ImpersonationACL principalACL = userImpersonationACL.get(impersonatingPrincipal);
+        ImpersonationACL userACL = userImpersonationACL.get(impersonatingUser);
+
+        Set<String> authorizedHosts = new HashSet<String>();
+        Set<String> authorizedGroups = new HashSet<String>();
+
+        if (principalACL != null) {
+            authorizedHosts.addAll(principalACL.authorizedHosts);
+            authorizedGroups.addAll(principalACL.authorizedGroups);
+        }
+
+        if (userACL != null) {
+            authorizedHosts.addAll(userACL.authorizedHosts);
+            authorizedGroups.addAll(userACL.authorizedGroups);
+        }
+
+        LOG.debug("user = {}, principal = {} is allowed to impersonate groups = {} from hosts = {} ",
+                impersonatingUser, impersonatingPrincipal, authorizedGroups, authorizedHosts);
+
+        if (!isAllowedToImpersonateFromHost(authorizedHosts, remoteAddress)) {
+            LOG.info("user = {}, principal = {} is not allowed to impersonate from host {} ",
+                    impersonatingUser, impersonatingPrincipal, remoteAddress);
+            return false;
+        }
+
+        if (!isAllowedToImpersonateUser(authorizedGroups, userBeingImpersonated)) {
+            LOG.info("user = {}, principal = {} is not allowed to impersonate any group that user {} is part of.",
+                    impersonatingUser, impersonatingPrincipal, userBeingImpersonated);
+            return false;
+        }
+
+        LOG.info("Allowing impersonation of user {} by user {}", userBeingImpersonated, impersonatingUser);
+        return true;
+    }
+
+    private boolean isAllowedToImpersonateFromHost(Set<String> authorizedHosts, InetAddress remoteAddress) {
+        return authorizedHosts.contains(remoteAddress.getCanonicalHostName()) ||
+                authorizedHosts.contains(remoteAddress.getHostName()) ||
+                authorizedHosts.contains(remoteAddress.getHostAddress());
+    }
+
+    private boolean isAllowedToImpersonateUser(Set<String> authorizedGroups, String userBeingImpersonated) {
+        Set<String> groups = null;
+        try {
+            groups = _groupMappingProvider.getGroups(userBeingImpersonated);
+        } catch (IOException e) {
+            throw new RuntimeException("failed to get groups for user " + userBeingImpersonated);
+        }
+
+        if (groups == null || groups.isEmpty()) {
+            return false;
+        }
+
+        for (String group : groups) {
+            if (authorizedGroups.contains(group)) {
+                return true;
+            }
+        }
+
+        return false;
+    }
+
+    protected class ImpersonationACL {
+        public String impersonatingUser;
+        //Groups this user is authorized to impersonate.
+        public Set<String> authorizedGroups;
+        //Hosts this user is authorized to impersonate from.
+        public Set<String> authorizedHosts;
+
+        private ImpersonationACL(String impersonatingUser, Set<String> authorizedGroups, Set<String> authorizedHosts) {
+            this.impersonatingUser = impersonatingUser;
+            this.authorizedGroups = authorizedGroups;
+            this.authorizedHosts = authorizedHosts;
+        }
+
+        @Override
+        public String toString() {
+            return "ImpersonationACL{" +
+                    "impersonatingUser='" + impersonatingUser + '\'' +
+                    ", authorizedGroups=" + authorizedGroups +
+                    ", authorizedHosts=" + authorizedHosts +
+                    '}';
+        }
+    }
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.security.auth;

import java.io.File;
import java.io.IOException;
import java.net.InetAddress;
import java.security.Principal;
import java.security.PrivilegedActionException;
import java.security.PrivilegedExceptionAction;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicReference;
import javax.security.auth.Subject;
import org.apache.storm.Config;
import org.apache.storm.Testing;
import org.apache.storm.cluster.ClusterStateContext;
import org.apache.storm.cluster.ClusterUtils;
import org.apache.storm.cluster.DaemonType;
import org.apache.storm.cluster.IStormClusterState;
import org.apache.storm.generated.Nimbus;
import org.apache.storm.generated.WorkerToken;
import org.apache.storm.generated.WorkerTokenServiceType;
import org.apache.storm.security.auth.authorizer.ImpersonationAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleACLAuthorizer;
import org.apache.storm.security.auth.authorizer.SimpleWhitelistAuthorizer;
import org.apache.storm.security.auth.digest.DigestSaslTransportPlugin;
import org.apache.storm.security.auth.workertoken.WorkerTokenManager;
import org.apache.storm.testing.InProcessZookeeper;
import org.apache.storm.thrift.transport.TTransportException;
import org.apache.storm.utils.ConfigUtils;
import org.apache.storm.utils.NimbusClient;
import org.apache.storm.utils.Time;
import org.apache.storm.utils.Utils;
import org.junit.jupiter.api.Test;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertThrows;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;
import static org.mockito.Mockito.*;

public class AuthTest {
    //3 seconds in milliseconds
    public static final int NIMBUS_TIMEOUT = 3_000;
    private static final Logger LOG = LoggerFactory.getLogger(AuthTest.class);
    private static final File BASE = new File("./src/test/resources/");
    private static final String DIGEST_JAAS_CONF = new File(BASE, "jaas_digest.conf").getAbsolutePath();
    private static final String BAD_PASSWORD_CONF = new File(BASE, "jaas_digest_bad_password.conf").getAbsolutePath();
    private static final String WRONG_USER_CONF = new File(BASE, "jaas_digest_unknown_user.conf").getAbsolutePath();
    private static final String MISSING_CLIENT = new File(BASE, "jaas_digest_missing_client.conf").getAbsolutePath();

    public static Principal mkPrincipal(final String name) {
        return new Principal() {
            @Override
            public String getName() {
                return name;
            }

            @Override
            public boolean equals(Object other) {
                return other instanceof Principal
                       && name.equals(((Principal) other).getName());
            }

            @Override
            public String toString() {
                return name;
            }

            @Override
            public int hashCode() {
                return name.hashCode();
            }
        };
    }

    public static Subject mkSubject(String name) {
        return new Subject(true, Collections.singleton(mkPrincipal(name)),
                           Collections.emptySet(), Collections.emptySet());
    }

    public static void withServer(Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(null, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        withServer(loginCfg, transportPluginClass, impl, null, null, body);
    }

    public static void withServer(String loginCfg,
                                  Class<? extends ITransportPlugin> transportPluginClass,
                                  Nimbus.Iface impl,
                                  InProcessZookeeper zk,
                                  Map<String, Object> extraConfs,
                                  MyBiConsumer<ThriftServer, Map<String, Object>> body) throws Exception {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.NIMBUS_THRIFT_PORT, 0);
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, transportPluginClass.getName());

        if (loginCfg != null) {
            conf.put("java.security.auth.login.config", loginCfg);
        }

        if (zk != null) {
            conf.put(Config.STORM_ZOOKEEPER_SERVERS, Collections.singletonList("localhost"));
            conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());
        }

        if (extraConfs != null) {
            conf.putAll(extraConfs);
        }

        Nimbus.Iface handler = impl != null ? impl : mock(Nimbus.Iface.class);
        final ThriftServer server = new ThriftServer(conf,
                                                     new Nimbus.Processor<>(handler),
                                                     ThriftConnectionType.NIMBUS);

        LOG.info("Created Server... {}", server);
        new Thread(() -> {
            LOG.info("Starting Serving...");
            server.serve();
        }).start();
        Testing.whileTimeout(
            () -> !server.isServing(),
            () -> {
                try {
                    Time.sleep(100);
                } catch (InterruptedException e) {
                    //Ignored
                }
            });
        try {
            LOG.info("Starting to run {}", body);
            body.accept(server, conf);
            LOG.info("{} finished with no exceptions", body);
        } finally {
            LOG.info("Stopping server {}", server);
            server.stop();
        }
    }

    public static void verifyIncorrectJaasConf(ThriftServer server, Map<String, Object> conf, String jaas,
                                               Class<? extends Exception> expectedException) {
        Map<String, Object> badConf = new HashMap<>(conf);
        badConf.put("java.security.auth.login.config", jaas);
        try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
            client.getClient().activate("bad_auth_test_topology");
            fail("An exception should have been thrown trying to connect.");
        } catch (Exception e) {
            LOG.info("Got Exception...", e);
            if (!Utils.exceptionCauseIsInstanceOf(expectedException, e)) {
                throw new AssertionError("Expecting " + expectedException.getClass().getName() + " but got " + e.getClass().getName(), e);
            }
        }
    }

    public static Subject createSubjectWith(WorkerToken wt) {
        //This is a bit ugly, but it shows how this would happen in a worker, so we will use the same APIs
        Map<String, String> creds = new HashMap<>();
        ClientAuthUtils.setWorkerToken(creds, wt);
        Subject subject = new Subject();
        ClientAuthUtils.updateSubject(subject, Collections.emptyList(), creds);
        return subject;
    }

    public static void tryConnectAs(Map<String, Object> conf, ThriftServer server, Subject subject, String topoId)
        throws PrivilegedActionException {
        Subject.doAs(subject, (PrivilegedExceptionAction<Void>) () -> {
            try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                client.getClient().activate(topoId); //Yes this should be a topo name, but it makes this simpler...
            }
            return null;
        });
    }

    public static Subject testConnectWithTokenFor(WorkerTokenManager wtMan, Map<String, Object> conf, ThriftServer server,
                                                  String user, String topoId) throws PrivilegedActionException {
        WorkerToken wt = wtMan.createOrUpdateTokenFor(WorkerTokenServiceType.NIMBUS, user, topoId);
        Subject subject = createSubjectWith(wt);
        tryConnectAs(conf, server, subject, topoId);
        return subject;
    }

    public static void verifyUserIs(AtomicReference<ReqContext> user, String userName) {
        //The user from the token is bob, so verify that the name was set correctly...
        ReqContext found = user.get();
        assertNotNull(found);
        assertEquals(userName, found.principal().getName());
        assertFalse(found.isImpersonating());
        user.set(null);
    }

    public static ReqContext mkImpersonatingReqContext(String impersonatingUser, String userBeingImpersonated, InetAddress remoteAddress) {
        ReqContext ret = new ReqContext(mkSubject(userBeingImpersonated));
        ret.setRemoteAddress(remoteAddress);
        ret.setRealPrincipal(mkPrincipal(impersonatingUser));
        return ret;
    }

    @Test
    public void kerbToLocalTest() {
        KerberosPrincipalToLocal kptol = new KerberosPrincipalToLocal();
        kptol.prepare(Collections.emptyMap());
        assertEquals("me", kptol.toLocal(mkPrincipal("me@realm")));
        assertEquals("simple", kptol.toLocal(mkPrincipal("simple")));
        assertEquals("someone", kptol.toLocal(mkPrincipal("someone/host@realm")));
    }

    @Test
    public void simpleAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        withServer(SimpleTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       //Verify digest is rejected...
                       Map<String, Object> badConf = new HashMap<>(conf);
                       badConf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, DigestSaslTransportPlugin.class.getName());
                       badConf.put("java.security.auth.login.config", DIGEST_JAAS_CONF);
                       badConf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                       try (NimbusClient client = new NimbusClient(badConf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_security_auth_test_topology");
    }

    @Test
    public void digestAuthTest() throws Exception {
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        withServer(DIGEST_JAAS_CONF,
                   DigestSaslTransportPlugin.class,
                   impl,
                   (ThriftServer server, Map<String, Object> conf) -> {
                       try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("security_auth_test_topology");
                       }

                       conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);

                       //Verify simple is rejected...
                       Map<String, Object> badTransport = new HashMap<>(conf);
                       badTransport.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, SimpleTransportPlugin.class.getName());
                       try (NimbusClient client = new NimbusClient(badTransport, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                           client.getClient().activate("bad_security_auth_test_topology");
                           fail("An exception should have been thrown trying to connect.");
                       } catch (Exception e) {
                           LOG.info("Got Exception...", e);
                           if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                               throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                           }
                       }
                       //The user here from the jaas conf is bob.  No impersonation is done, so verify that
                       ReqContext found = user.get();
                       assertNotNull(found);
                       assertEquals("bob", found.principal().getName());
                       assertFalse(found.isImpersonating());
                       user.set(null);

                       verifyIncorrectJaasConf(server, conf, BAD_PASSWORD_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, WRONG_USER_CONF, TTransportException.class);
                       verifyIncorrectJaasConf(server, conf, "./nonexistent.conf", RuntimeException.class);
                       verifyIncorrectJaasConf(server, conf, MISSING_CLIENT, IOException.class);
                   });
        verify(impl).activate("security_auth_test_topology");
        verify(impl, never()).activate("bad_auth_test_topology");
    }

    @Test
    public void workerTokenDigestAuthTest() throws Exception {
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - START\n\n");
        Nimbus.Iface impl = mock(Nimbus.Iface.class);
        final AtomicReference<ReqContext> user = new AtomicReference<>();
        doAnswer((invocation) -> {
            user.set(new ReqContext(ReqContext.context()));
            return null;
        }).when(impl).activate(anyString());

        Map<String, Object> extraConfs = new HashMap<>();
        //Let worker tokens work on insecure ZK...
        extraConfs.put("TESTING.ONLY.ENABLE.INSECURE.WORKER.TOKENS", true);

        try (InProcessZookeeper zk = new InProcessZookeeper()) {
            withServer(MISSING_CLIENT,
                       DigestSaslTransportPlugin.class,
                       impl,
                       zk,
                       extraConfs,
                       (ThriftServer server, Map<String, Object> conf) -> {
                           try (Time.SimulatedTime ignored = new Time.SimulatedTime()) {
                               conf.put(Config.STORM_NIMBUS_RETRY_TIMES, 0);
                               //We cannot connect if there is no client section in the jaas conf...
                               try (NimbusClient client = new NimbusClient(conf, "localhost", server.getPort(), NIMBUS_TIMEOUT)) {
                                   client.getClient().activate("bad_auth_test_topology");
                                   fail("We should not be able to connect without a token...");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(IOException.class, e)) {
                                       throw new AssertionError("Expecting IOException but got " + e.getClass().getName(), e);
                                   }
                               }

                               //Now let's create a token and verify that we can connect...
                               IStormClusterState state =
                                   ClusterUtils.mkStormClusterState(conf, new ClusterStateContext(DaemonType.NIMBUS, conf));
                               WorkerTokenManager wtMan = new WorkerTokenManager(conf, state);
                               Subject bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(12));

                               //Alice has no digest jaas section at all...
                               Subject alice = testConnectWithTokenFor(wtMan, conf, server, "alice", "topo-alice");
                               verifyUserIs(user, "alice");

                               Time.advanceTimeSecs(TimeUnit.HOURS.toSeconds(13));
                               //Verify that bob's token has expired

                               try {
                                   tryConnectAs(conf, server, bob, "bad_auth_test_topology");
                                   fail("We should not be able to connect with bad auth");
                               } catch (Exception e) {
                                   if (!Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {
                                       throw new AssertionError("Expecting TTransportException but got " + e.getClass().getName(), e);
                                   }
                               }
                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");

                               //Now see if we can create a new token for bob and try again.
                               bob = testConnectWithTokenFor(wtMan, conf, server, "bob", "topo-bob");
                               verifyUserIs(user, "bob");

                               tryConnectAs(conf, server, alice, "topo-alice");
                               verifyUserIs(user, "alice");
                           }
                       });
        }
        verify(impl, times(2)).activate("topo-bob");
        verify(impl, times(3)).activate("topo-alice");
        verify(impl, never()).activate("bad_auth_test_topology");
        LOG.info("\n\n\t\tworkerTokenDigestAuthTest - END\n\n");
    }

    @Test
    public void negativeWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertFalse(auth.permit(context, "activate", conf));
    }

    @Test
    public void positiveWhitelistAuthroizationTest() {
        SimpleWhitelistAuthorizer auth = new SimpleWhitelistAuthorizer();
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(SimpleWhitelistAuthorizer.WHITELIST_USERS_CONF, Collections.singletonList("user"));
        auth.prepare(conf);
        ReqContext context = new ReqContext(mkSubject("user"));
        assertTrue(auth.permit(context, "activate", conf));
    }

    @Test
    public void simpleAclUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertTrue(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertFalse(authorizer.permit(supervisor, "submitTopology", empty));

        assertTrue(authorizer.permit(userA, "fileUpload", null));
        assertTrue(authorizer.permit(userB, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(supervisor, "fileUpload", null));

        assertTrue(authorizer.permit(userA, "getNimbusConf", null));
        assertTrue(authorizer.permit(userB, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertFalse(authorizer.permit(supervisor, "getNimbusConf", null));

        assertTrue(authorizer.permit(userA, "getClusterInfo", null));
        assertTrue(authorizer.permit(userB, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertFalse(authorizer.permit(supervisor, "getClusterInfo", null));

        assertFalse(authorizer.permit(userA, "fileDownload", null));
        assertFalse(authorizer.permit(userB, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));

        assertTrue(authorizer.permit(userA, "killTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "killTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(userB, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertFalse(authorizer.permit(supervisor, "uploadNewCredentials", aAllowed));

        assertTrue(authorizer.permit(userA, "rebalance", aAllowed));
        assertFalse(authorizer.permit(userB, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertFalse(authorizer.permit(supervisor, "rebalance", aAllowed));

        assertTrue(authorizer.permit(userA, "activate", aAllowed));
        assertFalse(authorizer.permit(userB, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "activate", aAllowed));

        assertTrue(authorizer.permit(userA, "deactivate", aAllowed));
        assertFalse(authorizer.permit(userB, "deactivate", aAllowed));
        assertTrue(authorizer.permit(admin, "deactivate", aAllowed));
        assertFalse(authorizer.permit(supervisor, "deactivate", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyConf", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(userB, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getUserTopology", aAllowed));

        assertTrue(authorizer.permit(userA, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(userB, "getTopologyInfo", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
        assertFalse(authorizer.permit(supervisor, "getTopologyInfo", aAllowed));
    }

    @Test
    public void simpleAclNimbusUsersAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclNimbusGroupsAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS_GROUPS, Collections.singletonList("admin-group"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("supervisor"));
        clusterConf.put(Config.NIMBUS_USERS, Collections.singletonList("user-a"));
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN, FixedGroupsMapping.class.getName());
        Map<String, Object> groups = new HashMap<>();
        groups.put("admin", Collections.singleton("admin-group"));
        groups.put("not-admin", Collections.singleton("not-admin-group"));
        Map<String, Object> groupsParams = new HashMap<>();
        groupsParams.put(FixedGroupsMapping.STORM_FIXED_GROUP_MAPPING, groups);
        clusterConf.put(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS, groupsParams);

        ReqContext admin = new ReqContext(mkSubject("admin"));
        ReqContext notAdmin = new ReqContext(mkSubject("not-admin"));
        ReqContext supervisor = new ReqContext(mkSubject("supervisor"));
        ReqContext userA = new ReqContext(mkSubject("user-a"));
        ReqContext userB = new ReqContext(mkSubject("user-b"));

        final Map<String, Object> empty = Collections.emptyMap();

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(userA, "submitTopology", empty));
        assertFalse(authorizer.permit(userB, "submitTopology", empty));

        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertFalse(authorizer.permit(notAdmin, "fileUpload", null));
        assertFalse(authorizer.permit(userB, "fileUpload", null));

        assertTrue(authorizer.permit(supervisor, "fileDownload", null));
    }

    @Test
    public void simpleAclSameUserAuthTest() {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        clusterConf.put(Config.NIMBUS_ADMINS, Collections.singletonList("admin"));
        clusterConf.put(Config.NIMBUS_SUPERVISOR_USERS, Collections.singletonList("admin"));
        ReqContext admin = new ReqContext(mkSubject("admin"));

        final Map<String, Object> empty = Collections.emptyMap();
        final Map<String, Object> aAllowed = new HashMap<>();
        aAllowed.put(Config.TOPOLOGY_USERS, Collections.singletonList("user-a"));

        SimpleACLAuthorizer authorizer = new SimpleACLAuthorizer();
        authorizer.prepare(clusterConf);

        assertTrue(authorizer.permit(admin, "submitTopology", empty));
        assertTrue(authorizer.permit(admin, "fileUpload", null));
        assertTrue(authorizer.permit(admin, "getNimbusConf", null));
        assertTrue(authorizer.permit(admin, "getClusterInfo", null));
        assertTrue(authorizer.permit(admin, "fileDownload", null));
        assertTrue(authorizer.permit(admin, "killTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "uploadNewCredentials", aAllowed));
        assertTrue(authorizer.permit(admin, "rebalance", aAllowed));
        assertTrue(authorizer.permit(admin, "activate", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyConf", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getUserTopology", aAllowed));
        assertTrue(authorizer.permit(admin, "getTopologyInfo", aAllowed));
    }

    @Test
    public void shellBaseGroupsMappingTest() throws Exception {
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groups = new ShellBasedGroupsMapping();
        groups.prepare(clusterConf);

        String userName = System.getProperty("user.name");

        assertTrue(groups.getGroups(userName).size() >= 0);
        assertEquals(0, groups.getGroups("userDoesNotExist").size());
        assertEquals(0, groups.getGroups(null).size());
    }

    @Test
    public void getTransportPluginThrowsRunimeTest() {
        Map<String, Object> conf = ConfigUtils.readStormConfig();
        conf.put(Config.STORM_THRIFT_TRANSPORT_PLUGIN, "null.invalid");
        assertThrows(RuntimeException.class, () -> ClientAuthUtils.getTransportPlugin(ThriftConnectionType.NIMBUS, conf));
    }

    @Test
    public void impersonationAuthorizerTest() throws Exception {
        final String impersonatingUser = "admin";
        final String userBeingImpersonated = System.getProperty("user.name");
        Map<String, Object> clusterConf = ConfigUtils.readStormConfig();
        ShellBasedGroupsMapping groupMapper = new ShellBasedGroupsMapping();
        groupMapper.prepare(clusterConf);
        Set<String> groups = groupMapper.getGroups(userBeingImpersonated);

        InetAddress localHost = InetAddress.getLocalHost();

        Map<String, Object> acl = new HashMap<>();
        Map<String, Object> aclConf = new HashMap<>();
        aclConf.put("hosts", Collections.singletonList(localHost.getHostName()));
        aclConf.put("groups", groups);
        acl.put(impersonatingUser, aclConf);
        clusterConf.put(Config.NIMBUS_IMPERSONATION_ACL, acl);

        InetAddress unauthorizedHost = com.google.common.net.InetAddresses.forString("10.10.10.10");

        ImpersonationAuthorizer authorizer = new ImpersonationAuthorizer();
        authorizer.prepare(clusterConf);

        //non impersonating request, should be permitted.
        assertTrue(authorizer.permit(new ReqContext(mkSubject("anyuser")), "fileUplaod", null));

        //user with no impersonation acl should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext("user-with-no-acl", userBeingImpersonated, localHost),
                                      "someOperation", null));

        //request from hosts that are not authorized should be rejected
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, unauthorizedHost),
                                      "someOperation", null));

        //request to impersonate users from unauthorized groups should be rejected.
        assertFalse(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, "unauthorized-user", localHost),
                                      "someOperation", null));

        //request from authorized hosts and group should be allowed.
        assertTrue(authorizer.permit(mkImpersonatingReqContext(impersonatingUser, userBeingImpersonated, localHost),
                                     "someOperation", null));
    }

    public interface MyBiConsumer<T, U> {
        void accept(T t, U u) throws Exception;
    }
}
```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The provided diff introduces a new file, ImpersonationAuthorizer.java, which is directly related to security functionalities, specifically to authentication and authorization mechanisms within the system. This class implements the IAuthorizer interface, indicating its role in deciding whether a given request, identified by a context, operation, and topology configuration, should be permitted or denied. The class is designed to handle impersonation attempts, where one user (the impersonating user) attempts to perform operations on behalf of another user (the user being impersonated). The security implications of this addition are significant, as it directly impacts how impersonation attempts are authorized, potentially affecting the overall security posture of the system. The class uses a configuration-based approach to determine whether an impersonation attempt should be allowed, based on predefined ACLs (Access Control Lists) that specify which users are allowed to impersonate others, from which hosts, and in which groups the impersonated users must be. This approach requires careful management of the ACL configuration to ensure that only legitimate impersonation attempts are authorized while preventing unauthorized access. The logging within the class provides audit trails for impersonation attempts, which is a positive aspect from a security perspective. However, the effectiveness of this mechanism heavily depends on the correctness and security of the ACL configuration, the integrity of the group mapping provider, and the principal to local conversion mechanism. Any misconfiguration or vulnerabilities in these components could lead to unauthorized impersonation, potentially compromising the security of the system.",
  "confidence": 9
}
```

Diff number: 12
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
index cb6857906..ad642d8b3 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/digest/DigestSaslTransportPlugin.java
@@ -51,10 +51,10 @@ public class DigestSaslTransportPlugin extends SaslTransportPlugin {
     }
 
     @Override
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException, IOException {
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {
         ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);
-        TSaslClientTransport wrapper_transport = new TSaslClientTransport(DIGEST, 
-                null, 
+        TSaslClientTransport wrapper_transport = new TSaslClientTransport(DIGEST,
+                null,
                 AuthUtils.SERVICE, 
                 serverHost,
                 null,

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file are directly related to the authentication mechanism of the system. Specifically, the method signature for 'connect' in the DigestSaslTransportPlugin class has been modified to include an additional parameter 'asUser'. This alteration suggests a change in how authentication is performed, potentially allowing for different user contexts during the connection process. Such a change could impact the security of the system by altering how users are authenticated and potentially allowing for more granular control or impersonation capabilities. However, without more context on how the 'asUser' parameter is used within the authentication process, it's challenging to fully assess the impact. This change directly affects the authentication mechanism, which is a critical security component, thus making it security relevant.",
  "confidence": 8
}
```

Diff number: 13
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
index a0e4839df..0ff3d35bd 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
@@ -20,8 +20,13 @@ package backtype.storm.security.auth.digest;
 import java.io.IOException;
 import java.util.HashMap;
 import java.util.Map;
+
+import backtype.storm.security.auth.ReqContext;
+import backtype.storm.security.auth.SaslTransportPlugin;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
+import javax.security.auth.Subject;
 import javax.security.auth.callback.Callback;
 import javax.security.auth.callback.CallbackHandler;
 import javax.security.auth.callback.NameCallback;
@@ -107,8 +112,19 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private void handleAuthorizeCallback(AuthorizeCallback ac) {
         String authenticationID = ac.getAuthenticationID();
-        LOG.debug("Successfully authenticated client: authenticationID=" + authenticationID);
-        ac.setAuthorizedID(authenticationID);
+        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID + " authorizationID= " + ac.getAuthorizationID());
+
+        //if authorizationId is not set, set it to authenticationId.
+        if(ac.getAuthorizationID() == null) {
+            ac.setAuthorizedID(authenticationID);
+        }
+
+        //When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We
+        //add the authNid as the real user in reqContext's subject which will be used during authorization.
+        if(!authenticationID.equals(ac.getAuthorizationID())) {
+            ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        }
+
         ac.setAuthorized(true);
     }
 }

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The provided diff file introduces changes in the ServerCallbackHandler.java, which is part of the authentication mechanism in the system. Specifically, the changes impact how authentication and authorization IDs are handled during the callback process. The modifications include logging both the authenticationID and authorizationID at an info level, which could potentially expose sensitive information in logs that were previously only debugged. Additionally, the code now handles scenarios where the authorizationID is not set by defaulting it to the authenticationID, and it introduces a new condition where if the authenticationID and authorizationID are not equal, it treats the situation as an attempt of impersonation and adds the authenticationID as the real user in the request context's subject. This is a significant change as it directly affects the system's security by altering how user identities are verified and managed, potentially impacting the authorization process. The addition of impersonation handling suggests an enhancement in security by acknowledging and addressing the possibility of impersonation attacks. However, the change in logging level and the handling of authorization IDs without explicit checks could introduce new security considerations, such as information leakage or incorrect authorization decisions.",
  "confidence": 9
}
```

Diff number: 14
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
index 451f87bb4..b6cccadd2 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
@@ -32,6 +32,8 @@ import javax.security.auth.login.AppConfigurationEntry;
 import javax.security.auth.login.Configuration;
 import javax.security.auth.login.LoginException;
 import javax.security.sasl.Sasl;
+
+import org.apache.commons.lang.StringUtils;
 import org.apache.thrift.transport.TSaslClientTransport;
 import org.apache.thrift.transport.TSaslServerTransport;
 import org.apache.thrift.transport.TTransport;
@@ -51,7 +53,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
 
     public TTransportFactory getServerTransportFactory() throws IOException {
         //create an authentication callback handler
-        CallbackHandler server_callback_handler = new ServerCallbackHandler(login_conf);
+        CallbackHandler server_callback_handler = new ServerCallbackHandler(login_conf, storm_conf);
         
         //login our principal
         Subject subject = null;
@@ -92,7 +94,8 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
         return wrapFactory;
     }
 
-    public TTransport connect(TTransport transport, String serverHost) throws TTransportException, IOException {
+    @Override
+    public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {
         //create an authentication callback handler
         ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);
         
@@ -114,7 +117,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
                         +AuthUtils.LOGIN_CONTEXT_CLIENT+"\" in login configuration file "+ login_conf);
         }
 
-        final String principal = getPrincipal(subject); 
+        final String principal = StringUtils.isBlank(asUser) ? getPrincipal(subject) : asUser;
         String serviceName = AuthUtils.get(login_conf, AuthUtils.LOGIN_CONTEXT_CLIENT, "serviceName");
         if (serviceName == null) {
             serviceName = AuthUtils.SERVICE; 
@@ -138,7 +141,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
                     new PrivilegedExceptionAction<Void>() {
                 public Void run() {
                     try {
-                        LOG.debug("do as:"+ principal);
+                        LOG.info("do as:"+ principal);
                         sasalTransport.open();
                     }
                     catch (Exception e) {

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file directly impact the authentication mechanism of the system, specifically within the context of Kerberos authentication. The introduction of a new parameter 'asUser' in the 'connect' method allows for specifying a different user than the one obtained from the subject's principal. This change could potentially alter the authentication flow, enabling scenarios where actions can be performed on behalf of another user if not properly validated or authorized. Additionally, the modification from logging at a DEBUG level to an INFO level for the 'do as' operation increases the visibility of potentially sensitive operations, which could impact the confidentiality aspect of security by exposing more information in logs than necessary. The inclusion of 'org.apache.commons.lang.StringUtils' and its use to check if 'asUser' is blank before defaulting to the principal from the subject suggests an attempt to handle empty or null values, which is a good practice but also introduces new dependencies and logic paths that need to be securely managed. Overall, these changes are directly related to the system's security functionalities, specifically affecting authentication mechanisms and logging levels related to security events.",
  "confidence": 9
}
```

Diff number: 15
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
index 9dc75c4db..7b143f0d6 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/ServerCallbackHandler.java
@@ -18,22 +18,19 @@
 
 package backtype.storm.security.auth.kerberos;
 
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
+import backtype.storm.security.auth.AuthUtils;
+import backtype.storm.security.auth.ReqContext;
+import backtype.storm.security.auth.SaslTransportPlugin;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-import javax.security.auth.callback.Callback;
-import javax.security.auth.callback.CallbackHandler;
-import javax.security.auth.callback.NameCallback;
-import javax.security.auth.callback.PasswordCallback;
-import javax.security.auth.callback.UnsupportedCallbackException;
+
+import javax.security.auth.Subject;
+import javax.security.auth.callback.*;
 import javax.security.auth.login.AppConfigurationEntry;
 import javax.security.auth.login.Configuration;
 import javax.security.sasl.AuthorizeCallback;
-import javax.security.sasl.RealmCallback;
-
-import backtype.storm.security.auth.AuthUtils;
+import java.io.IOException;
+import java.util.Map;
 
 /**
  * SASL server side callback handler
@@ -43,7 +40,7 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private String userName;
 
-    public ServerCallbackHandler(Configuration configuration) throws IOException {
+    public ServerCallbackHandler(Configuration configuration, Map stormConf) throws IOException {
         if (configuration==null) return;
 
         AppConfigurationEntry configurationEntries[] = configuration.getAppConfigurationEntry(AuthUtils.LOGIN_CONTEXT_SERVER);
@@ -52,6 +49,7 @@ public class ServerCallbackHandler implements CallbackHandler {
             LOG.error(errorMessage);
             throw new IOException(errorMessage);
         }
+
     }
 
     public void handle(Callback[] callbacks) throws UnsupportedCallbackException {
@@ -78,9 +76,19 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private void handleAuthorizeCallback(AuthorizeCallback ac) {
         String authenticationID = ac.getAuthenticationID();
-        LOG.debug("Successfully authenticated client: authenticationID=" + authenticationID);
-        ac.setAuthorized(true);
+        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID + " authorizationID= " + ac.getAuthorizationID());
+
+        //if authorizationId is not set, set it to authenticationId.
+        if(ac.getAuthorizationID() == null) {
+            ac.setAuthorizedID(authenticationID);
+        }
 
-        ac.setAuthorizedID(authenticationID);
+        //When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We
+        //add the authNid as the real user in reqContext's subject which will be used during authorization.
+        if(!ac.getAuthenticationID().equals(ac.getAuthorizationID())) {
+            ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
+        }
+
+        ac.setAuthorized(true);
     }
 }

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file directly impact the authentication mechanism of the system, specifically within the Kerberos authentication flow. The modifications include changes to the ServerCallbackHandler class, which is a critical component in handling callbacks during Kerberos authentication. The addition of a new parameter to the ServerCallbackHandler constructor (Map stormConf) suggests an expansion in the configuration capabilities, potentially affecting how authentication settings are applied. Furthermore, the handling of the AuthorizeCallback has been modified to log more detailed information (including both authenticationID and authorizationID), and to set the authorizedID explicitly based on the authenticationID if the authorizationID is not set. This change could impact the system's security by altering how users are authorized and how their identities are managed post-authentication. Notably, the code now handles a scenario where the authenticationID and authorizationID do not match, indicating an attempt at impersonation. In such cases, it adds the authenticating user's ID as the 'real user' in the ReqContext's subject, which could have implications for access control decisions and logging. These changes are directly related to security functionalities, specifically authentication and potentially authorization, and thus are classified as security relevant.",
  "confidence": 9
}
```

Diff number: 16
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java b/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
index 3218e4994..b2a2a7d85 100644
--- a/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
+++ b/storm-core/src/jvm/backtype/storm/utils/DRPCClient.java
@@ -41,7 +41,7 @@ public class DRPCClient extends ThriftClient implements DistributedRPC.Iface {
     }
 
     public DRPCClient(Map conf, String host, int port, Integer timeout) throws TTransportException {
-        super(conf, ThriftConnectionType.DRPC, host, port, timeout);
+        super(conf, ThriftConnectionType.DRPC, host, port, timeout, null);
         this.host = host;
         this.port = port;
         this.client = new DistributedRPC.Client(_protocol);

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff shows a modification in the constructor call of a class, specifically adding an additional parameter (null) to the super constructor call of `ThriftClient`. This change does not directly modify any authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Without further context on what the additional parameter (null) represents or affects within the `ThriftClient` constructor, it's challenging to directly associate this change with a security functionality. Given the information available, the change appears to be more related to the configuration or setup of the `DRPCClient` rather than altering any security-critical components directly.",
  "confidence": 7
}
```

Diff number: 18
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
new file mode 100644
index 000000000..4d9f5da7b
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/HiveBolt.java
@@ -0,0 +1,289 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.task.OutputCollector;
+import backtype.storm.task.TopologyContext;
+import backtype.storm.tuple.Tuple;
+import backtype.storm.topology.base.BaseRichBolt;
+import backtype.storm.topology.OutputFieldsDeclarer;
+import org.apache.storm.hive.common.HiveWriter;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import org.apache.hive.hcatalog.streaming.*;
+import org.apache.storm.hive.common.HiveOptions;
+import org.apache.storm.hive.common.HiveUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.Map.Entry;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.io.IOException;
+
+public class HiveBolt extends  BaseRichBolt {
+    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
+    private OutputCollector collector;
+    private HiveOptions options;
+    private Integer currentBatchSize;
+    private ExecutorService callTimeoutPool;
+    private transient Timer heartBeatTimer;
+    private Boolean kerberosEnabled = false;
+    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
+    private UserGroupInformation ugi = null;
+    HashMap<HiveEndPoint, HiveWriter> allWriters;
+
+    public HiveBolt(HiveOptions options) {
+        this.options = options;
+        this.currentBatchSize = 0;
+    }
+
+    @Override
+    public void prepare(Map conf, TopologyContext topologyContext, OutputCollector collector)  {
+        try {
+            if(options.getKerberosPrincipal() == null && options.getKerberosKeytab() == null) {
+                kerberosEnabled = false;
+            } else if(options.getKerberosPrincipal() != null && options.getKerberosKeytab() != null) {
+                kerberosEnabled = true;
+            } else {
+                throw new IllegalArgumentException("To enable Kerberos, need to set both KerberosPrincipal " +
+                                                   " & KerberosKeytab");
+            }
+
+            if (kerberosEnabled) {
+                try {
+                    ugi = HiveUtils.authenticate(options.getKerberosKeytab(), options.getKerberosPrincipal());
+                } catch(HiveUtils.AuthenticationFailed ex) {
+                    LOG.error("Hive Kerberos authentication failed " + ex.getMessage(), ex);
+                    throw new IllegalArgumentException(ex);
+                }
+            }
+            this.collector = collector;
+            allWriters = new HashMap<HiveEndPoint,HiveWriter>();
+            String timeoutName = "hive-bolt-%d";
+            this.callTimeoutPool = Executors.newFixedThreadPool(1,
+                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
+            heartBeatTimer = new Timer();
+            setupHeartBeatTimer();
+        } catch(Exception e) {
+            LOG.warn("unable to make connection to hive ",e);
+        }
+    }
+
+    @Override
+    public void execute(Tuple tuple) {
+        try {
+            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
+            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
+            HiveWriter writer = getOrCreateWriter(endPoint);
+            if(timeToSendHeartBeat.compareAndSet(true, false)) {
+                enableHeartBeatOnAllWriters();
+            }
+            writer.write(options.getMapper().mapRecord(tuple));
+            currentBatchSize++;
+            if(currentBatchSize >= options.getBatchSize()) {
+                flushAllWriters();
+                currentBatchSize = 0;
+            }
+            collector.ack(tuple);
+        } catch(Exception e) {
+            this.collector.reportError(e);
+            collector.fail(tuple);
+            flushAndCloseWriters();
+        }
+    }
+
+    @Override
+    public void declareOutputFields(OutputFieldsDeclarer declarer) {
+
+    }
+
+    @Override
+    public void cleanup() {
+        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
+            try {
+                HiveWriter w = entry.getValue();
+                LOG.info("Flushing writer to {}", w);
+                w.flush(false);
+                LOG.info("Closing writer to {}", w);
+                w.close();
+            } catch (Exception ex) {
+                LOG.warn("Error while closing writer to " + entry.getKey() +
+                         ". Exception follows.", ex);
+                if (ex instanceof InterruptedException) {
+                    Thread.currentThread().interrupt();
+                }
+            }
+        }
+
+        ExecutorService toShutdown[] = {callTimeoutPool};
+        for (ExecutorService execService : toShutdown) {
+            execService.shutdown();
+            try {
+                while (!execService.isTerminated()) {
+                    execService.awaitTermination(
+                                 options.getCallTimeOut(), TimeUnit.MILLISECONDS);
+                }
+            } catch (InterruptedException ex) {
+                LOG.warn("shutdown interrupted on " + execService, ex);
+            }
+        }
+        callTimeoutPool = null;
+        super.cleanup();
+        LOG.info("Hive Bolt stopped");
+    }
+
+
+    private void setupHeartBeatTimer() {
+        if(options.getHeartBeatInterval()>0) {
+            heartBeatTimer.schedule(new TimerTask() {
+                    @Override
+                    public void run() {
+                        timeToSendHeartBeat.set(true);
+                        setupHeartBeatTimer();
+                    }
+                }, options.getHeartBeatInterval() * 1000);
+        }
+    }
+
+    private void flushAllWriters()
+        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
+        for(HiveWriter writer: allWriters.values()) {
+            writer.flush(true);
+        }
+    }
+
+    /**
+     * Closes all writers and remove them from cache
+     * @return number of writers retired
+     */
+    private void closeAllWriters() {
+        try {
+            //1) Retire writers
+            for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+                entry.getValue().close();
+            }
+            //2) Clear cache
+            allWriters.clear();
+        } catch(Exception e) {
+            LOG.warn("unable to close writers. ", e);
+        }
+    }
+
+    private void flushAndCloseWriters() {
+        try {
+            flushAllWriters();
+        } catch(Exception e) {
+            LOG.warn("unable to flush hive writers. ", e);
+        } finally {
+            closeAllWriters();
+        }
+    }
+
+    private void enableHeartBeatOnAllWriters() {
+        for (HiveWriter writer : allWriters.values()) {
+            writer.setHeartBeatNeeded();
+        }
+    }
+
+    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
+        throws HiveWriter.ConnectFailure, InterruptedException {
+        try {
+            HiveWriter writer = allWriters.get( endPoint );
+            if( writer == null ) {
+                LOG.debug("Creating Writer to Hive end point : " + endPoint);
+                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options);
+                if(allWriters.size() > options.getMaxOpenConnections()){
+                    int retired = retireIdleWriters();
+                    if(retired==0) {
+                        retireEldestWriter();
+                    }
+                }
+                allWriters.put(endPoint, writer);
+            }
+            return writer;
+        } catch (HiveWriter.ConnectFailure e) {
+            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
+            throw e;
+        }
+    }
+
+    /**
+     * Locate writer that has not been used for longest time and retire it
+     */
+    private void retireEldestWriter() {
+        long oldestTimeStamp = System.currentTimeMillis();
+        HiveEndPoint eldest = null;
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(entry.getValue().getLastUsed() < oldestTimeStamp) {
+                eldest = entry.getKey();
+                oldestTimeStamp = entry.getValue().getLastUsed();
+            }
+        }
+        try {
+            LOG.info("Closing least used Writer to Hive end point : " + eldest);
+            allWriters.remove(eldest).close();
+        } catch (IOException e) {
+            LOG.warn("Failed to close writer for end point: " + eldest, e);
+        } catch (InterruptedException e) {
+            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
+            Thread.currentThread().interrupt();
+        }
+    }
+
+    /**
+     * Locate all writers past idle timeout and retire them
+     * @return number of writers retired
+     */
+    private int retireIdleWriters() {
+        int count = 0;
+        long now = System.currentTimeMillis();
+        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();
+
+        //1) Find retirement candidates
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
+                ++count;
+                retirees.add(entry.getKey());
+            }
+        }
+        //2) Retire them
+        for(HiveEndPoint ep : retirees) {
+            try {
+                LOG.info("Closing idle Writer to Hive end point : {}", ep);
+                allWriters.remove(ep).close();
+            } catch (IOException e) {
+                LOG.warn("Failed to close writer for end point: {}. Error: "+ ep, e);
+            } catch (InterruptedException e) {
+                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
+                Thread.currentThread().interrupt();
+            }
+        }
+        return count;
+    }
+
+}

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The diff introduces changes that are directly related to security functionalities, specifically in the context of Kerberos authentication. The code checks for the presence of Kerberos principal and keytab configurations, enabling Kerberos authentication if both are provided. This is a critical security component as it affects how the system authenticates to Hive services, ensuring that data flows are secured and authenticated in environments that require Kerberos. Additionally, the handling of authentication failures and the setup for heartbeats in a Kerberos-enabled environment could impact the system's resilience to authentication issues and its ability to maintain authenticated sessions over time. The introduction of these changes requires careful review to ensure that the implementation does not introduce vulnerabilities, such as improper error handling or potential leaks of sensitive information in log files.",
  "confidence": 9
}
```

Diff number: 19
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/DelimitedRecordHiveMapper.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/DelimitedRecordHiveMapper.java
new file mode 100644
index 000000000..d5167952a
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/DelimitedRecordHiveMapper.java
@@ -0,0 +1,143 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.bolt.mapper;
+
+
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Tuple;
+import storm.trident.tuple.TridentTuple;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import org.apache.hive.hcatalog.streaming.DelimitedInputWriter;
+import org.apache.hive.hcatalog.streaming.HiveEndPoint;
+import org.apache.hive.hcatalog.streaming.RecordWriter;
+import org.apache.hive.hcatalog.streaming.StreamingException;
+import org.apache.hive.hcatalog.streaming.TransactionBatch;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Date;
+import java.text.SimpleDateFormat;
+import java.io.IOException;
+
+public class DelimitedRecordHiveMapper implements HiveMapper {
+    private static final Logger LOG = LoggerFactory.getLogger(DelimitedRecordHiveMapper.class);
+    private static final String DEFAULT_FIELD_DELIMITER = ",";
+    private Fields columnFields;
+    private Fields partitionFields;
+    private String[] columnNames;
+    private String timeFormat;
+    private String fieldDelimiter = DEFAULT_FIELD_DELIMITER;
+    private SimpleDateFormat parseDate;
+
+    public DelimitedRecordHiveMapper() {
+    }
+
+    public DelimitedRecordHiveMapper withColumnFields(Fields columnFields) {
+        this.columnFields = columnFields;
+        List<String> tempColumnNamesList = this.columnFields.toList();
+        columnNames = new String[tempColumnNamesList.size()];
+        tempColumnNamesList.toArray(columnNames);
+        return this;
+    }
+
+    public DelimitedRecordHiveMapper withPartitionFields(Fields partitionFields) {
+        this.partitionFields = partitionFields;
+        return this;
+    }
+
+    public DelimitedRecordHiveMapper withFieldDelimiter(String delimiter){
+        this.fieldDelimiter = delimiter;
+        return this;
+    }
+
+    public DelimitedRecordHiveMapper withTimeAsPartitionField(String timeFormat) {
+        this.timeFormat = timeFormat;
+        parseDate = new SimpleDateFormat(timeFormat);
+        return this;
+    }
+
+    @Override
+    public RecordWriter createRecordWriter(HiveEndPoint endPoint)
+        throws StreamingException, IOException, ClassNotFoundException {
+        return new DelimitedInputWriter(columnNames, fieldDelimiter,endPoint);
+    }
+
+    @Override
+    public void write(TransactionBatch txnBatch, Tuple tuple)
+        throws StreamingException, IOException, InterruptedException {
+        txnBatch.write(mapRecord(tuple));
+    }
+
+    @Override
+    public List<String> mapPartitions(Tuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(Tuple tuple) {
+        StringBuilder builder = new StringBuilder();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                builder.append(tuple.getValueByField(field));
+                builder.append(fieldDelimiter);
+            }
+        }
+        return builder.toString().getBytes();
+    }
+
+    @Override
+    public List<String> mapPartitions(TridentTuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(TridentTuple tuple) {
+        StringBuilder builder = new StringBuilder();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                builder.append(tuple.getValueByField(field));
+                builder.append(fieldDelimiter);
+            }
+        }
+        return builder.toString().getBytes();
+    }
+
+    private String getPartitionsByTimeFormat() {
+        Date d = new Date();
+        return parseDate.format(d.getTime());
+    }
+}

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new Java class, DelimitedRecordHiveMapper, which is primarily concerned with mapping and writing data to Hive using a delimited format. The changes involve the implementation of methods for creating record writers, writing transactions, mapping partitions, and mapping records, with customization options for column fields, partition fields, field delimiters, and time formats for partitioning. There are no direct modifications or interactions with authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The class deals with data formatting and transmission to Hive endpoints, which, while important for data processing workflows, does not inherently alter or impact the system's security posture. Therefore, the changes are classified as not directly relevant to security functionalities.",
  "confidence": 9
}
```

Diff number: 21
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java
new file mode 100644
index 000000000..ce3e4756b
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/bolt/mapper/JsonRecordHiveMapper.java
@@ -0,0 +1,132 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.bolt.mapper;
+
+
+import backtype.storm.tuple.Fields;
+import storm.trident.tuple.TridentTuple;
+import backtype.storm.tuple.Tuple;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.hive.hcatalog.streaming.HiveEndPoint;
+import org.apache.hive.hcatalog.streaming.RecordWriter;
+import org.apache.hive.hcatalog.streaming.StreamingException;
+import org.apache.hive.hcatalog.streaming.StrictJsonWriter;
+import org.apache.hive.hcatalog.streaming.TransactionBatch;
+import org.json.simple.JSONObject;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Date;
+import java.text.SimpleDateFormat;
+import java.io.IOException;
+
+public class JsonRecordHiveMapper implements HiveMapper {
+    private static final Logger LOG = LoggerFactory.getLogger(DelimitedRecordHiveMapper.class);
+    private Fields columnFields;
+    private Fields partitionFields;
+    private String timeFormat;
+    private SimpleDateFormat parseDate;
+
+    public JsonRecordHiveMapper() {
+    }
+
+    public JsonRecordHiveMapper withColumnFields(Fields columnFields) {
+        this.columnFields = columnFields;
+        return this;
+    }
+
+    public JsonRecordHiveMapper withPartitionFields(Fields partitionFields) {
+        this.partitionFields = partitionFields;
+        return this;
+    }
+
+    public JsonRecordHiveMapper withTimeAsPartitionField(String timeFormat) {
+        this.timeFormat = timeFormat;
+        parseDate = new SimpleDateFormat(timeFormat);
+        return this;
+    }
+
+    @Override
+    public RecordWriter createRecordWriter(HiveEndPoint endPoint)
+        throws StreamingException, IOException, ClassNotFoundException {
+        return new StrictJsonWriter(endPoint);
+    }
+
+    @Override
+    public void write(TransactionBatch txnBatch, Tuple tuple)
+        throws StreamingException, IOException, InterruptedException {
+        txnBatch.write(mapRecord(tuple));
+    }
+
+    @Override
+    public List<String> mapPartitions(Tuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(Tuple tuple) {
+        JSONObject obj = new JSONObject();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                obj.put(field,tuple.getValueByField(field));
+            }
+        }
+        return obj.toJSONString().getBytes();
+    }
+
+    @Override
+    public List<String> mapPartitions(TridentTuple tuple) {
+        List<String> partitionList = new ArrayList<String>();
+        if(this.partitionFields != null) {
+            for(String field: this.partitionFields) {
+                partitionList.add(tuple.getStringByField(field));
+            }
+        }
+        if (this.timeFormat != null) {
+            partitionList.add(getPartitionsByTimeFormat());
+        }
+        return partitionList;
+    }
+
+    @Override
+    public byte[] mapRecord(TridentTuple tuple) {
+        JSONObject obj = new JSONObject();
+        if(this.columnFields != null) {
+            for(String field: this.columnFields) {
+                obj.put(field,tuple.getValueByField(field));
+            }
+        }
+        return obj.toJSONString().getBytes();
+    }
+
+    private String getPartitionsByTimeFormat() {
+        Date d = new Date();
+        return parseDate.format(d.getTime());
+    }
+}

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new Java class, JsonRecordHiveMapper, within the Apache Storm project, specifically for the Hive integration. This class is primarily concerned with mapping JSON records to Hive table formats, including handling column and partition fields, and formatting time-based partition fields. The changes involve data formatting and serialization tasks, with no direct modifications to authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The class deals with data transformation and writing to Hive endpoints, which, while important for data processing workflows, does not inherently alter or impact the system's security posture. Therefore, the changes are classified as not directly relevant to security functionalities.",
  "confidence": 9
}
```

Diff number: 22
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java
new file mode 100644
index 000000000..d3162949a
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveOptions.java
@@ -0,0 +1,146 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.common;
+
+import java.io.Serializable;
+
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.storm.hive.bolt.mapper.HiveMapper;
+import org.apache.hive.hcatalog.streaming.*;
+
+
+public class HiveOptions implements Serializable {
+    protected HiveMapper mapper;
+    protected String databaseName;
+    protected String tableName;
+    protected String metaStoreURI;
+    protected Integer txnsPerBatch = 100;
+    protected Integer maxOpenConnections = 500;
+    protected Integer batchSize = 15000;
+    protected Integer idleTimeout = 0;
+    protected Integer callTimeout = 10000;
+    protected Integer heartBeatInterval = 240;
+    protected Boolean autoCreatePartitions = true;
+    protected String kerberosPrincipal;
+    protected String kerberosKeytab;
+
+    public HiveOptions(String metaStoreURI,String databaseName,String tableName,HiveMapper mapper) {
+        this.metaStoreURI = metaStoreURI;
+        this.databaseName = databaseName;
+        this.tableName = tableName;
+        this.mapper = mapper;
+    }
+
+    public HiveOptions withTxnsPerBatch(Integer txnsPerBatch) {
+        this.txnsPerBatch = txnsPerBatch;
+        return this;
+    }
+
+    public HiveOptions withMaxOpenConnections(Integer maxOpenConnections) {
+        this.maxOpenConnections = maxOpenConnections;
+        return this;
+    }
+
+    public HiveOptions withBatchSize(Integer batchSize) {
+        this.batchSize = batchSize;
+        return this;
+    }
+
+    public HiveOptions withIdleTimeout(Integer idleTimeout) {
+        this.idleTimeout = idleTimeout;
+        return this;
+    }
+
+    public HiveOptions withCallTimeout(Integer callTimeout) {
+        this.callTimeout = callTimeout;
+        return this;
+    }
+
+    public HiveOptions withHeartBeatInterval(Integer heartBeatInterval) {
+        this.heartBeatInterval = heartBeatInterval;
+        return this;
+    }
+
+    public HiveOptions withAutoCreatePartitions(Boolean autoCreatePartitions) {
+        this.autoCreatePartitions = autoCreatePartitions;
+        return this;
+    }
+
+    public HiveOptions withKerberosKeytab(String kerberosKeytab) {
+        this.kerberosKeytab = kerberosKeytab;
+        return this;
+    }
+
+    public HiveOptions withKerberosPrincipal(String kerberosPrincipal) {
+        this.kerberosPrincipal = kerberosPrincipal;
+        return this;
+    }
+
+    public String getMetaStoreURI() {
+        return metaStoreURI;
+    }
+
+    public String getDatabaseName() {
+        return databaseName;
+    }
+
+    public String getTableName() {
+        return tableName;
+    }
+
+    public HiveMapper getMapper() {
+        return mapper;
+    }
+
+    public Integer getBatchSize() {
+        return batchSize;
+    }
+
+    public Integer getCallTimeOut() {
+        return callTimeout;
+    }
+
+    public Integer getHeartBeatInterval() {
+        return heartBeatInterval;
+    }
+
+    public Integer getMaxOpenConnections() {
+        return maxOpenConnections;
+    }
+
+    public Integer getIdleTimeout() {
+        return idleTimeout;
+    }
+
+    public Integer getTxnsPerBatch() {
+        return txnsPerBatch;
+    }
+
+    public Boolean getAutoCreatePartitions() {
+        return autoCreatePartitions;
+    }
+
+    public String getKerberosPrincipal() {
+        return kerberosPrincipal;
+    }
+
+    public String getKerberosKeytab() {
+        return kerberosKeytab;
+    }
+}

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The introduction of the HiveOptions class in the diff file includes configurations for Kerberos authentication (kerberosPrincipal and kerberosKeytab). Kerberos is a network authentication protocol designed to provide strong authentication for client/server applications by using secret-key cryptography. The inclusion of Kerberos configuration options directly impacts the security functionality of the system by potentially altering the authentication flow. Specifically, the ability to configure a Kerberos principal and keytab file is crucial for enabling secure authentication to a Hive service in a Kerberized environment. This change suggests an enhancement or requirement for secure authentication mechanisms, which is directly relevant to the system's security posture. Additionally, the configuration options for transaction batches, connection limits, batch sizes, timeouts, and heartbeat intervals, while primarily operational, can indirectly influence the system's resilience to certain types of denial-of-service (DoS) attacks or system overloads, which also has a security dimension.",
  "confidence": 8
}
```

Diff number: 23
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveUtils.java b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveUtils.java
new file mode 100644
index 000000000..5483b077c
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/common/HiveUtils.java
@@ -0,0 +1,76 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.common;
+
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.storm.hive.bolt.mapper.HiveMapper;
+import org.apache.hive.hcatalog.streaming.*;
+
+import org.apache.hadoop.security.SecurityUtil;
+import org.apache.hadoop.security.UserGroupInformation;
+
+import java.util.List;
+import java.util.concurrent.ExecutorService;
+import java.io.File;
+import java.io.IOException;
+
+public class HiveUtils {
+
+    public static HiveEndPoint makeEndPoint(List<String> partitionVals, HiveOptions options) throws ConnectionError {
+        if(partitionVals==null) {
+            return new HiveEndPoint(options.getMetaStoreURI(), options.getDatabaseName(), options.getTableName(), null);
+        }
+        return new HiveEndPoint(options.getMetaStoreURI(), options.getDatabaseName(), options.getTableName(), partitionVals);
+    }
+
+    public static HiveWriter makeHiveWriter(HiveEndPoint endPoint, ExecutorService callTimeoutPool, UserGroupInformation ugi, HiveOptions options)
+        throws HiveWriter.ConnectFailure, InterruptedException {
+        return new HiveWriter(endPoint, options.getTxnsPerBatch(), options.getAutoCreatePartitions(),
+                              options.getCallTimeOut(), callTimeoutPool, options.getMapper(), ugi);
+    }
+
+    public static synchronized UserGroupInformation authenticate(String keytab, String principal)
+    throws AuthenticationFailed {
+        File kfile = new File(keytab);
+        if (!(kfile.isFile() && kfile.canRead())) {
+            throw new IllegalArgumentException("The keyTab file: "
+                                               + keytab + " is nonexistent or can't read. "
+                                               + "Please specify a readable keytab file for Kerberos auth.");
+        }
+        try {
+            principal = SecurityUtil.getServerPrincipal(principal, "");
+        } catch (Exception e) {
+            throw new AuthenticationFailed("Host lookup error when resolving principal " + principal, e);
+        }
+        try {
+            UserGroupInformation.loginUserFromKeytab(principal, keytab);
+            return UserGroupInformation.getLoginUser();
+        } catch (IOException e) {
+            throw new AuthenticationFailed("Login failed for principal " + principal, e);
+        }
+    }
+
+     public static class AuthenticationFailed extends Exception {
+         public AuthenticationFailed(String reason, Exception cause) {
+             super("Kerberos Authentication Failed. " + reason, cause);
+         }
+     }
+
+
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.bolt;

import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.SerializationError;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.Config;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.BatchHelper;
import org.apache.storm.utils.TupleUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveBolt extends BaseRichBolt {
    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
    @VisibleForTesting
    Map<HiveEndPoint, HiveWriter> allWriters;
    private OutputCollector collector;
    private HiveOptions options;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean sendHeartBeat = new AtomicBoolean(false);
    private UserGroupInformation ugi = null;
    private BatchHelper batchHelper;
    private boolean tokenAuthEnabled;

    public HiveBolt(HiveOptions options) {
        this.options = options;
    }

    @Override
    public void prepare(Map<String, Object> conf, TopologyContext topologyContext, OutputCollector collector) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            this.collector = collector;
            this.batchHelper = new BatchHelper(options.getBatchSize(), collector);
            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());

            sendHeartBeat.set(true);
            heartBeatTimer = new Timer(topologyContext.getThisTaskId() + "-hb-timer", true);
            setupHeartBeatTimer();

        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    @Override
    public void execute(Tuple tuple) {
        try {
            if (batchHelper.shouldHandle(tuple)) {
                List<String> partitionVals = options.getMapper().mapPartitions(tuple);
                HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
                HiveWriter writer = getOrCreateWriter(endPoint);
                writer.write(options.getMapper().mapRecord(tuple));
                batchHelper.addBatch(tuple);
            }

            if (batchHelper.shouldFlush()) {
                flushAllWriters(true);
                LOG.info("acknowledging tuples after writers flushed ");
                batchHelper.ack();
            }
            if (TupleUtils.isTick(tuple)) {
                retireIdleWriters();
            }
        } catch (SerializationError se) {
            LOG.info("Serialization exception occurred, tuple is acknowledged but not written to Hive.", tuple);
            this.collector.reportError(se);
            collector.ack(tuple);
        } catch (Exception e) {
            batchHelper.fail(e);
            abortAndCloseWriters();
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {

    }

    @Override
    public void cleanup() {
        sendHeartBeat.set(false);
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                HiveWriter w = entry.getValue();
                w.flushAndClose();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }

        callTimeoutPool = null;
        if (heartBeatTimer != null) {
            heartBeatTimer.cancel();
        }
        super.cleanup();
        LOG.info("Hive Bolt stopped");
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        Map<String, Object> conf = super.getComponentConfiguration();
        if (conf == null) {
            conf = new Config();
        }

        if (options.getTickTupleInterval() > 0) {
            conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, options.getTickTupleInterval());
        }

        return conf;
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat.get()) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    void flushAllWriters(boolean rollToNext)
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(rollToNext);
        }
    }

    void abortAndCloseWriters() {
        try {
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().abort();
            } catch (Exception e) {
                LOG.error("Failed to abort hive transaction batch, HiveEndPoint " + entry.getValue() + " due to exception ", e);
            }
        }
    }

    /**
     * Closes all writers and remove them from cache.
     */
    private void closeAllWriters() {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().close();
            } catch (Exception e) {
                LOG.warn("unable to close writers. ", e);
            }
        }
        //2) Clear cache
        allWriters.clear();
    }

    @VisibleForTesting
    HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.debug("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    LOG.info("cached HiveEndPoint size {} exceeded maxOpenConnections {} ", allWriters.size(),
                             options.getMaxOpenConnections());
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
                HiveUtils.logAllHiveEndPoints(allWriters);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }
    }

    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        LOG.info("Attempting close eldest writers");
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        LOG.info("Attempting close idle writers");
        int count = 0;
        long now = System.currentTimeMillis();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retire(entry.getKey());
            }
        }
        return count;
    }

    private void retire(HiveEndPoint ep) {
        try {
            HiveWriter writer = allWriters.remove(ep);
            if (writer != null) {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                writer.flushAndClose();
            }
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
        }
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.bolt;

import com.google.common.annotations.VisibleForTesting;
import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.SerializationError;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.Config;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.task.TopologyContext;
import org.apache.storm.topology.OutputFieldsDeclarer;
import org.apache.storm.topology.base.BaseRichBolt;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.utils.BatchHelper;
import org.apache.storm.utils.TupleUtils;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveBolt extends BaseRichBolt {
    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
    @VisibleForTesting
    Map<HiveEndPoint, HiveWriter> allWriters;
    private OutputCollector collector;
    private HiveOptions options;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean sendHeartBeat = new AtomicBoolean(false);
    private UserGroupInformation ugi = null;
    private BatchHelper batchHelper;
    private boolean tokenAuthEnabled;

    public HiveBolt(HiveOptions options) {
        this.options = options;
    }

    @Override
    public void prepare(Map<String, Object> conf, TopologyContext topologyContext, OutputCollector collector) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            this.collector = collector;
            this.batchHelper = new BatchHelper(options.getBatchSize(), collector);
            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());

            sendHeartBeat.set(true);
            heartBeatTimer = new Timer(topologyContext.getThisTaskId() + "-hb-timer", true);
            setupHeartBeatTimer();

        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    @Override
    public void execute(Tuple tuple) {
        try {
            if (batchHelper.shouldHandle(tuple)) {
                List<String> partitionVals = options.getMapper().mapPartitions(tuple);
                HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
                HiveWriter writer = getOrCreateWriter(endPoint);
                writer.write(options.getMapper().mapRecord(tuple));
                batchHelper.addBatch(tuple);
            }

            if (batchHelper.shouldFlush()) {
                flushAllWriters(true);
                LOG.info("acknowledging tuples after writers flushed ");
                batchHelper.ack();
            }
            if (TupleUtils.isTick(tuple)) {
                retireIdleWriters();
            }
        } catch (SerializationError se) {
            LOG.info("Serialization exception occurred, tuple is acknowledged but not written to Hive.", tuple);
            this.collector.reportError(se);
            collector.ack(tuple);
        } catch (Exception e) {
            batchHelper.fail(e);
            abortAndCloseWriters();
        }
    }

    @Override
    public void declareOutputFields(OutputFieldsDeclarer declarer) {

    }

    @Override
    public void cleanup() {
        sendHeartBeat.set(false);
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                HiveWriter w = entry.getValue();
                w.flushAndClose();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }

        callTimeoutPool = null;
        if (heartBeatTimer != null) {
            heartBeatTimer.cancel();
        }
        super.cleanup();
        LOG.info("Hive Bolt stopped");
    }

    @Override
    public Map<String, Object> getComponentConfiguration() {
        Map<String, Object> conf = super.getComponentConfiguration();
        if (conf == null) {
            conf = new Config();
        }

        if (options.getTickTupleInterval() > 0) {
            conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, options.getTickTupleInterval());
        }

        return conf;
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat.get()) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    void flushAllWriters(boolean rollToNext)
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(rollToNext);
        }
    }

    void abortAndCloseWriters() {
        try {
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().abort();
            } catch (Exception e) {
                LOG.error("Failed to abort hive transaction batch, HiveEndPoint " + entry.getValue() + " due to exception ", e);
            }
        }
    }

    /**
     * Closes all writers and remove them from cache.
     */
    private void closeAllWriters() {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                entry.getValue().close();
            } catch (Exception e) {
                LOG.warn("unable to close writers. ", e);
            }
        }
        //2) Clear cache
        allWriters.clear();
    }

    @VisibleForTesting
    HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.debug("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    LOG.info("cached HiveEndPoint size {} exceeded maxOpenConnections {} ", allWriters.size(),
                             options.getMaxOpenConnections());
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
                HiveUtils.logAllHiveEndPoints(allWriters);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }
    }

    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        LOG.info("Attempting close eldest writers");
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        LOG.info("Attempting close idle writers");
        int count = 0;
        long now = System.currentTimeMillis();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retire(entry.getKey());
            }
        }
        return count;
    }

    private void retire(HiveEndPoint ep) {
        try {
            HiveWriter writer = allWriters.remove(ep);
            if (writer != null) {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                writer.flushAndClose();
            }
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
        }
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.trident;

import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.IMetricsContext;
import org.apache.storm.topology.FailedException;
import org.apache.storm.trident.operation.TridentCollector;
import org.apache.storm.trident.state.State;
import org.apache.storm.trident.tuple.TridentTuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveState implements State {
    private static final Logger LOG = LoggerFactory.getLogger(HiveState.class);
    private HiveOptions options;
    private Integer currentBatchSize;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
    private Boolean sendHeartBeat = true;
    private UserGroupInformation ugi = null;
    private Boolean kerberosEnabled = false;
    private Map<HiveEndPoint, HiveWriter> allWriters;
    private boolean tokenAuthEnabled;

    public HiveState(HiveOptions options) {
        this.options = options;
        this.currentBatchSize = 0;
    }


    @Override
    public void beginCommit(Long txId) {
    }

    @Override
    public void commit(Long txId) {
        try {
            flushAllWriters();
            currentBatchSize = 0;
        } catch (HiveWriter.TxnFailure | InterruptedException | HiveWriter.CommitFailure | HiveWriter.TxnBatchFailure ex) {
            LOG.warn("Commit failed. Failing the batch.", ex);
            throw new FailedException(ex);
        }
    }

    public void prepare(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
            heartBeatTimer = new Timer("hive-hb-timer", true);
            setupHeartBeatTimer();
        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
        try {
            writeTuples(tuples);
        } catch (Exception e) {
            abortAndCloseWriters();
            LOG.warn("hive streaming failed.", e);
            throw new FailedException(e);
        }
    }

    private void writeTuples(List<TridentTuple> tuples)
        throws Exception {
        for (TridentTuple tuple : tuples) {
            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
            HiveWriter writer = getOrCreateWriter(endPoint);
            writer.write(options.getMapper().mapRecord(tuple));
            currentBatchSize++;
            if (currentBatchSize >= options.getBatchSize()) {
                flushAllWriters();
                currentBatchSize = 0;
            }
        }
    }

    private void abortAndCloseWriters() {
        try {
            sendHeartBeat = false;
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().abort();
        }
    }


    /**
     * Closes all writers and remove them from cache.
     * @return number of writers retired
     */
    private void closeAllWriters() throws InterruptedException, IOException {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().close();
        }
        //2) Clear cache
        allWriters.clear();
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void flushAllWriters()
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(true);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.info("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }

    }


    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        int count = 0;
        long now = System.currentTimeMillis();
        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retirees.add(entry.getKey());
            }
        }
        //2) Retire them
        for (HiveEndPoint ep : retirees) {
            try {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                allWriters.remove(ep).flushAndClose();
            } catch (IOException e) {
                LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
            } catch (InterruptedException e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
                Thread.currentThread().interrupt();
            } catch (Exception e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            }
        }
        return count;
    }

    public void cleanup() {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                sendHeartBeat = false;
                HiveWriter w = entry.getValue();
                LOG.info("Flushing writer to {}", w);
                w.flush(false);
                LOG.info("Closing writer to {}", w);
                w.close();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }
        heartBeatTimer.cancel();
        callTimeoutPool = null;
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.trident;

import com.google.common.util.concurrent.ThreadFactoryBuilder;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.Timer;
import java.util.TimerTask;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveUtils;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.IMetricsContext;
import org.apache.storm.topology.FailedException;
import org.apache.storm.trident.operation.TridentCollector;
import org.apache.storm.trident.state.State;
import org.apache.storm.trident.tuple.TridentTuple;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class HiveState implements State {
    private static final Logger LOG = LoggerFactory.getLogger(HiveState.class);
    private HiveOptions options;
    private Integer currentBatchSize;
    private ExecutorService callTimeoutPool;
    private transient Timer heartBeatTimer;
    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
    private Boolean sendHeartBeat = true;
    private UserGroupInformation ugi = null;
    private Boolean kerberosEnabled = false;
    private Map<HiveEndPoint, HiveWriter> allWriters;
    private boolean tokenAuthEnabled;

    public HiveState(HiveOptions options) {
        this.options = options;
        this.currentBatchSize = 0;
    }


    @Override
    public void beginCommit(Long txId) {
    }

    @Override
    public void commit(Long txId) {
        try {
            flushAllWriters();
            currentBatchSize = 0;
        } catch (HiveWriter.TxnFailure | InterruptedException | HiveWriter.CommitFailure | HiveWriter.TxnBatchFailure ex) {
            LOG.warn("Commit failed. Failing the batch.", ex);
            throw new FailedException(ex);
        }
    }

    public void prepare(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
        try {
            tokenAuthEnabled = HiveUtils.isTokenAuthEnabled(conf);
            try {
                ugi = HiveUtils.authenticate(tokenAuthEnabled, options.getKerberosKeytab(), options.getKerberosPrincipal());
            } catch (HiveUtils.AuthenticationFailed ex) {
                LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
                throw new IllegalArgumentException(ex);
            }

            allWriters = new ConcurrentHashMap<HiveEndPoint, HiveWriter>();
            String timeoutName = "hive-bolt-%d";
            this.callTimeoutPool = Executors.newFixedThreadPool(1,
                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
            heartBeatTimer = new Timer("hive-hb-timer", true);
            setupHeartBeatTimer();
        } catch (Exception e) {
            LOG.warn("unable to make connection to hive ", e);
        }
    }

    public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
        try {
            writeTuples(tuples);
        } catch (Exception e) {
            abortAndCloseWriters();
            LOG.warn("hive streaming failed.", e);
            throw new FailedException(e);
        }
    }

    private void writeTuples(List<TridentTuple> tuples)
        throws Exception {
        for (TridentTuple tuple : tuples) {
            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
            HiveWriter writer = getOrCreateWriter(endPoint);
            writer.write(options.getMapper().mapRecord(tuple));
            currentBatchSize++;
            if (currentBatchSize >= options.getBatchSize()) {
                flushAllWriters();
                currentBatchSize = 0;
            }
        }
    }

    private void abortAndCloseWriters() {
        try {
            sendHeartBeat = false;
            abortAllWriters();
            closeAllWriters();
        } catch (Exception ie) {
            LOG.warn("unable to close hive connections. ", ie);
        }
    }

    /**
     * Abort current Txn on all writers.
     */
    private void abortAllWriters() throws InterruptedException, StreamingException, HiveWriter.TxnBatchFailure {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().abort();
        }
    }


    /**
     * Closes all writers and remove them from cache.
     * @return number of writers retired
     */
    private void closeAllWriters() throws InterruptedException, IOException {
        //1) Retire writers
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            entry.getValue().close();
        }
        //2) Clear cache
        allWriters.clear();
    }

    private void setupHeartBeatTimer() {
        if (options.getHeartBeatInterval() > 0) {
            heartBeatTimer.schedule(new TimerTask() {
                @Override
                public void run() {
                    try {
                        if (sendHeartBeat) {
                            LOG.debug("Start sending heartbeat on all writers");
                            sendHeartBeatOnAllWriters();
                            setupHeartBeatTimer();
                        }
                    } catch (Exception e) {
                        LOG.warn("Failed to heartbeat on HiveWriter ", e);
                    }
                }
            }, options.getHeartBeatInterval() * 1000);
        }
    }

    private void flushAllWriters()
        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.flush(true);
        }
    }

    private void sendHeartBeatOnAllWriters() throws InterruptedException {
        for (HiveWriter writer : allWriters.values()) {
            writer.heartBeat();
        }
    }

    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
        throws HiveWriter.ConnectFailure, InterruptedException {
        try {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                LOG.info("Creating Writer to Hive end point : " + endPoint);
                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options, tokenAuthEnabled);
                if (allWriters.size() > (options.getMaxOpenConnections() - 1)) {
                    int retired = retireIdleWriters();
                    if (retired == 0) {
                        retireEldestWriter();
                    }
                }
                allWriters.put(endPoint, writer);
            }
            return writer;
        } catch (HiveWriter.ConnectFailure e) {
            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
            throw e;
        }

    }


    /**
     * Locate writer that has not been used for longest time and retire it.
     */
    private void retireEldestWriter() {
        long oldestTimeStamp = System.currentTimeMillis();
        HiveEndPoint eldest = null;
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (entry.getValue().getLastUsed() < oldestTimeStamp) {
                eldest = entry.getKey();
                oldestTimeStamp = entry.getValue().getLastUsed();
            }
        }
        try {
            LOG.info("Closing least used Writer to Hive end point : " + eldest);
            allWriters.remove(eldest).flushAndClose();
        } catch (IOException e) {
            LOG.warn("Failed to close writer for end point: " + eldest, e);
        } catch (InterruptedException e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
            Thread.currentThread().interrupt();
        } catch (Exception e) {
            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
        }
    }

    /**
     * Locate all writers past idle timeout and retire them.
     * @return number of writers retired
     */
    private int retireIdleWriters() {
        int count = 0;
        long now = System.currentTimeMillis();
        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();

        //1) Find retirement candidates
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            if (now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
                ++count;
                retirees.add(entry.getKey());
            }
        }
        //2) Retire them
        for (HiveEndPoint ep : retirees) {
            try {
                LOG.info("Closing idle Writer to Hive end point : {}", ep);
                allWriters.remove(ep).flushAndClose();
            } catch (IOException e) {
                LOG.warn("Failed to close writer for end point: {}. Error: " + ep, e);
            } catch (InterruptedException e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
                Thread.currentThread().interrupt();
            } catch (Exception e) {
                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
            }
        }
        return count;
    }

    public void cleanup() {
        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
            try {
                sendHeartBeat = false;
                HiveWriter w = entry.getValue();
                LOG.info("Flushing writer to {}", w);
                w.flush(false);
                LOG.info("Closing writer to {}", w);
                w.close();
            } catch (Exception ex) {
                LOG.warn("Error while closing writer to " + entry.getKey() + ". Exception follows.",
                        ex);
                if (ex instanceof InterruptedException) {
                    Thread.currentThread().interrupt();
                }
            }
        }

        ExecutorService[] toShutdown = { callTimeoutPool };
        for (ExecutorService execService : toShutdown) {
            execService.shutdown();
            try {
                while (!execService.isTerminated()) {
                    execService.awaitTermination(
                        options.getCallTimeOut(), TimeUnit.MILLISECONDS);
                }
            } catch (InterruptedException ex) {
                LOG.warn("shutdown interrupted on " + execService, ex);
            }
        }
        heartBeatTimer.cancel();
        callTimeoutPool = null;
    }

}

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The provided diff file introduces changes that are directly related to security functionalities, specifically in the context of authentication mechanisms. The addition of the 'authenticate' method within the HiveUtils class implements Kerberos authentication for Hive connections, which is a critical security component. Kerberos is a network authentication protocol designed to provide strong authentication for client/server applications by using secret-key cryptography. The method checks for the existence and readability of the keytab file, resolves the principal name, and attempts to log in a user from the keytab file. If any of these steps fail, it throws an AuthenticationFailed exception, indicating a failure in the authentication process. This change directly impacts the system's security by enforcing authenticated access to Hive, ensuring that only authorized users can perform actions on the Hive database. The use of Kerberos authentication is a significant security enhancement as it mitigates the risk of unauthorized access. However, the security of this implementation also depends on the proper configuration and management of Kerberos principals and keytab files, as well as the secure storage of these files to prevent unauthorized access.",
  "confidence": 9
}
```

Diff number: 25
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java
new file mode 100644
index 000000000..6050aa886
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveState.java
@@ -0,0 +1,306 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.storm.hive.trident;
+
+import storm.trident.operation.TridentCollector;
+import storm.trident.state.State;
+import storm.trident.tuple.TridentTuple;
+import backtype.storm.task.IMetricsContext;
+import backtype.storm.topology.FailedException;
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.storm.hive.common.HiveWriter;
+import org.apache.hive.hcatalog.streaming.*;
+import org.apache.storm.hive.common.HiveOptions;
+import org.apache.storm.hive.common.HiveUtils;
+import org.apache.hadoop.security.UserGroupInformation;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.net.URI;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.Timer;
+import java.util.TimerTask;
+import java.util.Map.Entry;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class HiveState implements State {
+    private static final Logger LOG = LoggerFactory.getLogger(HiveState.class);
+    private HiveOptions options;
+    private Integer currentBatchSize;
+    private ExecutorService callTimeoutPool;
+    private transient Timer heartBeatTimer;
+    private AtomicBoolean timeToSendHeartBeat = new AtomicBoolean(false);
+    private UserGroupInformation ugi = null;
+    private Boolean kerberosEnabled = false;
+    HashMap<HiveEndPoint, HiveWriter> allWriters;
+
+    public HiveState(HiveOptions options) {
+        this.options = options;
+        this.currentBatchSize = 0;
+    }
+
+
+    @Override
+    public void beginCommit(Long txId) {
+    }
+
+    @Override
+    public void commit(Long txId) {
+    }
+
+    public void prepare(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions)  {
+        try {
+            if(options.getKerberosPrincipal() == null && options.getKerberosKeytab() == null) {
+                kerberosEnabled = false;
+            } else if(options.getKerberosPrincipal() != null && options.getKerberosKeytab() != null) {
+                kerberosEnabled = true;
+            } else {
+                throw new IllegalArgumentException("To enable Kerberos, need to set both KerberosPrincipal " +
+                                                   " & KerberosKeytab");
+            }
+
+            if (kerberosEnabled) {
+                try {
+                    ugi = HiveUtils.authenticate(options.getKerberosKeytab(), options.getKerberosPrincipal());
+                } catch(HiveUtils.AuthenticationFailed ex) {
+                    LOG.error("Hive kerberos authentication failed " + ex.getMessage(), ex);
+                    throw new IllegalArgumentException(ex);
+                }
+            }
+
+            allWriters = new HashMap<HiveEndPoint,HiveWriter>();
+            String timeoutName = "hive-bolt-%d";
+            this.callTimeoutPool = Executors.newFixedThreadPool(1,
+                                                                new ThreadFactoryBuilder().setNameFormat(timeoutName).build());
+            heartBeatTimer= new Timer();
+            setupHeartBeatTimer();
+        } catch(Exception e) {
+            LOG.warn("unable to make connection to hive ",e);
+        }
+    }
+
+    public void updateState(List<TridentTuple> tuples, TridentCollector collector) {
+        try {
+            writeTuples(tuples);
+        } catch (Exception e) {
+            abortAndCloseWriters();
+            LOG.warn("hive streaming failed.",e);
+            throw new FailedException(e);
+        }
+    }
+
+    private void writeTuples(List<TridentTuple> tuples)
+        throws Exception {
+        if(timeToSendHeartBeat.compareAndSet(true, false)) {
+            enableHeartBeatOnAllWriters();
+        }
+        for (TridentTuple tuple : tuples) {
+            List<String> partitionVals = options.getMapper().mapPartitions(tuple);
+            HiveEndPoint endPoint = HiveUtils.makeEndPoint(partitionVals, options);
+            HiveWriter writer = getOrCreateWriter(endPoint);
+            writer.write(options.getMapper().mapRecord(tuple));
+            currentBatchSize++;
+            if(currentBatchSize >= options.getBatchSize()) {
+                flushAllWriters();
+                currentBatchSize = 0;
+            }
+        }
+    }
+
+    private void abortAndCloseWriters() {
+        try {
+            abortAllWriters();
+            closeAllWriters();
+        } catch(InterruptedException e) {
+            LOG.warn("unable to close hive connections. ", e);
+        } catch(IOException ie) {
+            LOG.warn("unable to close hive connections. ", ie);
+        }
+    }
+
+    /**
+     * Abort current Txn on all writers
+     * @return number of writers retired
+     */
+    private void abortAllWriters() throws InterruptedException {
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            entry.getValue().abort();
+        }
+    }
+
+
+    /**
+     * Closes all writers and remove them from cache
+     * @return number of writers retired
+     */
+    private void closeAllWriters() throws InterruptedException, IOException {
+        //1) Retire writers
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            entry.getValue().close();
+        }
+        //2) Clear cache
+        allWriters.clear();
+    }
+
+    private void setupHeartBeatTimer() {
+        if(options.getHeartBeatInterval()>0) {
+            heartBeatTimer.schedule(new TimerTask() {
+                    @Override
+                    public void run() {
+                        timeToSendHeartBeat.set(true);
+                        setupHeartBeatTimer();
+                    }
+                }, options.getHeartBeatInterval() * 1000);
+        }
+    }
+
+    private void flushAllWriters()
+        throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure, InterruptedException {
+        for(HiveWriter writer: allWriters.values()) {
+            writer.flush(true);
+        }
+    }
+
+    private void enableHeartBeatOnAllWriters() {
+        for (HiveWriter writer : allWriters.values()) {
+            writer.setHeartBeatNeeded();
+        }
+    }
+
+    private HiveWriter getOrCreateWriter(HiveEndPoint endPoint)
+        throws HiveWriter.ConnectFailure, InterruptedException {
+        try {
+            HiveWriter writer = allWriters.get( endPoint );
+            if( writer == null ) {
+                LOG.info("Creating Writer to Hive end point : " + endPoint);
+                writer = HiveUtils.makeHiveWriter(endPoint, callTimeoutPool, ugi, options);
+                if(allWriters.size() > options.getMaxOpenConnections()){
+                    int retired = retireIdleWriters();
+                    if(retired==0) {
+                        retireEldestWriter();
+                    }
+                }
+                allWriters.put(endPoint, writer);
+            }
+            return writer;
+        } catch (HiveWriter.ConnectFailure e) {
+            LOG.error("Failed to create HiveWriter for endpoint: " + endPoint, e);
+            throw e;
+        }
+
+    }
+
+
+
+    /**
+     * Locate writer that has not been used for longest time and retire it
+     */
+    private void retireEldestWriter() {
+        long oldestTimeStamp = System.currentTimeMillis();
+        HiveEndPoint eldest = null;
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(entry.getValue().getLastUsed() < oldestTimeStamp) {
+                eldest = entry.getKey();
+                oldestTimeStamp = entry.getValue().getLastUsed();
+            }
+        }
+        try {
+            LOG.info("Closing least used Writer to Hive end point : " + eldest);
+            allWriters.remove(eldest).close();
+        } catch (IOException e) {
+            LOG.warn("Failed to close writer for end point: " + eldest, e);
+        } catch (InterruptedException e) {
+            LOG.warn("Interrupted when attempting to close writer for end point: " + eldest, e);
+            Thread.currentThread().interrupt();
+        }
+    }
+
+    /**
+     * Locate all writers past idle timeout and retire them
+     * @return number of writers retired
+     */
+    private int retireIdleWriters() {
+        int count = 0;
+        long now = System.currentTimeMillis();
+        ArrayList<HiveEndPoint> retirees = new ArrayList<HiveEndPoint>();
+
+        //1) Find retirement candidates
+        for (Entry<HiveEndPoint,HiveWriter> entry : allWriters.entrySet()) {
+            if(now - entry.getValue().getLastUsed() > options.getIdleTimeout()) {
+                ++count;
+                retirees.add(entry.getKey());
+            }
+        }
+        //2) Retire them
+        for(HiveEndPoint ep : retirees) {
+            try {
+                LOG.info("Closing idle Writer to Hive end point : {}", ep);
+                allWriters.remove(ep).close();
+            } catch (IOException e) {
+                LOG.warn("Failed to close writer for end point: {}. Error: "+ ep, e);
+            } catch (InterruptedException e) {
+                LOG.warn("Interrupted when attempting to close writer for end point: " + ep, e);
+                Thread.currentThread().interrupt();
+            }
+        }
+        return count;
+    }
+
+    public void cleanup() {
+        for (Entry<HiveEndPoint, HiveWriter> entry : allWriters.entrySet()) {
+            try {
+                HiveWriter w = entry.getValue();
+                LOG.info("Flushing writer to {}", w);
+                w.flush(false);
+                LOG.info("Closing writer to {}", w);
+                w.close();
+            } catch (Exception ex) {
+                LOG.warn("Error while closing writer to " + entry.getKey() +
+                         ". Exception follows.", ex);
+                if (ex instanceof InterruptedException) {
+                    Thread.currentThread().interrupt();
+                }
+            }
+        }
+
+        ExecutorService toShutdown[] = {callTimeoutPool};
+        for (ExecutorService execService : toShutdown) {
+            execService.shutdown();
+            try {
+                while (!execService.isTerminated()) {
+                    execService.awaitTermination(
+                                                 options.getCallTimeOut(), TimeUnit.MILLISECONDS);
+                }
+            } catch (InterruptedException ex) {
+                LOG.warn("shutdown interrupted on " + execService, ex);
+            }
+        }
+        callTimeoutPool = null;
+    }
+
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.trident;

import java.util.Map;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.task.IMetricsContext;
import org.apache.storm.trident.state.State;
import org.apache.storm.trident.state.StateFactory;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


public class HiveStateFactory implements StateFactory {
    private static final Logger LOG = LoggerFactory.getLogger(HiveStateFactory.class);
    private HiveOptions options;

    public HiveStateFactory() {}

    /**
     * The options for connecting to Hive.
     */
    public HiveStateFactory withOptions(HiveOptions options) {
        if (options.getTickTupleInterval() != HiveOptions.DEFAULT_TICK_TUPLE_INTERVAL_SECS) {
            LOG.error("Tick tuple interval will be ignored for trident."
                    + " The Hive writers are flushed after each batch.");
        }
        this.options = options;
        return this;
    }

    @Override
    public State makeState(Map<String, Object> conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
        LOG.info("makeState(partitonIndex={}, numpartitions={}", partitionIndex, numPartitions);
        HiveState state = new HiveState(this.options);
        state.prepare(conf, metrics, partitionIndex, numPartitions);
        return state;
    }
}

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The provided diff file introduces changes that are directly relevant to system security, particularly in the context of authentication mechanisms and potentially the security of communications. The code modifications include the implementation of a Kerberos authentication mechanism for Hive connections, as indicated by the checks for `options.getKerberosPrincipal()` and `options.getKerberosKeytab()`, and the subsequent authentication logic using `HiveUtils.authenticate()`. Kerberos is a network authentication protocol designed to provide strong authentication for client/server applications by using secret-key cryptography. The correct implementation and configuration of Kerberos are crucial for ensuring the security of the system's communications with Hive. Additionally, the introduction of a heartbeat mechanism (`setupHeartBeatTimer()`, `enableHeartBeatOnAllWriters()`) could have implications for the system's resilience to certain types of denial-of-service (DoS) attacks, although this is more tangential to direct security functionalities. However, the primary security concern here is the proper handling of Kerberos authentication, which, if misconfigured or improperly managed, could lead to unauthorized access or compromise of the Hive connection. Given the information in the diff, there is a high confidence in the assessment that these changes are security-relevant, particularly due to the introduction of Kerberos authentication.",
  "confidence": 9
}
```

Diff number: 26
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java
new file mode 100644
index 000000000..8f3b9e94d
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveStateFactory.java
@@ -0,0 +1,31 @@
+package org.apache.storm.hive.trident;
+
+import backtype.storm.task.IMetricsContext;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+import storm.trident.state.State;
+import storm.trident.state.StateFactory;
+import org.apache.storm.hive.common.HiveOptions;
+
+import java.util.Map;
+
+
+public class HiveStateFactory implements StateFactory {
+    private static final Logger LOG = LoggerFactory.getLogger(HiveStateFactory.class);
+    private HiveOptions options;
+
+    public HiveStateFactory(){}
+
+    public HiveStateFactory withOptions(HiveOptions options){
+        this.options = options;
+        return this;
+    }
+
+    @Override
+    public State makeState(Map conf, IMetricsContext metrics, int partitionIndex, int numPartitions) {
+        LOG.info("makeState(partitonIndex={}, numpartitions={}", partitionIndex, numPartitions);
+        HiveState state = new HiveState(this.options);
+        state.prepare(conf, metrics, partitionIndex, numPartitions);
+        return state;
+    }
+}

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new file, HiveStateFactory.java, which is part of the Apache Storm project, specifically for integration with Hive. The changes involve the creation of a factory class for generating HiveState instances, which are likely used for managing state information in interactions with a Hive database. The modifications include basic class setup, logging configurations, and a method for creating HiveState instances with specific configurations. There is no direct manipulation or mention of authentication mechanisms, encryption algorithms, access control procedures, or modifications to logging levels related to security events. The logging that is introduced is for informational purposes (e.g., logging the state creation process) and does not pertain to security events. Therefore, these changes are unrelated to the critical security components listed and do not directly impact the system's security posture.",
  "confidence": 9
}
```

Diff number: 27
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java
new file mode 100644
index 000000000..b0b32f10f
--- /dev/null
+++ b/external/storm-hive/src/main/java/org/apache/storm/hive/trident/HiveUpdater.java
@@ -0,0 +1,14 @@
+package org.apache.storm.hive.trident;
+
+import storm.trident.operation.TridentCollector;
+import storm.trident.state.BaseStateUpdater;
+import storm.trident.tuple.TridentTuple;
+
+import java.util.List;
+
+public class HiveUpdater extends BaseStateUpdater<HiveState>{
+    @Override
+    public void updateState(HiveState state, List<TridentTuple> tuples, TridentCollector collector) {
+        state.updateState(tuples, collector);
+    }
+}

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new Java class, HiveUpdater, which extends BaseStateUpdater to update the state in a Hive database using the Trident API in Apache Storm. The changes involve the implementation of the updateState method, which takes a HiveState object, a list of TridentTuples, and a TridentCollector as parameters to update the state in Hive. There are no direct modifications or references to authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The primary functionality is centered around data state management within a Hive database through the Storm framework, which does not inherently alter or impact the system's security posture. Therefore, the changes are unrelated to security concerns and focus on data processing and state management.",
  "confidence": 9
}
```

Diff number: 28
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveSetupUtil.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveSetupUtil.java
new file mode 100644
index 000000000..d49281993
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveSetupUtil.java
@@ -0,0 +1,220 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import org.apache.hadoop.fs.FileStatus;
+import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.fs.RawLocalFileSystem;
+import org.apache.hadoop.fs.permission.FsPermission;
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
+import org.apache.hadoop.hive.metastore.IMetaStoreClient;
+import org.apache.hadoop.hive.metastore.TableType;
+import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
+import org.apache.hadoop.hive.metastore.api.Database;
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.apache.hadoop.hive.metastore.api.MetaException;
+import org.apache.hadoop.hive.metastore.api.Partition;
+import org.apache.hadoop.hive.metastore.api.SerDeInfo;
+import org.apache.hadoop.hive.metastore.api.StorageDescriptor;
+import org.apache.hadoop.hive.metastore.api.Table;
+import org.apache.hadoop.hive.ql.io.orc.OrcInputFormat;
+import org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat;
+import org.apache.hadoop.hive.ql.io.orc.OrcSerde;
+import org.apache.hadoop.hive.ql.metadata.HiveException;
+import org.apache.hadoop.hive.serde.serdeConstants;
+import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.thrift.TException;
+
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.net.URI;
+import java.net.URISyntaxException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+public class HiveSetupUtil {
+    public static class RawFileSystem extends RawLocalFileSystem {
+        private static final URI NAME;
+        static {
+            try {
+                NAME = new URI("raw:///");
+            } catch (URISyntaxException se) {
+                throw new IllegalArgumentException("bad uri", se);
+            }
+        }
+
+        @Override
+        public URI getUri() {
+            return NAME;
+        }
+
+        @Override
+        public FileStatus getFileStatus(Path path) throws IOException {
+            File file = pathToFile(path);
+            if (!file.exists()) {
+                throw new FileNotFoundException("Can't find " + path);
+            }
+            // get close enough
+            short mod = 0;
+            if (file.canRead()) {
+                mod |= 0444;
+            }
+            if (file.canWrite()) {
+                mod |= 0200;
+            }
+            if (file.canExecute()) {
+                mod |= 0111;
+            }
+            ShimLoader.getHadoopShims();
+            return new FileStatus(file.length(), file.isDirectory(), 1, 1024,
+                                  file.lastModified(), file.lastModified(),
+                                  FsPermission.createImmutable(mod), "owen", "users", path);
+        }
+    }
+
+    private final static String txnMgr = "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager";
+
+    public static HiveConf getHiveConf() {
+        HiveConf conf = new HiveConf();
+        // String metastoreDBLocation = "jdbc:derby:databaseName=/tmp/metastore_db;create=true";
+        // conf.set("javax.jdo.option.ConnectionDriverName","org.apache.derby.jdbc.EmbeddedDriver");
+        // conf.set("javax.jdo.option.ConnectionURL",metastoreDBLocation);
+        conf.set("fs.raw.impl", RawFileSystem.class.getName());
+        conf.setVar(HiveConf.ConfVars.HIVE_TXN_MANAGER, txnMgr);
+        conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, true);
+        return conf;
+    }
+
+    public static void createDbAndTable(HiveConf conf, String databaseName,
+                                        String tableName, List<String> partVals,
+                                        String[] colNames, String[] colTypes,
+                                        String[] partNames, String dbLocation)
+        throws Exception {
+        IMetaStoreClient client = new HiveMetaStoreClient(conf);
+        try {
+            Database db = new Database();
+            db.setName(databaseName);
+            db.setLocationUri(dbLocation);
+            client.createDatabase(db);
+
+            Table tbl = new Table();
+            tbl.setDbName(databaseName);
+            tbl.setTableName(tableName);
+            tbl.setTableType(TableType.MANAGED_TABLE.toString());
+            StorageDescriptor sd = new StorageDescriptor();
+            sd.setCols(getTableColumns(colNames, colTypes));
+            sd.setNumBuckets(1);
+            sd.setLocation(dbLocation + Path.SEPARATOR + tableName);
+            if(partNames!=null && partNames.length!=0) {
+                tbl.setPartitionKeys(getPartitionKeys(partNames));
+            }
+
+            tbl.setSd(sd);
+
+            sd.setBucketCols(new ArrayList<String>(2));
+            sd.setSerdeInfo(new SerDeInfo());
+            sd.getSerdeInfo().setName(tbl.getTableName());
+            sd.getSerdeInfo().setParameters(new HashMap<String, String>());
+            sd.getSerdeInfo().getParameters().put(serdeConstants.SERIALIZATION_FORMAT, "1");
+
+            sd.getSerdeInfo().setSerializationLib(OrcSerde.class.getName());
+            sd.setInputFormat(OrcInputFormat.class.getName());
+            sd.setOutputFormat(OrcOutputFormat.class.getName());
+
+            Map<String, String> tableParams = new HashMap<String, String>();
+            tbl.setParameters(tableParams);
+            client.createTable(tbl);
+            try {
+                if(partVals!=null && partVals.size() > 0) {
+                    addPartition(client, tbl, partVals);
+                }
+            } catch(AlreadyExistsException e) {
+            }
+        } finally {
+            client.close();
+        }
+    }
+
+    // delete db and all tables in it
+    public static void dropDB(HiveConf conf, String databaseName) throws HiveException, MetaException {
+        IMetaStoreClient client = new HiveMetaStoreClient(conf);
+        try {
+            for (String table : client.listTableNamesByFilter(databaseName, "", (short) -1)) {
+                client.dropTable(databaseName, table, true, true);
+            }
+            client.dropDatabase(databaseName);
+        } catch (TException e) {
+            client.close();
+        }
+    }
+
+    private static void addPartition(IMetaStoreClient client, Table tbl
+                                     , List<String> partValues)
+        throws IOException, TException {
+        Partition part = new Partition();
+        part.setDbName(tbl.getDbName());
+        part.setTableName(tbl.getTableName());
+        StorageDescriptor sd = new StorageDescriptor(tbl.getSd());
+        sd.setLocation(sd.getLocation() + Path.SEPARATOR + makePartPath(tbl.getPartitionKeys(), partValues));
+        part.setSd(sd);
+        part.setValues(partValues);
+        client.add_partition(part);
+    }
+
+    private static String makePartPath(List<FieldSchema> partKeys, List<String> partVals) {
+        if(partKeys.size()!=partVals.size()) {
+            throw new IllegalArgumentException("Partition values:" + partVals +
+                                               ", does not match the partition Keys in table :" + partKeys );
+        }
+        StringBuffer buff = new StringBuffer(partKeys.size()*20);
+        int i=0;
+        for(FieldSchema schema : partKeys) {
+            buff.append(schema.getName());
+            buff.append("=");
+            buff.append(partVals.get(i));
+            if(i!=partKeys.size()-1) {
+                buff.append(Path.SEPARATOR);
+            }
+            ++i;
+        }
+        return buff.toString();
+    }
+
+    private static List<FieldSchema> getTableColumns(String[] colNames, String[] colTypes) {
+        List<FieldSchema> fields = new ArrayList<FieldSchema>();
+        for (int i=0; i<colNames.length; ++i) {
+            fields.add(new FieldSchema(colNames[i], colTypes[i], ""));
+        }
+        return fields;
+    }
+
+    private static List<FieldSchema> getPartitionKeys(String[] partNames) {
+        List<FieldSchema> fields = new ArrayList<FieldSchema>();
+        for (int i=0; i < partNames.length; ++i) {
+           fields.add(new FieldSchema(partNames[i], serdeConstants.STRING_TYPE_NAME, ""));
+        }
+        return fields;
+    }
+
+}

```
Context:
```
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.bolt;

import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import com.google.common.collect.Lists;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Set;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.txn.TxnDbUtil;
import org.apache.hadoop.hive.serde.serdeConstants;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.storm.Config;
import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
import org.apache.storm.hive.bolt.mapper.JsonRecordHiveMapper;
import org.apache.storm.hive.common.HiveOptions;
import org.apache.storm.hive.common.HiveWriter;
import org.apache.storm.task.GeneralTopologyContext;
import org.apache.storm.task.OutputCollector;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.TupleImpl;
import org.apache.storm.tuple.Values;
import org.apache.storm.utils.MockTupleHelpers;
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.mockito.Mock;
import org.mockito.Mockito;
import org.mockito.MockitoAnnotations;
import org.mockito.invocation.InvocationOnMock;
import org.mockito.stubbing.Answer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.Mockito.any;
import static org.mockito.Mockito.never;
import static org.mockito.Mockito.verify;
import static org.mockito.Mockito.verifyNoInteractions;

public class TestHiveBolt {
    final static String dbName = "testdb";
    final static String tblName = "test_table";
    final static String dbName1 = "testdb1";
    final static String tblName1 = "test_table1";
    final static String PART1_NAME = "city";
    final static String PART2_NAME = "state";
    final static String[] partNames = { PART1_NAME, PART2_NAME };
    private static final String COL1 = "id";
    private static final String COL2 = "msg";
    private static final Logger LOG = LoggerFactory.getLogger(HiveBolt.class);
    final String partitionVals = "sunnyvale,ca";
    final String[] colNames = { COL1, COL2 };
    final String[] colNames1 = { COL2, COL1 };
    final String metaStoreURI;
    private final HiveConf conf;
    private String[] colTypes = { serdeConstants.INT_TYPE_NAME, serdeConstants.STRING_TYPE_NAME };
    private Config config = new Config();
    private TestingHiveBolt bolt;
    ;
    private ObjectMapper objectMapper = new ObjectMapper();
    @Mock
    private OutputCollector collector;

    public TestHiveBolt() throws Exception {
        //metaStoreURI = "jdbc:derby:;databaseName="+System.getProperty("java.io.tmpdir") +"metastore_db;create=true";
        metaStoreURI = null;
        conf = HiveSetupUtil.getHiveConf();
        TxnDbUtil.setConfValues(conf);
        if (metaStoreURI != null) {
            conf.setVar(HiveConf.ConfVars.METASTOREURIS, metaStoreURI);
        }
    }

    @BeforeEach
    public void setup() throws Exception {
        MockitoAnnotations.initMocks(this);
    }

    @Test
    public void testWithByteArrayIdandMessage()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 100;
        String msg = "test-123";
        String city = "sunnyvale";
        String state = "ca";

        Set<Tuple> tupleSet = new HashSet<Tuple>();
        for (int i = 0; i < 4; i++) {
            Tuple tuple = generateTestTuple(id, msg, city, state);
            bolt.execute(tuple);
            tupleSet.add(tuple);
        }

        List<String> partVals = Lists.newArrayList(city, state);

        for (Tuple t : tupleSet) {
            verify(collector).ack(t);
        }

        assertEquals(4, bolt.getRecordWritten(partVals).size());

        bolt.cleanup();
    }

    @Test
    public void testWithoutPartitions()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName1, tblName1, mapper)
            .withTxnsPerBatch(2).withBatchSize(2).withAutoCreatePartitions(false);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 100;
        String msg = "test-123";
        String city = "sunnyvale";
        String state = "ca";

        Set<Tuple> tupleSet = new HashSet<Tuple>();
        for (int i = 0; i < 4; i++) {
            Tuple tuple = generateTestTuple(id, msg, city, state);
            bolt.execute(tuple);
            tupleSet.add(tuple);
        }

        List<String> partVals = Collections.emptyList();

        for (Tuple t : tupleSet) {
            verify(collector).ack(t);
        }

        List<byte[]> recordWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordWritten);
        assertEquals(4, recordWritten.size());

        bolt.cleanup();
    }

    @Test
    public void testWithTimeformat()
        throws Exception {
        String timeFormat = "yyyy/MM/dd";
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withTimeAsPartitionField(timeFormat);
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName1, tblName1, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(1)
            .withMaxOpenConnections(1);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 100;
        String msg = "test-123";
        Date d = new Date();
        SimpleDateFormat parseDate = new SimpleDateFormat(timeFormat);
        String today = parseDate.format(d.getTime());

        List<Tuple> tuples = new ArrayList<>();
        for (int i = 0; i < 2; i++) {
            Tuple tuple = generateTestTuple(id, msg, null, null);
            tuples.add(tuple);
            bolt.execute(tuple);
        }

        for (Tuple t : tuples) {
            verify(collector).ack(t);
        }

        List<String> partVals = Lists.newArrayList(today);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(2, recordsWritten.size());

        byte[] mapped = generateDelimiteredRecord(Lists.newArrayList(id, msg), mapper.getFieldDelimiter());

        for (byte[] record : recordsWritten) {
            assertArrayEquals(mapped, record);
        }

        bolt.cleanup();
    }

    @Test
    public void testData()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(1);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Integer id = 1;
        String msg = "SJC";
        String city = "Sunnyvale";
        String state = "CA";

        Tuple tuple1 = generateTestTuple(id, msg, city, state);

        bolt.execute(tuple1);
        verify(collector).ack(tuple1);

        List<String> partVals = Lists.newArrayList(city, state);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(1, recordsWritten.size());

        byte[] mapped = generateDelimiteredRecord(Lists.newArrayList(id, msg), mapper.getFieldDelimiter());
        assertArrayEquals(mapped, recordsWritten.get(0));

        bolt.cleanup();
    }

    @Test
    public void testJsonWriter()
        throws Exception {
        // json record doesn't need columns to be in the same order
        // as table in hive.
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(1);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, collector);

        Integer id = 1;
        String msg = "SJC";
        String city = "Sunnyvale";
        String state = "CA";

        Tuple tuple1 = generateTestTuple(id, msg, city, state);

        bolt.execute(tuple1);
        verify(collector).ack(tuple1);

        List<String> partVals = Lists.newArrayList(city, state);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(1, recordsWritten.size());

        byte[] written = recordsWritten.get(0);

        Map<String, ?> writtenMap = objectMapper.readValue(new String(written), new TypeReference<Map<String, ?>>() {
        });

        Map<String, Object> expected = new HashMap<>();
        expected.put(COL1, id);
        expected.put(COL2, msg);

        assertEquals(expected, writtenMap);

        bolt.cleanup();
    }

    @Test
    public void testNoAcksUntilFlushed() {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Tuple tuple1 = generateTestTuple(1, "SJC", "Sunnyvale", "CA");
        Tuple tuple2 = generateTestTuple(2, "SFO", "San Jose", "CA");

        bolt.execute(tuple1);
        verifyNoInteractions(collector);

        bolt.execute(tuple2);
        verify(collector).ack(tuple1);
        verify(collector).ack(tuple2);
        bolt.cleanup();
    }

    @Test
    public void testNoAcksIfFlushFails() throws Exception {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        HiveBolt failingBolt = new FlushFailureHiveBolt(hiveOptions);

        failingBolt.prepare(config, null, new OutputCollector(collector));

        Tuple tuple1 = generateTestTuple(1, "SJC", "Sunnyvale", "CA");
        Tuple tuple2 = generateTestTuple(2, "SFO", "San Jose", "CA");

        failingBolt.execute(tuple1);
        failingBolt.execute(tuple2);

        verify(collector, never()).ack(tuple1);
        verify(collector, never()).ack(tuple2);

        failingBolt.cleanup();
    }

    @Test
    public void testTickTuple() {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Tuple tuple1 = generateTestTuple(1, "SJC", "Sunnyvale", "CA");
        Tuple tuple2 = generateTestTuple(2, "SFO", "San Jose", "CA");

        bolt.execute(tuple1);

        //The tick should cause tuple1 to be ack'd
        Tuple mockTick = MockTupleHelpers.mockTickTuple();
        bolt.execute(mockTick);
        verify(collector).ack(tuple1);

        //The second tuple should NOT be ack'd because the batch should be cleared and this will be
        //the first transaction in the new batch
        bolt.execute(tuple2);
        verify(collector, never()).ack(tuple2);

        bolt.cleanup();
    }

    @Test
    public void testNoTickEmptyBatches() throws Exception {
        JsonRecordHiveMapper mapper = new JsonRecordHiveMapper()
            .withColumnFields(new Fields(colNames1))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(2)
            .withBatchSize(2);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        //The tick should NOT cause any acks since the batch was empty except for acking itself
        Tuple mockTick = MockTupleHelpers.mockTickTuple();
        bolt.execute(mockTick);
        verifyNoInteractions(collector);

        bolt.cleanup();
    }

    @Test
    public void testMultiPartitionTuples()
        throws Exception {
        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveOptions hiveOptions = new HiveOptions(metaStoreURI, dbName, tblName, mapper)
            .withTxnsPerBatch(10)
            .withBatchSize(10);

        bolt = new TestingHiveBolt(hiveOptions);
        bolt.prepare(config, null, new OutputCollector(collector));

        Integer id = 1;
        String msg = "test";
        String city = "San Jose";
        String state = "CA";

        List<Tuple> tuples = new ArrayList<>();
        for (int i = 0; i < 100; i++) {
            Tuple tuple = generateTestTuple(id, msg, city, state);
            tuples.add(tuple);
            bolt.execute(tuple);
        }

        for (Tuple t : tuples) {
            verify(collector).ack(t);
        }

        List<String> partVals = Lists.newArrayList(city, state);

        List<byte[]> recordsWritten = bolt.getRecordWritten(partVals);
        assertNotNull(recordsWritten);
        assertEquals(100, recordsWritten.size());


        byte[] mapped = generateDelimiteredRecord(Lists.newArrayList(id, msg), mapper.getFieldDelimiter());

        for (byte[] record : recordsWritten) {
            assertArrayEquals(mapped, record);
        }

        bolt.cleanup();
    }

    private Tuple generateTestTuple(Object id, Object msg, Object city, Object state) {
        TopologyBuilder builder = new TopologyBuilder();
        GeneralTopologyContext topologyContext = new GeneralTopologyContext(builder.createTopology(),
                                                                            new Config(), new HashMap(), new HashMap(), new HashMap(), "") {
            @Override
            public Fields getComponentOutputFields(String componentId, String streamId) {
                return new Fields("id", "msg", "city", "state");
            }
        };
        return new TupleImpl(topologyContext, new Values(id, msg, city, state), "", 1, "");
    }

    private byte[] generateDelimiteredRecord(List<?> values, String fieldDelimiter) {
        StringBuilder builder = new StringBuilder();
        for (Object value : values) {
            builder.append(value);
            builder.append(fieldDelimiter);
        }
        return builder.toString().getBytes();
    }

    private static class TestingHiveBolt extends HiveBolt {

        protected Map<List<String>, List<byte[]>> partitionValuesToWrittenRecords = new HashMap<>();

        public TestingHiveBolt(HiveOptions options) {
            super(options);
        }

        @Override
        HiveWriter getOrCreateWriter(final HiveEndPoint endPoint)
            throws HiveWriter.ConnectFailure, InterruptedException {
            HiveWriter writer = allWriters.get(endPoint);
            if (writer == null) {
                // always provide mocked HiveWriter
                writer = Mockito.mock(HiveWriter.class);
                try {
                    Mockito.doAnswer(new Answer<Void>() {
                        @Override
                        public Void answer(InvocationOnMock invocation) throws Throwable {
                            Object[] arguments = invocation.getArguments();
                            List<String> partitionVals = endPoint.partitionVals;
                            List<byte[]> writtenRecords = partitionValuesToWrittenRecords.get(partitionVals);
                            if (writtenRecords == null) {
                                writtenRecords = new ArrayList<>();
                                partitionValuesToWrittenRecords.put(partitionVals, writtenRecords);
                            }
                            writtenRecords.add((byte[]) arguments[0]);
                            return null;
                        }
                    }).when(writer).write(any(byte[].class));
                } catch (Exception exc) {
                    throw new RuntimeException(exc);
                }
            }
            return writer;
        }

        public Map<List<String>, List<byte[]>> getPartitionValuesToWrittenRecords() {
            return partitionValuesToWrittenRecords;
        }

        public List<byte[]> getRecordWritten(List<String> partitionValues) {
            return partitionValuesToWrittenRecords.get(partitionValues);
        }
    }

    private static class FlushFailureHiveBolt extends TestingHiveBolt {

        public FlushFailureHiveBolt(HiveOptions options) {
            super(options);
        }

        @Override
        void flushAllWriters(boolean rollToNext) throws HiveWriter.CommitFailure, HiveWriter.TxnBatchFailure, HiveWriter.TxnFailure,
            InterruptedException {
            if (rollToNext) {
                throw new InterruptedException();
            } else {
                super.flushAllWriters(false);
            }
        }
    }

}
/**
 * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the Apache License, Version
 * 2.0 (the "License"); you may not use this file except in compliance with the License.  You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions
 * and limitations under the License.
 */

package org.apache.storm.hive.common;

import com.google.common.util.concurrent.ThreadFactoryBuilder;

import java.io.File;
import java.io.IOException;
import java.util.Arrays;
import java.util.HashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.txn.TxnDbUtil;
import org.apache.hadoop.security.UserGroupInformation;
import org.apache.hive.hcatalog.streaming.HiveEndPoint;
import org.apache.hive.hcatalog.streaming.RecordWriter;
import org.apache.hive.hcatalog.streaming.SerializationError;
import org.apache.hive.hcatalog.streaming.StreamingConnection;
import org.apache.hive.hcatalog.streaming.StreamingException;
import org.apache.hive.hcatalog.streaming.TransactionBatch;
import org.apache.storm.Config;
import org.apache.storm.hive.bolt.HiveSetupUtil;
import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
import org.apache.storm.hive.bolt.mapper.HiveMapper;
import org.apache.storm.task.GeneralTopologyContext;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.tuple.Fields;
import org.apache.storm.tuple.Tuple;
import org.apache.storm.tuple.TupleImpl;
import org.apache.storm.tuple.Values;
import org.junit.jupiter.api.Test;
import org.mockito.Mockito;

import static org.junit.jupiter.api.Assertions.assertEquals;

public class TestHiveWriter {
    public static final String PART1_NAME = "city";
    public static final String PART2_NAME = "state";
    public static final String[] partNames = { PART1_NAME, PART2_NAME };
    final static String dbName = "testdb";
    final static String tblName = "test_table2";
    final String[] partitionVals = { "sunnyvale", "ca" };
    final String[] colNames = { "id", "msg" };
    private final int port;
    private final String metaStoreURI;
    private final HiveConf conf;
    int timeout = 10000; // msec
    UserGroupInformation ugi = null;
    private ExecutorService callTimeoutPool;

    public TestHiveWriter() throws Exception {
        port = 9083;
        metaStoreURI = null;
        int callTimeoutPoolSize = 1;
        callTimeoutPool = Executors.newFixedThreadPool(callTimeoutPoolSize,
                                                       new ThreadFactoryBuilder().setNameFormat("hiveWriterTest").build());

        // 1) Start metastore
        conf = HiveSetupUtil.getHiveConf();
        TxnDbUtil.setConfValues(conf);
        if (metaStoreURI != null) {
            conf.setVar(HiveConf.ConfVars.METASTOREURIS, metaStoreURI);
        }
    }

    @Test
    public void testInstantiate() throws Exception {
        DelimitedRecordHiveMapper mapper = new MockedDelemiteredRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
        TestingHiveWriter writer = new TestingHiveWriter(endPoint, 10, true, timeout
            , callTimeoutPool, mapper, ugi, false);
        writer.close();
    }

    @Test
    public void testWriteBasic() throws Exception {
        DelimitedRecordHiveMapper mapper = new MockedDelemiteredRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));
        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
        TestingHiveWriter writer = new TestingHiveWriter(endPoint, 10, true, timeout
            , callTimeoutPool, mapper, ugi, false);
        writeTuples(writer, mapper, 3);
        writer.flush(false);
        writer.close();
        Mockito.verify(writer.getMockedTxBatch(), Mockito.times(3)).write(Mockito.any(byte[].class));
    }

    @Test
    public void testWriteMultiFlush() throws Exception {
        DelimitedRecordHiveMapper mapper = new MockedDelemiteredRecordHiveMapper()
            .withColumnFields(new Fields(colNames))
            .withPartitionFields(new Fields(partNames));

        HiveEndPoint endPoint = new HiveEndPoint(metaStoreURI, dbName, tblName, Arrays.asList(partitionVals));
        TestingHiveWriter writer = new TestingHiveWriter(endPoint, 10, true, timeout
            , callTimeoutPool, mapper, ugi, false);
        Tuple tuple = generateTestTuple("1", "abc");
        writer.write(mapper.mapRecord(tuple));
        tuple = generateTestTuple("2", "def");
        writer.write(mapper.mapRecord(tuple));
        assertEquals(writer.getTotalRecords(), 2);
        Mockito.verify(writer.getMockedTxBatch(), Mockito.times(2)).write(Mockito.any(byte[].class));
        Mockito.verify(writer.getMockedTxBatch(), Mockito.never()).commit();
        writer.flush(true);
        assertEquals(writer.getTotalRecords(), 0);
        Mockito.verify(writer.getMockedTxBatch(), Mockito.atLeastOnce()).commit();

        tuple = generateTestTuple("3", "ghi");
        writer.write(mapper.mapRecord(tuple));
        writer.flush(true);

        tuple = generateTestTuple("4", "klm");
        writer.write(mapper.mapRecord(tuple));
        writer.flush(true);
        writer.close();
        Mockito.verify(writer.getMockedTxBatch(), Mockito.times(4)).write(Mockito.any(byte[].class));
    }

    private Tuple generateTestTuple(Object id, Object msg) {
        TopologyBuilder builder = new TopologyBuilder();
        GeneralTopologyContext topologyContext = new GeneralTopologyContext(builder.createTopology(),
                                                                            new Config(), new HashMap(), new HashMap(), new HashMap(), "") {
            @Override
            public Fields getComponentOutputFields(String componentId, String streamId) {
                return new Fields("id", "msg");
            }
        };
        return new TupleImpl(topologyContext, new Values(id, msg), "", 1, "");
    }

    private void writeTuples(HiveWriter writer, HiveMapper mapper, int count)
        throws HiveWriter.WriteFailure, InterruptedException, SerializationError {
        Integer id = 100;
        String msg = "test-123";
        for (int i = 1; i <= count; i++) {
            Tuple tuple = generateTestTuple(id, msg);
            writer.write(mapper.mapRecord(tuple));
        }
    }

    private static class TestingHiveWriter extends HiveWriter {

        private StreamingConnection mockedStreamingConn;
        private TransactionBatch mockedTxBatch;

        public TestingHiveWriter(HiveEndPoint endPoint, int txnsPerBatch, boolean autoCreatePartitions, long callTimeout,
                                 ExecutorService callTimeoutPool, HiveMapper mapper, UserGroupInformation ugi,
                                 boolean tokenAuthEnabled) throws InterruptedException, ConnectFailure {
            super(endPoint, txnsPerBatch, autoCreatePartitions, callTimeout, callTimeoutPool, mapper, ugi, tokenAuthEnabled);
        }

        @Override
        synchronized StreamingConnection newConnection(UserGroupInformation ugi, boolean tokenAuthEnabled) throws InterruptedException,
            ConnectFailure {
            if (mockedStreamingConn == null) {
                mockedStreamingConn = Mockito.mock(StreamingConnection.class);
                mockedTxBatch = Mockito.mock(TransactionBatch.class);

                try {
                    Mockito.when(mockedStreamingConn.fetchTransactionBatch(Mockito.anyInt(), Mockito.any(RecordWriter.class)))
                           .thenReturn(mockedTxBatch);
                } catch (StreamingException e) {
                    throw new RuntimeException(e);
                }
            }

            return mockedStreamingConn;
        }

        public TransactionBatch getMockedTxBatch() {
            return mockedTxBatch;
        }
    }

    private static class MockedDelemiteredRecordHiveMapper extends DelimitedRecordHiveMapper {
        private final RecordWriter mockedRecordWriter;

        public MockedDelemiteredRecordHiveMapper() {
            this.mockedRecordWriter = Mockito.mock(RecordWriter.class);
        }

        @Override
        public RecordWriter createRecordWriter(HiveEndPoint endPoint) throws StreamingException, IOException, ClassNotFoundException {
            return mockedRecordWriter;
        }

        public RecordWriter getMockedRecordWriter() {
            return mockedRecordWriter;
        }
    }

}
```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new file, `HiveSetupUtil.java`, which is primarily focused on setting up a testing environment for Hive integration within the Apache Storm project. The changes include methods for configuring Hive, creating databases and tables, managing partitions, and handling file system interactions specific to testing scenarios. These modifications do not directly impact critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Instead, they are centered around test setup and configuration for Hive integration, which is typically isolated from production code and does not affect the system's security posture. Furthermore, the use of a `RawFileSystem` class and the manipulation of Hive configurations and metadata for testing purposes do not inherently alter or compromise security mechanisms within the system.",
  "confidence": 9
}
```

Diff number: 29
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopology.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopology.java
new file mode 100644
index 000000000..e9ecbd06f
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopology.java
@@ -0,0 +1,150 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.Config;
+import backtype.storm.LocalCluster;
+import backtype.storm.StormSubmitter;
+import backtype.storm.spout.SpoutOutputCollector;
+import backtype.storm.task.TopologyContext;
+import backtype.storm.topology.OutputFieldsDeclarer;
+import backtype.storm.topology.TopologyBuilder;
+import backtype.storm.topology.base.BaseRichSpout;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Values;
+
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.common.HiveOptions;
+
+import java.util.Map;
+import java.util.UUID;
+import java.util.concurrent.ConcurrentHashMap;
+
+
+public class HiveTopology {
+    static final String USER_SPOUT_ID = "user-spout";
+    static final String BOLT_ID = "my-hive-bolt";
+    static final String TOPOLOGY_NAME = "hive-test-topology1";
+
+    public static void main(String[] args) throws Exception {
+        String metaStoreURI = args[0];
+        String dbName = args[1];
+        String tblName = args[2];
+        String[] colNames = {"id","name","phone","street","city","state"};
+        Config config = new Config();
+        config.setNumWorkers(1);
+        UserDataSpout spout = new UserDataSpout();
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames));
+        HiveOptions hiveOptions;
+        if (args.length == 6) {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(100)
+                .withIdleTimeout(10)
+                .withKerberosKeytab(args[4])
+                .withKerberosPrincipal(args[5]);
+        } else {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(100)
+                .withIdleTimeout(10);
+        }
+
+        HiveBolt hiveBolt = new HiveBolt(hiveOptions);
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.setSpout(USER_SPOUT_ID, spout, 1);
+        // SentenceSpout --> MyBolt
+        builder.setBolt(BOLT_ID, hiveBolt, 1)
+                .shuffleGrouping(USER_SPOUT_ID);
+        if (args.length == 3) {
+            LocalCluster cluster = new LocalCluster();
+            cluster.submitTopology(TOPOLOGY_NAME, config, builder.createTopology());
+            waitForSeconds(20);
+            cluster.killTopology(TOPOLOGY_NAME);
+            System.out.println("cluster begin to shutdown");
+            cluster.shutdown();
+            System.out.println("cluster shutdown");
+            System.exit(0);
+        } else if(args.length >= 4) {
+            StormSubmitter.submitTopology(args[3], config, builder.createTopology());
+        } else {
+            System.out.println("Usage: HiveTopology metastoreURI dbName tableName [topologyNamey] [keytab file] [principal name]");
+        }
+    }
+
+    public static void waitForSeconds(int seconds) {
+        try {
+            Thread.sleep(seconds * 1000);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    public static class UserDataSpout extends BaseRichSpout {
+        private ConcurrentHashMap<UUID, Values> pending;
+        private SpoutOutputCollector collector;
+        private String[] sentences = {
+                "1,user1,123456,street1,sunnyvale,ca",
+                "2,user2,123456,street2,sunnyvale,ca",
+                "3,user3,123456,street3,san jose,ca",
+                "4,user4,123456,street4,san jose,ca",
+        };
+        private int index = 0;
+        private int count = 0;
+        private long total = 0L;
+
+        public void declareOutputFields(OutputFieldsDeclarer declarer) {
+            declarer.declare(new Fields("id","name","phone","street","city","state"));
+        }
+
+        public void open(Map config, TopologyContext context,
+                         SpoutOutputCollector collector) {
+            this.collector = collector;
+            this.pending = new ConcurrentHashMap<UUID, Values>();
+        }
+
+        public void nextTuple() {
+            String[] user = sentences[index].split(",");
+            Values values = new Values(Integer.parseInt(user[0]),user[1],user[2],user[3],user[4],user[5]);
+            UUID msgId = UUID.randomUUID();
+            this.pending.put(msgId, values);
+            this.collector.emit(values, msgId);
+            index++;
+            if (index >= sentences.length) {
+                index = 0;
+            }
+            count++;
+            total++;
+            if(count > 1000){
+                count = 0;
+                System.out.println("Pending count: " + this.pending.size() + ", total: " + this.total);
+            }
+            Thread.yield();
+        }
+
+        public void ack(Object msgId) {
+            this.pending.remove(msgId);
+        }
+
+        public void fail(Object msgId) {
+            System.out.println("**** RESENDING FAILED TUPLE");
+            this.collector.emit(this.pending.get(msgId), msgId);
+        }
+    }
+}

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The diff file introduces changes that include the configuration and usage of a HiveOptions object with potential security implications. Specifically, the code conditionally configures HiveOptions with Kerberos authentication parameters ('withKerberosKeytab' and 'withKerberosPrincipal') if certain arguments are provided. Kerberos is a network authentication protocol designed to provide strong authentication for client/server applications by using secret-key cryptography. The inclusion of Kerberos authentication parameters directly impacts the security posture of the system by potentially enhancing the security of the communication between the application and the Hive server, assuming that the Hive server is also configured to use Kerberos authentication. This change could help in mitigating the risk of unauthorized access to the Hive server, thereby protecting sensitive data processed or stored by the Hive server. However, the security impact of this change is contingent upon the correct and secure configuration of Kerberos, including the protection of keytab files and the secure management of Kerberos principals.",
  "confidence": 8
}
```

Diff number: 30
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopologyPartitioned.java b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopologyPartitioned.java
new file mode 100644
index 000000000..c3197c22f
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/bolt/HiveTopologyPartitioned.java
@@ -0,0 +1,153 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.bolt;
+
+import backtype.storm.Config;
+import backtype.storm.LocalCluster;
+import backtype.storm.StormSubmitter;
+import backtype.storm.spout.SpoutOutputCollector;
+import backtype.storm.task.TopologyContext;
+import backtype.storm.topology.OutputFieldsDeclarer;
+import backtype.storm.topology.TopologyBuilder;
+import backtype.storm.topology.base.BaseRichSpout;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Values;
+import backtype.storm.utils.Utils;
+
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.common.HiveOptions;
+
+import java.util.Map;
+import java.util.UUID;
+import java.util.concurrent.ConcurrentHashMap;
+
+
+public class HiveTopologyPartitioned {
+    static final String USER_SPOUT_ID = "hive-user-spout-partitioned";
+    static final String BOLT_ID = "my-hive-bolt-partitioned";
+    static final String TOPOLOGY_NAME = "hive-test-topology-partitioned";
+
+    public static void main(String[] args) throws Exception {
+        String metaStoreURI = args[0];
+        String dbName = args[1];
+        String tblName = args[2];
+        String[] partNames = {"city","state"};
+        String[] colNames = {"id","name","phone","street"};
+        Config config = new Config();
+        config.setNumWorkers(1);
+        UserDataSpout spout = new UserDataSpout();
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions;
+        if (args.length == 6) {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(1000)
+                .withIdleTimeout(10)
+                .withKerberosKeytab(args[4])
+                .withKerberosPrincipal(args[5]);
+        } else {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(1000)
+                .withIdleTimeout(10);
+        }
+
+        HiveBolt hiveBolt = new HiveBolt(hiveOptions);
+        TopologyBuilder builder = new TopologyBuilder();
+        builder.setSpout(USER_SPOUT_ID, spout, 1);
+        // SentenceSpout --> MyBolt
+        builder.setBolt(BOLT_ID, hiveBolt, 1)
+                .shuffleGrouping(USER_SPOUT_ID);
+        if (args.length == 3) {
+            LocalCluster cluster = new LocalCluster();
+            cluster.submitTopology(TOPOLOGY_NAME, config, builder.createTopology());
+            waitForSeconds(20);
+            cluster.killTopology(TOPOLOGY_NAME);
+            System.out.println("cluster begin to shutdown");
+            cluster.shutdown();
+            System.out.println("cluster shutdown");
+            System.exit(0);
+        } else if(args.length >= 4) {
+            StormSubmitter.submitTopology(args[3], config, builder.createTopology());
+        } else {
+            System.out.println("Usage: HiveTopologyPartitioned metastoreURI dbName tableName [topologyNamey] [keytab file] [principal name]");
+        }
+    }
+
+    public static void waitForSeconds(int seconds) {
+        try {
+            Thread.sleep(seconds * 1000);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    public static class UserDataSpout extends BaseRichSpout {
+        private ConcurrentHashMap<UUID, Values> pending;
+        private SpoutOutputCollector collector;
+        private String[] sentences = {
+                "1,user1,123456,street1,sunnyvale,ca",
+                "2,user2,123456,street2,sunnyvale,ca",
+                "3,user3,123456,street3,san jose,ca",
+                "4,user4,123456,street4,san jose,ca",
+        };
+        private int index = 0;
+        private int count = 0;
+        private long total = 0L;
+
+        public void declareOutputFields(OutputFieldsDeclarer declarer) {
+            declarer.declare(new Fields("id","name","phone","street","city","state"));
+        }
+
+        public void open(Map config, TopologyContext context,
+                         SpoutOutputCollector collector) {
+            this.collector = collector;
+            this.pending = new ConcurrentHashMap<UUID, Values>();
+        }
+
+        public void nextTuple() {
+            String[] user = sentences[index].split(",");
+            Values values = new Values(Integer.parseInt(user[0]),user[1],user[2],user[3],user[4],user[5]);
+            UUID msgId = UUID.randomUUID();
+            this.pending.put(msgId, values);
+            this.collector.emit(values, msgId);
+            index++;
+            if (index >= sentences.length) {
+                index = 0;
+            }
+            count++;
+            total++;
+            if(count > 1000){
+		Utils.sleep(1000);
+                count = 0;
+                System.out.println("Pending count: " + this.pending.size() + ", total: " + this.total);
+            }
+        }
+
+        public void ack(Object msgId) {
+            this.pending.remove(msgId);
+        }
+
+        public void fail(Object msgId) {
+            System.out.println("**** RESENDING FAILED TUPLE");
+            this.collector.emit(this.pending.get(msgId), msgId);
+        }
+    }
+}

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file introduces a new file, specifically for a test topology in Apache Storm's Hive integration, named HiveTopologyPartitioned. The changes primarily involve the setup of a test topology for data processing, including the definition of a spout for emitting user data and a bolt for processing this data with Hive. The modifications include configurations for a HiveOptions object, potentially with Kerberos authentication parameters (keytab file and principal name), but these are standard configurations for secure Hive connections and do not directly impact the security posture of the system. The code is focused on data processing logic and test setup, without altering or introducing new authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, the changes are not directly related to security functionalities but are part of test infrastructure setup.",
  "confidence": 8
}
```

Diff number: 33
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java b/external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java
new file mode 100644
index 000000000..bc607f3f9
--- /dev/null
+++ b/external/storm-hive/src/test/java/org/apache/storm/hive/trident/TridentHiveTopology.java
@@ -0,0 +1,190 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ * http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.storm.hive.trident;
+
+
+import org.apache.storm.hive.bolt.mapper.DelimitedRecordHiveMapper;
+import org.apache.storm.hive.common.HiveOptions;
+
+import backtype.storm.Config;
+import backtype.storm.LocalCluster;
+import backtype.storm.StormSubmitter;
+import backtype.storm.generated.StormTopology;
+import backtype.storm.tuple.Fields;
+import backtype.storm.tuple.Values;
+import backtype.storm.task.TopologyContext;
+import storm.trident.operation.TridentCollector;
+import storm.trident.spout.IBatchSpout;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import storm.trident.Stream;
+import storm.trident.TridentState;
+import storm.trident.TridentTopology;
+import storm.trident.state.StateFactory;
+
+
+public class TridentHiveTopology {
+    public static StormTopology buildTopology(String metaStoreURI, String dbName, String tblName, Object keytab, Object principal) {
+        int batchSize = 100;
+        FixedBatchSpout spout = new FixedBatchSpout(batchSize);
+        spout.setCycle(true);
+        TridentTopology topology = new TridentTopology();
+        Stream stream = topology.newStream("hiveTridentspout1",spout);
+        String[] partNames = {"city","state"};
+        String[] colNames = {"id","name","phone","street"};
+        Fields hiveFields = new Fields("id","name","phone","street","city","state");
+        DelimitedRecordHiveMapper mapper = new DelimitedRecordHiveMapper()
+            .withColumnFields(new Fields(colNames))
+            .withPartitionFields(new Fields(partNames));
+        HiveOptions hiveOptions;
+        if (keytab != null && principal != null) {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(batchSize)
+                .withIdleTimeout(10)
+                .withCallTimeout(30000)
+                .withKerberosKeytab((String)keytab)
+                .withKerberosPrincipal((String)principal);
+        } else  {
+            hiveOptions = new HiveOptions(metaStoreURI,dbName,tblName,mapper)
+                .withTxnsPerBatch(10)
+                .withBatchSize(batchSize)
+                .withCallTimeout(30000)
+                .withIdleTimeout(10);
+        }
+        StateFactory factory = new HiveStateFactory().withOptions(hiveOptions);
+        TridentState state = stream.partitionPersist(factory, hiveFields, new HiveUpdater(), new Fields());
+        return topology.build();
+    }
+
+    public static void waitForSeconds(int seconds) {
+        try {
+            Thread.sleep(seconds * 1000);
+        } catch (InterruptedException e) {
+        }
+    }
+
+    public static void main(String[] args) {
+        String metaStoreURI = args[0];
+        String dbName = args[1];
+        String tblName = args[2];
+        Config conf = new Config();
+        conf.setMaxSpoutPending(5);
+        if(args.length == 3) {
+            LocalCluster cluster = new LocalCluster();
+            cluster.submitTopology("tridentHiveTopology", conf, buildTopology(metaStoreURI, dbName, tblName,null,null));
+            System.out.println("waiting for 60 seconds");
+            waitForSeconds(60);
+            System.out.println("killing topology");
+            cluster.killTopology("tridenHiveTopology");
+            System.out.println("cluster shutdown");
+            cluster.shutdown();
+            System.out.println("cluster shutdown");
+            System.exit(0);
+        } else if(args.length == 4) {
+            try {
+                StormSubmitter.submitTopology(args[3], conf, buildTopology(metaStoreURI, dbName, tblName,null,null));
+            } catch(Exception e) {
+                System.out.println("Failed to submit topology "+e);
+            }
+        } else if (args.length == 6) {
+            try {
+                StormSubmitter.submitTopology(args[3], conf, buildTopology(metaStoreURI, dbName, tblName,args[4],args[5]));
+            } catch(Exception e) {
+                System.out.println("Failed to submit topology "+e);
+            }
+        } else {
+            System.out.println("Usage: TridentHiveTopology metastoreURI dbName tableName [topologyNamey]");
+        }
+    }
+
+    public static class FixedBatchSpout implements IBatchSpout {
+        int maxBatchSize;
+        HashMap<Long, List<List<Object>>> batches = new HashMap<Long, List<List<Object>>>();
+        private Values[] outputs = {
+            new Values("1","user1","123456","street1","sunnyvale","ca"),
+            new Values("2","user2","123456","street2","sunnyvale","ca"),
+            new Values("3","user3","123456","street3","san jose","ca"),
+            new Values("4","user4","123456","street4","san jose","ca"),
+        };
+        private int index = 0;
+        boolean cycle = false;
+
+        public FixedBatchSpout(int maxBatchSize) {
+            this.maxBatchSize = maxBatchSize;
+        }
+
+        public void setCycle(boolean cycle) {
+            this.cycle = cycle;
+        }
+
+        @Override
+        public Fields getOutputFields() {
+            return new Fields("id","name","phone","street","city","state");
+        }
+
+        @Override
+        public void open(Map conf, TopologyContext context) {
+            index = 0;
+        }
+
+        @Override
+        public void emitBatch(long batchId, TridentCollector collector) {
+            List<List<Object>> batch = this.batches.get(batchId);
+            if(batch == null){
+                batch = new ArrayList<List<Object>>();
+                if(index>=outputs.length && cycle) {
+                    index = 0;
+                }
+                for(int i=0; i < maxBatchSize; index++, i++) {
+                    if(index == outputs.length){
+                        index=0;
+                    }
+                    batch.add(outputs[index]);
+                }
+                this.batches.put(batchId, batch);
+            }
+            for(List<Object> list : batch){
+                collector.emit(list);
+            }
+        }
+
+        @Override
+        public void ack(long batchId) {
+            this.batches.remove(batchId);
+        }
+
+        @Override
+        public void close() {
+        }
+
+        @Override
+        public Map getComponentConfiguration() {
+            Config conf = new Config();
+            conf.setMaxTaskParallelism(1);
+            return conf;
+        }
+
+    }
+
+}

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The code changes introduce a conditional branch that configures HiveOptions with Kerberos authentication details (keytab and principal) if they are provided. This directly impacts the system's security by potentially altering the authentication mechanism used for accessing Hive. Kerberos is a network authentication protocol designed to provide strong authentication for client/server applications by using secret-key cryptography. The inclusion of Kerberos authentication options enhances the security posture of the system by ensuring that access to Hive is secured through a robust authentication mechanism. However, the security impact depends on the correct implementation and secure handling of the keytab and principal information. If mishandled, it could lead to security vulnerabilities, such as unauthorized access. Therefore, this change is directly related to security functionalities, specifically in the context of authentication mechanisms.",
  "confidence": 9
}
```

Diff number: 34
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java b/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java
index cc723c332..7bac4c8d7 100644
--- a/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java
+++ b/external/storm-jdbc/src/main/java/org/apache/storm/jdbc/common/Util.java
@@ -24,10 +24,11 @@ import java.sql.Timestamp;
 import java.sql.Types;
 
 public class Util {
-    public static String getSqlTypeName(int sqlType) {
+    private static String getSqlTypeName(int sqlType) {
         try {
+            Integer val = new Integer(sqlType);
             for (Field field : Types.class.getFields()) {
-                if (sqlType == field.get(null)) {
+                if (val.equals(field.get(null))) {
                     return field.getName();
                 }
             }

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff file shows modifications within a utility class related to the handling of SQL type names. Specifically, the changes involve making the 'getSqlTypeName' method private and altering the comparison mechanism from a direct integer comparison to using an Integer object's equals method. These changes do not directly impact critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. The modifications seem to be more about code quality and consistency rather than altering the security posture of the system. There is no direct or immediate implication on system security based on the nature of these changes.",
  "confidence": 9
}
```

Diff number: 36
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
index b6cccadd2..ecb0daf6a 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java
@@ -141,7 +141,7 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {
                     new PrivilegedExceptionAction<Void>() {
                 public Void run() {
                     try {
-                        LOG.info("do as:"+ principal);
+                        LOG.debug("do as:"+ principal);
                         sasalTransport.open();
                     }
                     catch (Exception e) {

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The change in the diff file is related to the logging level of a message from 'info' to 'debug'. This modification does not directly impact any critical security functionalities such as authentication mechanisms, encryption algorithms, access control procedures, or the logging of security events. It merely changes the verbosity level of a log message, which could potentially affect the amount of information available in logs under default settings but does not alter the security posture of the system. Without additional context suggesting that the logged information is sensitive or critical to security, this change is not directly related to security concerns.",
  "confidence": 8
}
```

Diff number: 37
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/Config.java b/storm-core/src/jvm/backtype/storm/Config.java
index 1237c28f2..1655cf6ca 100644
--- a/storm-core/src/jvm/backtype/storm/Config.java
+++ b/storm-core/src/jvm/backtype/storm/Config.java
@@ -441,6 +441,14 @@ public class Config extends HashMap<String, Object> {
     public static final String NIMBUS_AUTHORIZER = "nimbus.authorizer";
     public static final Object NIMBUS_AUTHORIZER_SCHEMA = String.class;
 
+
+    /**
+     * Impersonation user ACL config entries.
+     */
+    public static final String NIMBUS_IMPERSONATION_AUTHORIZER = "nimbus.impersonation.authorizer";
+    public static final Object NIMBUS_IMPERSONATION_AUTHORIZER_SCHEMA = String.class;
+
+
     /**
      * Impersonation user ACL config entries.
      */

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The addition of the 'NIMBUS_IMPERSONATION_AUTHORIZER' configuration parameter directly impacts the security functionalities of the system. This parameter appears to be related to the authorization process for user impersonation within the system. Impersonation features, if not properly secured and monitored, can be exploited by attackers to escalate privileges or perform actions on behalf of legitimate users without authorization. The introduction of this parameter suggests that the system is either implementing or enhancing its capabilities around user impersonation, which is a critical security component. The security implications of this change depend on how this new configuration is used and enforced within the system's authentication and authorization mechanisms. It is essential to ensure that only authorized users can define or modify this parameter and that its usage is strictly controlled and audited.",
  "confidence": 9
}
```

Diff number: 38
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
index 1e947ae67..d6431be56 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/ImpersonationAuthorizer.java
@@ -13,6 +13,7 @@ import java.util.*;
 
 public class ImpersonationAuthorizer implements IAuthorizer {
     private static final Logger LOG = LoggerFactory.getLogger(ImpersonationAuthorizer.class);
+    protected static final String WILD_CARD = "*";
 
     protected Map<String, ImpersonationACL> userImpersonationACL;
     protected IPrincipalToLocal _ptol;
@@ -97,12 +98,17 @@ public class ImpersonationAuthorizer implements IAuthorizer {
     }
 
     private boolean isAllowedToImpersonateFromHost(Set<String> authorizedHosts, InetAddress remoteAddress) {
-        return authorizedHosts.contains(remoteAddress.getCanonicalHostName()) ||
+        return authorizedHosts.contains(WILD_CARD) ||
+                authorizedHosts.contains(remoteAddress.getCanonicalHostName()) ||
                 authorizedHosts.contains(remoteAddress.getHostName()) ||
                 authorizedHosts.contains(remoteAddress.getHostAddress());
     }
 
     private boolean isAllowedToImpersonateUser(Set<String> authorizedGroups, String userBeingImpersonated) {
+        if(authorizedGroups.contains(WILD_CARD)) {
+            return true;
+        }
+
         Set<String> groups = null;
         try {
             groups = _groupMappingProvider.getGroups(userBeingImpersonated);

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes introduced in the ImpersonationAuthorizer.java file directly impact the system's security by modifying the authentication mechanism. Specifically, the addition of a wildcard ('*') as a valid entry for both authorized hosts and groups significantly lowers the security posture. By allowing a wildcard to represent any host or group, the system opens up the possibility for any user to impersonate any other user from any host, provided they are able to pass the initial authentication checks. This effectively bypasses more granular access controls and could allow unauthorized access to sensitive functions or data, assuming the attacker has some level of authenticated access. The use of wildcards in security-sensitive checks should be carefully considered and, in most cases, avoided due to the broad permissions it grants. This change could potentially allow an attacker to escalate privileges or access data and functionalities intended to be restricted, thus directly impacting the security of the system.",
  "confidence": 9
}
```

Diff number: 39
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
index 25114b959..888eefb4f 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java
@@ -50,7 +50,6 @@ public class SimpleACLAuthorizer implements IAuthorizer {
     protected Set<String> _supervisors;
     protected IPrincipalToLocal _ptol;
     protected IGroupMappingServiceProvider _groupMappingProvider;
-    protected ImpersonationAuthorizer _impersonationAuthorizer;
     /**
      * Invoked once immediately after construction
      * @param conf Storm configuration
@@ -69,8 +68,6 @@ public class SimpleACLAuthorizer implements IAuthorizer {
 
         _ptol = AuthUtils.GetPrincipalToLocalPlugin(conf);
         _groupMappingProvider = AuthUtils.GetGroupMappingServiceProviderPlugin(conf);
-        _impersonationAuthorizer = new ImpersonationAuthorizer();
-        _impersonationAuthorizer.prepare(conf);
     }
 
     /**
@@ -91,10 +88,6 @@ public class SimpleACLAuthorizer implements IAuthorizer {
         String principal = context.principal().getName();
         String user = _ptol.toLocal(context.principal());
 
-        if(!_impersonationAuthorizer.permit(context, operation, topology_conf)) {
-            return false;
-        }
-
         if (_admins.contains(principal) || _admins.contains(user)) {
             return true;
         }

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file directly impact the security functionalities of the system. Specifically, the removal of the ImpersonationAuthorizer and its associated checks within the SimpleACLAuthorizer class could significantly alter the system's security posture. The ImpersonationAuthorizer likely played a crucial role in determining whether an operation should be permitted based on impersonation rules, which are essential for a secure multi-user environment. By removing these checks, the system may now allow operations that were previously blocked due to impersonation concerns, potentially opening up vulnerabilities related to unauthorized access or privilege escalation. Furthermore, the removal of the initialization and preparation of the ImpersonationAuthorizer (`_impersonationAuthorizer.prepare(conf);`) suggests that the system's ability to dynamically adjust its authorization behavior based on configuration changes has been diminished. This could lead to a static authorization mechanism that does not adapt to updated security policies or configurations, thereby weakening the system's defense against evolving threats.",
  "confidence": 9
}
```

Diff number: 40
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
index 0ff3d35bd..1788dab08 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/digest/ServerCallbackHandler.java
@@ -112,7 +112,7 @@ public class ServerCallbackHandler implements CallbackHandler {
 
     private void handleAuthorizeCallback(AuthorizeCallback ac) {
         String authenticationID = ac.getAuthenticationID();
-        LOG.info("Successfully authenticated client: authenticationID=" + authenticationID + " authorizationID= " + ac.getAuthorizationID());
+        LOG.info("Successfully authenticated client: authenticationID = " + authenticationID + " authorizationID = " + ac.getAuthorizationID());
 
         //if authorizationId is not set, set it to authenticationId.
         if(ac.getAuthorizationID() == null) {
@@ -122,6 +122,7 @@ public class ServerCallbackHandler implements CallbackHandler {
         //When authNid and authZid are not equal , authNId is attempting to impersonate authZid, We
         //add the authNid as the real user in reqContext's subject which will be used during authorization.
         if(!authenticationID.equals(ac.getAuthorizationID())) {
+            LOG.info("Impersonation attempt  authenticationID = " + ac.getAuthenticationID() + " authorizationID = " + ac.getAuthorizationID());
             ReqContext.context().setRealPrincipal(new SaslTransportPlugin.User(ac.getAuthenticationID()));
         }
 

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file are directly related to the system's authentication mechanism, specifically within the context of handling authorization callbacks and logging related to authentication and impersonation attempts. The addition of a log statement for impersonation attempts ('Impersonation attempt  authenticationID = ... authorizationID = ...') is particularly relevant to security as it provides an audit trail for such attempts, which could be crucial for detecting unauthorized access attempts or breaches. This change enhances the system's ability to log and monitor potentially malicious activities, thereby directly impacting the system's security posture by improving its audit capabilities and potentially aiding in the detection and response to security incidents. The modification of the log format for successful authentication events also relates to security, albeit in a less direct manner, by potentially standardizing log formats to aid in automated log analysis and monitoring.",
  "confidence": 9
}
```

Diff number: 41
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
index 86455580a..59c5b065d 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
@@ -72,16 +72,24 @@ public class DefaultHttpCredentialsPlugin implements IHttpCredentialsPlugin {
     public ReqContext populateContext(ReqContext context,
             HttpServletRequest req) {
         String userName = getUserName(req);
-        Principal p = null;
-        if (userName != null) {
-            p = new SingleUserPrincipal(userName);
+
+        String doAsUser = req.getHeader("doAsUser");
+        if(doAsUser == null) {
+            doAsUser = req.getParameter("doAsUser");
         }
-        Set<Principal> principals = new HashSet<Principal>(1);
-        if (p != null) {
-            principals.add(p);
+
+        if(doAsUser != null) {
+            context.setRealPrincipal(new SingleUserPrincipal(userName));
+            userName = doAsUser;
         }
-        Subject s = new Subject(true, principals, new HashSet(), new HashSet());
-        context.setSubject(s);
+
+        if(userName != null) {
+            Subject s = new Subject();
+            Principal p = new SingleUserPrincipal(userName);
+            s.getPrincipals().add(p);
+            context.setSubject(s);
+        }
+
         return context;
     }
 }

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The provided diff file introduces changes to the authentication mechanism within the DefaultHttpCredentialsPlugin class of a system, specifically in how user identities are handled during HTTP requests. The modifications allow for a 'doAsUser' parameter to be extracted either from the request's header or parameter list, which then can override the initially authenticated user ('userName') for subsequent operations. This change directly impacts the system's security by introducing the potential for privilege escalation or unauthorized actions if the 'doAsUser' feature is not properly validated or restricted. It alters the way the system authenticates and authorizes requests, making it critical to ensure that this functionality cannot be abused to impersonate other users without adequate checks. The addition of this feature must be accompanied by rigorous validation and possibly authorization checks to ensure that the requesting user has the right to 'act as' the specified 'doAsUser'. Without such checks, this could lead to a significant security vulnerability.",
  "confidence": 9
}
```

Diff number: 42
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/StormSubmitter.java b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
index 63cc5a146..a4ccf5f87 100644
--- a/storm-core/src/jvm/backtype/storm/StormSubmitter.java
+++ b/storm-core/src/jvm/backtype/storm/StormSubmitter.java
@@ -268,7 +268,7 @@ public class StormSubmitter {
     @SuppressWarnings("unchecked")
     public static void submitTopology(String name, Map stormConf, StormTopology topology, SubmitOptions opts,
              ProgressListener progressListener) throws AlreadyAliveException, InvalidTopologyException, AuthorizationException {
-
+        submitTopologyAs(name, stormConf, topology, opts, progressListener, null);
     }
 
     /**

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The provided diff shows a modification within the StormSubmitter class of the Storm project, where the method 'submitTopology' now calls 'submitTopologyAs' with an additional parameter set to null. This change appears to be a refactoring aimed at consolidating the submission logic into a single method rather than introducing or modifying any security functionalities directly. There is no direct manipulation of authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Therefore, the change does not seem to have an immediate impact on the system's security posture.",
  "confidence": 9
}
```

Diff number: 43
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java b/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
index b6c3de487..cf169dc40 100644
--- a/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
+++ b/external/storm-kafka/src/jvm/storm/kafka/bolt/KafkaBolt.java
@@ -101,11 +101,10 @@ public class KafkaBolt<K, V> extends BaseRichBolt {
             } else {
                 LOG.warn("skipping key = " + key + ", topic selector returned null.");
             }
-        } catch (Exception ex) {
-            LOG.error("Could not send message with key = " + key
-                    + " and value = " + message + " to topic = " + topic, ex);
-        } finally {
             collector.ack(input);
+        } catch (Exception ex) {
+            collector.reportError(ex);
+            collector.fail(input);
         }
     }
 

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "not_security_relevant",
  "reason": "The changes in the diff file are primarily related to error handling within a KafkaBolt class, which is part of a message processing system. The modifications include removing detailed error logging and replacing it with a more generic error reporting mechanism alongside marking the message as failed. These changes do not directly impact critical security components such as authentication mechanisms, encryption algorithms, access control procedures, or logging levels related to security events. Instead, they alter how exceptions are handled and reported during message processing, which could be more about maintaining system stability and reliability rather than altering the security posture. While logging is a component of security monitoring, the specific change from detailed logging to error reporting does not inherently weaken or strengthen the system's security; it changes the verbosity and method of error handling.",
  "confidence": 8
}
```

Diff number: 45
Input: You are a security evaluator, tasked with analyzing code changes to identify their impact on system security.
Your focus should be on detecting modifications that directly affect critical security components such as authentication mechanisms, encryption algorithms, access control procedures, and logging levels related to security events.
Please review the provided diff file, concentrating solely on the lines that start with '+' or '-'.
Your analysis should discern whether the changes in this diff file are directly related to security functionalities, could potentially impact the system's security, or are unrelated to security concerns.
Your analysis must accurately categorize the security relevance of the diff file, offering a clear rationale for your classification and indicating your confidence level in your assessment.
Consider the immediate implications of the changes on system security, especially for modifications to critical components.
You will also receive context containing class(es) where the methods in the diff file were called. If no methods are present in the diff file, this context will be empty.
Use this information for your evaluation.
The output should be formatted as a JSON instance that conforms to the JSON schema below.

As an example, for the schema {"properties": {"foo": {"title": "Foo", "description": "a list of strings", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}
the object {"foo": ["bar", "baz"]} is a well-formatted instance of the schema. The object {"properties": {"foo": ["bar", "baz"]}} is not well-formatted.

Here is the output schema:
```
{"properties": {"security_relevancy": {"description": "A string whose value is one of the following: ['security_relevant' (if the code directly impacts security functionalities), 'not_security_relevant' (if the changes do not involve any security functionalities)].", "title": "Security Relevancy", "type": "string"}, "reason": {"description": "Provide a detailed explanation for your classification. If the changes are not related to security, explain why.", "title": "Reason", "type": "string"}, "confidence": {"description": "Rate your confidence in your assessment from 0-10, with 0 being not confident and 10 being extremely confident.", "title": "Confidence", "type": "integer"}}, "required": ["security_relevancy", "reason", "confidence"]}
```
Diff file:
```
diff --git a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
index 59c5b065d..e2469e5d9 100644
--- a/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
+++ b/storm-core/src/jvm/backtype/storm/security/auth/DefaultHttpCredentialsPlugin.java
@@ -83,12 +83,13 @@ public class DefaultHttpCredentialsPlugin implements IHttpCredentialsPlugin {
             userName = doAsUser;
         }
 
+        Set<Principal> principals = new HashSet<Principal>();
         if(userName != null) {
-            Subject s = new Subject();
             Principal p = new SingleUserPrincipal(userName);
-            s.getPrincipals().add(p);
-            context.setSubject(s);
+            principals.add(p);
         }
+        Subject s = new Subject(true, principals, new HashSet(), new HashSet());
+        context.setSubject(s);
 
         return context;
     }

```
Context:
```

```
Analyse the changes with a critical eye towards their impact on the security posture of the system, paying close attention to how they might alter authentication flows, data protection mechanisms, or the security of communications.
Your goal is to provide a nuanced and thorough evaluation that helps in understanding the security implications of the code changes presented.

Output: ```json
{
  "security_relevancy": "security_relevant",
  "reason": "The changes in the diff file are directly related to the authentication mechanism of the system. Specifically, the modifications impact how user principals are created and associated with a Subject in the authentication process. Previously, a new Subject was created and a Principal was added to it only if a userName was not null. With the new changes, a HashSet of Principals is created outside of the conditional block, allowing for the potential addition of multiple Principals. A new Subject is then created with this set of Principals, regardless of whether userName is null or not, and the Subject is marked as read-only by setting the readOnly flag to true in the Subject's constructor. This could potentially enhance the security by ensuring the immutability of the Subject once it's created and populated with Principals. However, the impact of these changes on the system's security depends on how the rest of the system interacts with these Subjects and Principals, particularly in terms of authorization decisions and access control enforcement based on these authentication tokens.",
  "confidence": 8
}
```

Number of diff files: 45
Number of processed files: 36
Statistics: [{'security': 22, 'mean': 8}, {'not': 14, 'mean': 8}]
Error number: 9
Accuracy: 0.7777777777777778
Precision: 0.7727272727
Recall: 0.85
F1 Score: 0.8095238095